<!DOCTYPE html>
<!-- saved from url=(0127)https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub -->
<html><script src="chrome-extension://nimlmejbmnecnaghgmbahmbaddhjbecg/content/location/location.js" id="nimlmejbmnecnaghgmbahmbaddhjbecg"></script><script src="chrome-extension://nimlmejbmnecnaghgmbahmbaddhjbecg/libs/extend-native-history-api.js"></script><script src="chrome-extension://nimlmejbmnecnaghgmbahmbaddhjbecg/libs/requests.js"></script><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Keeping up with AGI</title><link rel="shortcut icon" href="https://ssl.gstatic.com/docs/documents/images/kix-favicon-2023q4.ico"><meta name="referrer" content="strict-origin-when-cross-origin"><script nonce="">
        var DOCS_timing = {};
        var DOCS_drawing_load = {};
        var DOCS_drawing_decode = {};

        function _DOCS_record_fn(id) {
          if (window.performance && window.performance.now) {
            DOCS_drawing_load[id] = performance.now();
            const imgEl = document.getElementById(id);
            if (imgEl && imgEl.decode) {
              imgEl.decode().then(() => {
                DOCS_drawing_decode[id] = performance.now();
                if (typeof DOCS_notifyDrawingDecode === 'function') {
                  DOCS_notifyDrawingDecode(id);
                }
              });
            }
          }

          if (typeof DOCS_notifyDrawingLoad === 'function') {
            DOCS_notifyDrawingLoad(id);
          }
        }

        function DOCS_record_drawing_load(id) {
          const imgEl = document.getElementById(id);
          if (imgEl.complete) {
            _DOCS_record_fn(id);
          }
          imgEl.onload = function() {
            _DOCS_record_fn(id);
          }
        }

        document.documentElement.addEventListener(
          'load',
          function(event) {
            if (!event || !event.target) {
              return;
            }
            const target = event.target;

            if (target.tagName == 'IMG' &&  target.id && target.id.startsWith('ed.')) {
              DOCS_record_drawing_load(target.id);
            }
          },
          // Captures load events in the DOM because they do not bubble.
          /* useCapture= */ true
        );
      </script><style nonce="">
      @import url("https://fonts.googleapis.com/css?family=Google+Sans");
      @import url("https://fonts.googleapis.com/css?family=Roboto");

      body {
        font-family: Roboto, arial, sans, sans-serif;
        margin: 0;
      }

      iframe {
        border: 0;
        frameborder: 0;
        height: 100%;
        width: 100%;
      }

      #banners {
        align-items: center;
        background: white;
        display: block;
        justify-content: space-between;
        position: fixed;
        top: 0;
        width: 100%;
        z-index: 100;
      }

      #banners #publish-banner {
        background-color: #e8f0fe;
        border-bottom: 1px #ccc solid;
        color: #202124;
        display: flex;
        flex: 1 1 0%;
        height: 60px;
        width: 100%;
      }

      #publish-banner-icon {
        display: flex;
        fill: #1967d2;
        margin: auto 20px;
      }

      #publish-banner-text {
        flex-grow: 1;
        margin: auto 0;
        overflow: hidden;
        text-overflow: ellipsis;
        white-space: nowrap;
      }

      #publish-banner-buttons {
        margin: auto 25px auto 0;
        white-space: nowrap;
      }

      #publish-banner-buttons span {
        align-self: center;
        background-color: inherit;
        border: none;
        font-family: "Google Sans", Roboto, RobotoDraft, Helvetica, Arial, sans-serif;
        margin: 0 16px 0 32px;
      }

      #publish-banner-buttons a {
        color: #1967d2;
        cursor: pointer;
        font-family: "Google Sans", Roboto, RobotoDraft, Helvetica, Arial, sans-serif;
        font-size: 14px;
        font-weight: 500;
        line-height: 24px;
        text-decoration: none;
      }

      #banners #title-banner {
        background: white;
        border-bottom: 1px #ccc solid;
        display: flex;
        flex: 1 1 0%;
        height: 60px;
        width: 100%;
      }

      #banners #title-banner #title {
        flex-grow: 1;
        font-family: 'Google Sans';
        font-size: large;
        margin: auto 0 auto 20px;
        overflow: hidden;
        text-overflow: ellipsis;
        white-space: nowrap;
        width: 70%;
      }

      #banners #title-banner #interval {
        margin: auto 25px auto 0;
        font-family: Roboto;
        font-size: small;;
      }

      #footer {
        background: #f0f0f0;
        border-bottom: 1px #ccc solid;
        bottom: 0;
        font-family: Roboto;
        font-size: small;
        padding: 10px 10px;
        position: fixed;
        text-align: center;
        width: 100%;
      }

      @media only screen and (max-device-width: 800px) {
        #banners #title-banner {
          border-bottom-width: 5px;
          height: auto;
          display: block;
        }

        #banners #title-banner #title {
          font-size: 3em;
          margin: auto 0 auto 20px;
          width: 90%;
        }

        #banners #title-banner #interval {
          font-size: 1.5em;
          margin: 10px 0 auto 25px;
        }

        #footer {
          font-size: 2em;
        }
      }

      .dash {
        padding: 0 6px;
      }

    
        #contents {
          padding: 100px 20% 50px 20%;
        }

        @media only screen and (max-device-width: 800px) {
          #contents {
            padding: 150px 5% 80px;
          }
        }
      </style></head><body bis_register="W3sibWFzdGVyIjp0cnVlLCJleHRlbnNpb25JZCI6Im5pbWxtZWpibW5lY25hZ2hnbWJhaG1iYWRkaGpiZWNnIiwiYWRibG9ja2VyU3RhdHVzIjp7IkRJU1BMQVkiOiJkaXNhYmxlZCIsIkZBQ0VCT09LIjoiZGlzYWJsZWQiLCJUV0lUVEVSIjoiZGlzYWJsZWQiLCJSRURESVQiOiJkaXNhYmxlZCIsIlBJTlRFUkVTVCI6ImRpc2FibGVkIiwiSU5TVEFHUkFNIjoiZGlzYWJsZWQiLCJDT05GSUciOiJkaXNhYmxlZCJ9LCJ2ZXJzaW9uIjoiMi4wLjE0Iiwic2NvcmUiOjIwMDE0fV0=" br-mode="off" saccades-color="" fixation-strength="2" saccades-interval="0" style="--fixation-edge-opacity: 80%; --br-line-height: 1; --br-boldness: 600;"><div id="banners"><div id="publish-banner"><span id="publish-banner-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 0 24 24" width="24px"><path d="M11 7h2v2h-2zm0 4h2v6h-2zm1-9C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8z"></path></svg></span><div id="publish-banner-text">Publié à l'aide de Google&nbsp;Docs</div><div id="publish-banner-buttons"><span><a href="https://drive.google.com/u/0/abuse?id=AKkXjoxULP_ElH_8MFBjfJEiuBvOi9alWB5dUuyoN5d91RA4n65hA2xTz_gObKCD5FzGAfn5U2JiBxqU0KE3jBo:0">Signaler un abus</a></span><span><a target="_blank" title="Learn more" href="https://support.google.com/docs/answer/183965">En savoir plus</a></span></div></div><div id="title-banner"><div id="title">Keeping up with AGI</div><div id="interval"><span>Mise à jour automatique effectuée toutes les 5&nbsp;minutes</span></div></div></div><div id="contents"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=gcQ6QfUPPcP5HQL3McU2d1fzQMDTHCRuS8xCcdDlbCrzVnOlvUk088oCLYWb6HFJ);.lst-kix_bjy5owhtjxgd-6>li:before{content:"\0025cf   "}ol.lst-kix_1zrk6ew5gg6c-0{list-style-type:none}.lst-kix_bjy5owhtjxgd-4>li:before{content:"\0025cb   "}.lst-kix_bjy5owhtjxgd-8>li:before{content:"\0025a0   "}ol.lst-kix_1zrk6ew5gg6c-2{list-style-type:none}.lst-kix_bjy5owhtjxgd-3>li:before{content:"\0025cf   "}.lst-kix_bjy5owhtjxgd-7>li:before{content:"\0025cb   "}ol.lst-kix_1zrk6ew5gg6c-1{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-4{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-3{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-0.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-0 0}ol.lst-kix_1zrk6ew5gg6c-6{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-5{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-8{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-7{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-7.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-7 0}.lst-kix_bjy5owhtjxgd-5>li:before{content:"\0025a0   "}.lst-kix_zgk3or4ux7h0-8>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-8}.lst-kix_bjy5owhtjxgd-0>li:before{content:"\0025cb   "}.lst-kix_wfrxk728tq4y-7>li:before{content:"\0025cb   "}.lst-kix_bjy5owhtjxgd-2>li:before{content:"\0025a0   "}.lst-kix_wfrxk728tq4y-8>li:before{content:"\0025a0   "}.lst-kix_bjy5owhtjxgd-1>li:before{content:"\0025cb   "}ol.lst-kix_5eivyzuskjhk-1.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-1 0}.lst-kix_5eivyzuskjhk-0>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-0}ul.lst-kix_a14d8f51jepj-5{list-style-type:none}.lst-kix_1zrk6ew5gg6c-8>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-8}ul.lst-kix_a14d8f51jepj-4{list-style-type:none}ul.lst-kix_a14d8f51jepj-3{list-style-type:none}ul.lst-kix_a14d8f51jepj-2{list-style-type:none}ul.lst-kix_a14d8f51jepj-8{list-style-type:none}ul.lst-kix_a14d8f51jepj-7{list-style-type:none}ul.lst-kix_a14d8f51jepj-6{list-style-type:none}.lst-kix_wqv6tve4w11-5>li:before{content:"\0025a0   "}.lst-kix_wqv6tve4w11-6>li:before{content:"\0025cf   "}.lst-kix_wqv6tve4w11-2>li:before{content:"\0025a0   "}.lst-kix_v7w0v08yjiqh-2>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-2}.lst-kix_wqv6tve4w11-3>li:before{content:"\0025cf   "}.lst-kix_wqv6tve4w11-4>li:before{content:"\0025cb   "}ol.lst-kix_acn97mrcuhh3-7.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-7 0}.lst-kix_acn97mrcuhh3-4>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-4}.lst-kix_wqv6tve4w11-1>li:before{content:"\0025cb   "}.lst-kix_mbxt44yhtx4c-0>li:before{content:"\0025cf   "}.lst-kix_wqv6tve4w11-0>li:before{content:"\0025cf   "}.lst-kix_mbxt44yhtx4c-2>li:before{content:"\0025a0   "}.lst-kix_a14d8f51jepj-1>li:before{content:"\0025cb   "}.lst-kix_a14d8f51jepj-0>li:before{content:"\0025cf   "}.lst-kix_a14d8f51jepj-2>li:before{content:"\0025a0   "}.lst-kix_a14d8f51jepj-3>li:before{content:"\0025cf   "}.lst-kix_mbxt44yhtx4c-1>li:before{content:"\0025cb   "}.lst-kix_mbxt44yhtx4c-5>li:before{content:"\0025a0   "}.lst-kix_a14d8f51jepj-5>li:before{content:"\0025a0   "}.lst-kix_a14d8f51jepj-4>li:before{content:"\0025cb   "}.lst-kix_a14d8f51jepj-6>li:before{content:"\0025cf   "}.lst-kix_mbxt44yhtx4c-4>li:before{content:"\0025cb   "}.lst-kix_mbxt44yhtx4c-3>li:before{content:"\0025cf   "}.lst-kix_mbxt44yhtx4c-6>li:before{content:"\0025cf   "}.lst-kix_mbxt44yhtx4c-8>li:before{content:"\0025a0   "}ol.lst-kix_5eivyzuskjhk-6.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-6 0}.lst-kix_wqv6tve4w11-7>li:before{content:"\0025cb   "}.lst-kix_wqv6tve4w11-8>li:before{content:"\0025a0   "}.lst-kix_mbxt44yhtx4c-7>li:before{content:"\0025cb   "}.lst-kix_aqaqvdtizx75-1>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-1}.lst-kix_wbdf5s4tmdvx-3>li:before{content:"-  "}.lst-kix_wbdf5s4tmdvx-7>li:before{content:"-  "}.lst-kix_zgk3or4ux7h0-1>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-1}ol.lst-kix_aqaqvdtizx75-3.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-3 0}.lst-kix_wbdf5s4tmdvx-5>li:before{content:"-  "}.lst-kix_a14d8f51jepj-8>li:before{content:"\0025a0   "}ol.lst-kix_acn97mrcuhh3-2.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-2 0}.lst-kix_aqaqvdtizx75-2>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-2}.lst-kix_wbdf5s4tmdvx-1>li:before{content:"-  "}.lst-kix_y1s77pjhafgq-1>li:before{content:"\0025cb   "}.lst-kix_v7w0v08yjiqh-8>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-8,lower-roman) ". "}.lst-kix_y1s77pjhafgq-5>li:before{content:"\0025a0   "}ol.lst-kix_jinjweoua4r-2.start{counter-reset:lst-ctn-kix_jinjweoua4r-2 0}.lst-kix_k3d1c84bigo4-1>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-1}.lst-kix_y1s77pjhafgq-3>li:before{content:"\0025cf   "}ul.lst-kix_a14d8f51jepj-1{list-style-type:none}ul.lst-kix_a14d8f51jepj-0{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-5.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-5 0}.lst-kix_v7w0v08yjiqh-6>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-6,decimal) ". "}ul.lst-kix_y1s77pjhafgq-8{list-style-type:none}ul.lst-kix_y1s77pjhafgq-7{list-style-type:none}ul.lst-kix_y1s77pjhafgq-0{list-style-type:none}.lst-kix_v7w0v08yjiqh-4>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-4,lower-latin) ". "}ul.lst-kix_y1s77pjhafgq-2{list-style-type:none}ul.lst-kix_y1s77pjhafgq-1{list-style-type:none}ul.lst-kix_y1s77pjhafgq-4{list-style-type:none}.lst-kix_v7w0v08yjiqh-2>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-2,lower-roman) ". "}ul.lst-kix_y1s77pjhafgq-3{list-style-type:none}ul.lst-kix_y1s77pjhafgq-6{list-style-type:none}ul.lst-kix_y1s77pjhafgq-5{list-style-type:none}.lst-kix_wfrxk728tq4y-5>li:before{content:"\0025a0   "}.lst-kix_v7w0v08yjiqh-0>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-0,decimal) ". "}.lst-kix_k3d1c84bigo4-8>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-8}ol.lst-kix_zgk3or4ux7h0-5.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-5 0}.lst-kix_k3d1c84bigo4-2>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-2}.lst-kix_wfrxk728tq4y-3>li:before{content:"\0025cf   "}.lst-kix_wfrxk728tq4y-1>li:before{content:"\0025cb   "}.lst-kix_ub4bbw2io895-2>li:before{content:"\0025a0   "}.lst-kix_ub4bbw2io895-4>li:before{content:"\0025cb   "}.lst-kix_ub4bbw2io895-1>li:before{content:"\0025cb   "}.lst-kix_ub4bbw2io895-5>li:before{content:"\0025a0   "}.lst-kix_ub4bbw2io895-8>li:before{content:"\0025a0   "}ol.lst-kix_k3d1c84bigo4-1.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-1 0}.lst-kix_5eivyzuskjhk-2>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-2,lower-roman) ". "}ol.lst-kix_k3d1c84bigo4-7.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-7 0}.lst-kix_5eivyzuskjhk-3>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-3,decimal) ". "}ol.lst-kix_zgk3or4ux7h0-4.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-4 0}.lst-kix_5eivyzuskjhk-6>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-6,decimal) ". "}ol.lst-kix_aqaqvdtizx75-8{list-style-type:none}.lst-kix_v7w0v08yjiqh-3>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-3}ol.lst-kix_jinjweoua4r-0{list-style-type:none}ol.lst-kix_jinjweoua4r-1{list-style-type:none}ol.lst-kix_jinjweoua4r-2{list-style-type:none}.lst-kix_q8r4saa8tu1f-5>li:before{content:"-  "}.lst-kix_q8r4saa8tu1f-6>li:before{content:"-  "}.lst-kix_5eivyzuskjhk-8>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-8}ol.lst-kix_aqaqvdtizx75-0{list-style-type:none}ol.lst-kix_jinjweoua4r-7{list-style-type:none}ol.lst-kix_aqaqvdtizx75-1{list-style-type:none}ol.lst-kix_jinjweoua4r-8{list-style-type:none}ol.lst-kix_aqaqvdtizx75-2{list-style-type:none}ol.lst-kix_aqaqvdtizx75-3{list-style-type:none}ol.lst-kix_aqaqvdtizx75-4{list-style-type:none}ol.lst-kix_jinjweoua4r-3{list-style-type:none}.lst-kix_q8r4saa8tu1f-1>li:before{content:"-  "}.lst-kix_q8r4saa8tu1f-2>li:before{content:"-  "}.lst-kix_acn97mrcuhh3-5>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-5}ol.lst-kix_aqaqvdtizx75-5{list-style-type:none}ol.lst-kix_jinjweoua4r-4{list-style-type:none}ol.lst-kix_aqaqvdtizx75-6{list-style-type:none}ol.lst-kix_jinjweoua4r-5{list-style-type:none}ol.lst-kix_aqaqvdtizx75-7{list-style-type:none}ol.lst-kix_jinjweoua4r-6{list-style-type:none}.lst-kix_y1s77pjhafgq-8>li:before{content:"\0025a0   "}.lst-kix_5eivyzuskjhk-7>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-7,lower-latin) ". "}.lst-kix_y1s77pjhafgq-7>li:before{content:"\0025cb   "}.lst-kix_4pvuujemz8cl-5>li:before{content:"\0025a0   "}ol.lst-kix_k3d1c84bigo4-2.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-2 0}.lst-kix_jinjweoua4r-2>li{counter-increment:lst-ctn-kix_jinjweoua4r-2}.lst-kix_4pvuujemz8cl-6>li:before{content:"\0025cf   "}ol.lst-kix_jinjweoua4r-3.start{counter-reset:lst-ctn-kix_jinjweoua4r-3 0}.lst-kix_4pvuujemz8cl-1>li:before{content:"\0025cb   "}ol.lst-kix_aqaqvdtizx75-4.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-4 0}.lst-kix_zgk3or4ux7h0-7>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-7}.lst-kix_4pvuujemz8cl-2>li:before{content:"\0025a0   "}ul.lst-kix_4pvuujemz8cl-1{list-style-type:none}.lst-kix_jinjweoua4r-1>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-1,lower-latin) ". "}ul.lst-kix_sqt2qadhlypt-1{list-style-type:none}ul.lst-kix_4pvuujemz8cl-0{list-style-type:none}ul.lst-kix_sqt2qadhlypt-0{list-style-type:none}ul.lst-kix_sqt2qadhlypt-3{list-style-type:none}.lst-kix_wbdf5s4tmdvx-8>li:before{content:"-  "}ul.lst-kix_sqt2qadhlypt-2{list-style-type:none}.lst-kix_bfqtgh3hjsb-2>li:before{content:"\0025a0   "}ul.lst-kix_sqt2qadhlypt-5{list-style-type:none}ul.lst-kix_sqt2qadhlypt-4{list-style-type:none}ul.lst-kix_sqt2qadhlypt-7{list-style-type:none}ul.lst-kix_sqt2qadhlypt-6{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-3.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-3 0}ul.lst-kix_sqt2qadhlypt-8{list-style-type:none}.lst-kix_wbdf5s4tmdvx-4>li:before{content:"-  "}.lst-kix_66y50s42p1gp-0>li:before{content:"\0025cf   "}.lst-kix_wbdf5s4tmdvx-0>li:before{content:"-  "}.lst-kix_a14d8f51jepj-7>li:before{content:"\0025cb   "}.lst-kix_bfqtgh3hjsb-6>li:before{content:"\0025cf   "}.lst-kix_5eivyzuskjhk-6>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-6}ul.lst-kix_bfqtgh3hjsb-5{list-style-type:none}ul.lst-kix_u3o2eyw0w08p-4{list-style-type:none}ul.lst-kix_bfqtgh3hjsb-6{list-style-type:none}ul.lst-kix_u3o2eyw0w08p-5{list-style-type:none}.lst-kix_y1s77pjhafgq-0>li:before{content:"\0025cf   "}.lst-kix_zgk3or4ux7h0-0>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-0}ul.lst-kix_bfqtgh3hjsb-7{list-style-type:none}.lst-kix_v7w0v08yjiqh-7>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-7,lower-latin) ". "}ul.lst-kix_u3o2eyw0w08p-2{list-style-type:none}ul.lst-kix_bfqtgh3hjsb-8{list-style-type:none}.lst-kix_aqaqvdtizx75-5>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-5,lower-roman) ". "}ul.lst-kix_u3o2eyw0w08p-3{list-style-type:none}ul.lst-kix_u3o2eyw0w08p-8{list-style-type:none}ul.lst-kix_u3o2eyw0w08p-6{list-style-type:none}ul.lst-kix_u3o2eyw0w08p-7{list-style-type:none}.lst-kix_y1s77pjhafgq-4>li:before{content:"\0025cb   "}ul.lst-kix_bfqtgh3hjsb-0{list-style-type:none}ul.lst-kix_bfqtgh3hjsb-1{list-style-type:none}.lst-kix_cuhf2vtjitup-5>li:before{content:"\0025a0   "}ul.lst-kix_bfqtgh3hjsb-2{list-style-type:none}ul.lst-kix_bfqtgh3hjsb-3{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-0.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-0 0}ul.lst-kix_bfqtgh3hjsb-4{list-style-type:none}ol.lst-kix_jinjweoua4r-8.start{counter-reset:lst-ctn-kix_jinjweoua4r-8 0}.lst-kix_aqaqvdtizx75-1>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-1,lower-latin) ". "}ol.lst-kix_acn97mrcuhh3-8.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-8 0}.lst-kix_zgk3or4ux7h0-6>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-6,decimal) ". "}.lst-kix_jinjweoua4r-7>li{counter-increment:lst-ctn-kix_jinjweoua4r-7}ol.lst-kix_k3d1c84bigo4-6.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-6 0}ol.lst-kix_aqaqvdtizx75-8.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-8 0}ol.lst-kix_1zrk6ew5gg6c-2.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-2 0}.lst-kix_jinjweoua4r-0>li{counter-increment:lst-ctn-kix_jinjweoua4r-0}.lst-kix_zgk3or4ux7h0-2>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-2,lower-roman) ". "}.lst-kix_cuhf2vtjitup-1>li:before{content:"\0025cb   "}.lst-kix_acn97mrcuhh3-3>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-3}ol.lst-kix_jinjweoua4r-7.start{counter-reset:lst-ctn-kix_jinjweoua4r-7 0}ul.lst-kix_eu4nl4pb7sgr-3{list-style-type:none}ul.lst-kix_eu4nl4pb7sgr-2{list-style-type:none}.lst-kix_v7w0v08yjiqh-3>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-3,decimal) ". "}ul.lst-kix_eu4nl4pb7sgr-5{list-style-type:none}ul.lst-kix_eu4nl4pb7sgr-4{list-style-type:none}.lst-kix_66y50s42p1gp-8>li:before{content:"\0025a0   "}.lst-kix_1zrk6ew5gg6c-1>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-1}ul.lst-kix_eu4nl4pb7sgr-1{list-style-type:none}ul.lst-kix_eu4nl4pb7sgr-0{list-style-type:none}.lst-kix_luwys0sctije-5>li:before{content:"\0025a0   "}ol.lst-kix_aqaqvdtizx75-7.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-7 0}.lst-kix_wfrxk728tq4y-4>li:before{content:"\0025cb   "}ul.lst-kix_eu4nl4pb7sgr-7{list-style-type:none}ul.lst-kix_eu4nl4pb7sgr-6{list-style-type:none}ul.lst-kix_eu4nl4pb7sgr-8{list-style-type:none}.lst-kix_jinjweoua4r-5>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-5,lower-roman) ". "}.lst-kix_luwys0sctije-1>li:before{content:"\0025cb   "}.lst-kix_wfrxk728tq4y-0>li:before{content:"\0025cf   "}.lst-kix_66y50s42p1gp-4>li:before{content:"\0025cb   "}.lst-kix_5eivyzuskjhk-1>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-1}ul.lst-kix_dy2rea82z3y1-6{list-style-type:none}ul.lst-kix_dy2rea82z3y1-5{list-style-type:none}ul.lst-kix_dy2rea82z3y1-8{list-style-type:none}ul.lst-kix_dy2rea82z3y1-7{list-style-type:none}.lst-kix_jinjweoua4r-3>li{counter-increment:lst-ctn-kix_jinjweoua4r-3}.lst-kix_v7w0v08yjiqh-4>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-4}.lst-kix_k3d1c84bigo4-7>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-7}ol.lst-kix_v7w0v08yjiqh-6.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-6 0}ul.lst-kix_dy2rea82z3y1-0{list-style-type:none}.lst-kix_zgk3or4ux7h0-6>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-6}ul.lst-kix_dy2rea82z3y1-2{list-style-type:none}ul.lst-kix_dy2rea82z3y1-1{list-style-type:none}ul.lst-kix_dy2rea82z3y1-4{list-style-type:none}ol.lst-kix_aqaqvdtizx75-2.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-2 0}ul.lst-kix_dy2rea82z3y1-3{list-style-type:none}ul.lst-kix_75m3dcepr5qf-4{list-style-type:none}ul.lst-kix_75m3dcepr5qf-3{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-1.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-1 0}ul.lst-kix_75m3dcepr5qf-6{list-style-type:none}ul.lst-kix_75m3dcepr5qf-5{list-style-type:none}ul.lst-kix_75m3dcepr5qf-8{list-style-type:none}ul.lst-kix_r0viidqdyfja-6{list-style-type:none}ul.lst-kix_75m3dcepr5qf-7{list-style-type:none}ul.lst-kix_r0viidqdyfja-7{list-style-type:none}ul.lst-kix_r0viidqdyfja-8{list-style-type:none}ul.lst-kix_r0viidqdyfja-2{list-style-type:none}ul.lst-kix_r0viidqdyfja-3{list-style-type:none}ul.lst-kix_r0viidqdyfja-4{list-style-type:none}ul.lst-kix_r0viidqdyfja-5{list-style-type:none}ul.lst-kix_75m3dcepr5qf-0{list-style-type:none}ul.lst-kix_75m3dcepr5qf-2{list-style-type:none}ul.lst-kix_r0viidqdyfja-0{list-style-type:none}ul.lst-kix_75m3dcepr5qf-1{list-style-type:none}ul.lst-kix_r0viidqdyfja-1{list-style-type:none}ol.lst-kix_5eivyzuskjhk-7.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-7 0}ol.lst-kix_5eivyzuskjhk-0.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-0 0}.lst-kix_5eivyzuskjhk-2>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-2}.lst-kix_xumdcbvsxp2a-7>li:before{content:"-  "}.lst-kix_v7w0v08yjiqh-8>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-8}.lst-kix_k3d1c84bigo4-3>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-3}.lst-kix_egzktlggi0ic-8>li:before{content:"\0025a0   "}.lst-kix_xumdcbvsxp2a-4>li:before{content:"-  "}.lst-kix_xumdcbvsxp2a-6>li:before{content:"-  "}.lst-kix_egzktlggi0ic-3>li:before{content:"\0025cf   "}.lst-kix_egzktlggi0ic-1>li:before{content:"\0025cb   "}.lst-kix_egzktlggi0ic-0>li:before{content:"\0025cf   "}ol.lst-kix_k3d1c84bigo4-3.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-3 0}ol.lst-kix_v7w0v08yjiqh-1.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-1 0}.lst-kix_acn97mrcuhh3-1>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-1,lower-latin) ". "}.lst-kix_acn97mrcuhh3-4>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-4,lower-latin) ". "}.lst-kix_egzktlggi0ic-6>li:before{content:"\0025cf   "}.lst-kix_jinjweoua4r-2>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-2,lower-roman) ". "}.lst-kix_1zrk6ew5gg6c-3>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-3}.lst-kix_66y50s42p1gp-1>li:before{content:"\0025cb   "}.lst-kix_3l8b9azc1l9n-2>li:before{content:"\0025cf   "}.lst-kix_acn97mrcuhh3-7>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-7,lower-latin) ". "}.lst-kix_bfqtgh3hjsb-1>li:before{content:"\0025cb   "}.lst-kix_5eivyzuskjhk-5>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-5}.lst-kix_zgk3or4ux7h0-2>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-2}.lst-kix_bfqtgh3hjsb-7>li:before{content:"\0025cb   "}ol.lst-kix_aqaqvdtizx75-0.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-0 0}.lst-kix_nl3k49f7iihb-3>li:before{content:"\0025cf   "}ul.lst-kix_nl3k49f7iihb-4{list-style-type:none}.lst-kix_cuhf2vtjitup-4>li:before{content:"\0025cb   "}ul.lst-kix_nl3k49f7iihb-3{list-style-type:none}ul.lst-kix_nl3k49f7iihb-6{list-style-type:none}ul.lst-kix_nl3k49f7iihb-5{list-style-type:none}ul.lst-kix_nl3k49f7iihb-8{list-style-type:none}ul.lst-kix_nl3k49f7iihb-7{list-style-type:none}.lst-kix_aqaqvdtizx75-4>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-4,lower-latin) ". "}ol.lst-kix_k3d1c84bigo4-0.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-0 0}.lst-kix_nl3k49f7iihb-1>li:before{content:"\0025cb   "}.lst-kix_sqt2qadhlypt-2>li:before{content:"\0025a0   "}.lst-kix_sqt2qadhlypt-4>li:before{content:"\0025cb   "}.lst-kix_zgk3or4ux7h0-3>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-3}ul.lst-kix_nl3k49f7iihb-0{list-style-type:none}.lst-kix_scc7wd6335r8-3>li:before{content:"\0025cf   "}ul.lst-kix_nl3k49f7iihb-2{list-style-type:none}ul.lst-kix_nl3k49f7iihb-1{list-style-type:none}.lst-kix_v7w0v08yjiqh-0>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-0}.lst-kix_3l8b9azc1l9n-4>li:before{content:"\0025cf   "}.lst-kix_aqaqvdtizx75-2>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-2,lower-roman) ". "}.lst-kix_tuirwe3v41gd-6>li:before{content:"-  "}.lst-kix_9jdc0ima468w-5>li:before{content:"\0025a0   "}.lst-kix_zgk3or4ux7h0-7>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-7,lower-latin) ". "}ul.lst-kix_66y50s42p1gp-1{list-style-type:none}ul.lst-kix_66y50s42p1gp-0{list-style-type:none}.lst-kix_zgk3or4ux7h0-1>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-1,lower-latin) ". "}ul.lst-kix_66y50s42p1gp-7{list-style-type:none}ul.lst-kix_66y50s42p1gp-6{list-style-type:none}ul.lst-kix_66y50s42p1gp-8{list-style-type:none}ul.lst-kix_66y50s42p1gp-3{list-style-type:none}.lst-kix_v7w0v08yjiqh-1>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-1}.lst-kix_tuirwe3v41gd-8>li:before{content:"-  "}.lst-kix_cuhf2vtjitup-2>li:before{content:"\0025a0   "}ul.lst-kix_66y50s42p1gp-2{list-style-type:none}ul.lst-kix_66y50s42p1gp-5{list-style-type:none}ul.lst-kix_66y50s42p1gp-4{list-style-type:none}.lst-kix_1lunem3k9g20-3>li:before{content:"\0025cf   "}.lst-kix_acn97mrcuhh3-6>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-6}.lst-kix_1zrk6ew5gg6c-7>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-7,lower-latin) ". "}.lst-kix_v7w0v08yjiqh-7>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-7}.lst-kix_1zrk6ew5gg6c-1>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-1,lower-latin) ". "}.lst-kix_luwys0sctije-4>li:before{content:"\0025cb   "}.lst-kix_hb0nnp6nh7i4-5>li:before{content:"\0025a0   "}.lst-kix_9jdc0ima468w-7>li:before{content:"\0025cb   "}.lst-kix_hb0nnp6nh7i4-7>li:before{content:"\0025cb   "}ol.lst-kix_v7w0v08yjiqh-4.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-4 0}.lst-kix_jinjweoua4r-4>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-4,lower-latin) ". "}.lst-kix_luwys0sctije-2>li:before{content:"\0025a0   "}.lst-kix_tuirwe3v41gd-0>li:before{content:"-  "}.lst-kix_1lunem3k9g20-1>li:before{content:"\0025cb   "}.lst-kix_66y50s42p1gp-3>li:before{content:"\0025cf   "}.lst-kix_ub4bbw2io895-3>li:before{content:"\0025cf   "}.lst-kix_ub4bbw2io895-0>li:before{content:"\0025cf   "}.lst-kix_ub4bbw2io895-6>li:before{content:"\0025cf   "}.lst-kix_k3d1c84bigo4-4>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-4,lower-latin) ". "}.lst-kix_aqaqvdtizx75-8>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-8}.lst-kix_kd4buve6yzmu-6>li:before{content:"\0025cf   "}ol.lst-kix_v7w0v08yjiqh-3.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-3 0}.lst-kix_5eivyzuskjhk-0>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-0,decimal) ". "}.lst-kix_5eivyzuskjhk-5>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-5,lower-roman) ". "}ol.lst-kix_5eivyzuskjhk-4.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-4 0}.lst-kix_q8r4saa8tu1f-8>li:before{content:"-  "}.lst-kix_k3d1c84bigo4-7>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-7,lower-latin) ". "}.lst-kix_bv50cptb9bx1-5>li:before{content:"-  "}.lst-kix_q8r4saa8tu1f-0>li:before{content:"-  "}.lst-kix_1zrk6ew5gg6c-0>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-0}ol.lst-kix_v7w0v08yjiqh-2.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-2 0}.lst-kix_q8r4saa8tu1f-3>li:before{content:"-  "}.lst-kix_5eivyzuskjhk-8>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-8,lower-roman) ". "}.lst-kix_bv50cptb9bx1-2>li:before{content:"-  "}.lst-kix_h503zhbbnh4k-0>li:before{content:"\0025cf   "}.lst-kix_lcf5q6ulbeb6-7>li:before{content:"\0025cb   "}ul.lst-kix_ub4bbw2io895-8{list-style-type:none}ul.lst-kix_ub4bbw2io895-7{list-style-type:none}ul.lst-kix_ub4bbw2io895-6{list-style-type:none}ul.lst-kix_ub4bbw2io895-5{list-style-type:none}.lst-kix_4pvuujemz8cl-4>li:before{content:"\0025cb   "}.lst-kix_4pvuujemz8cl-7>li:before{content:"\0025cb   "}.lst-kix_dy2rea82z3y1-5>li:before{content:"\0025a0   "}.lst-kix_lcf5q6ulbeb6-2>li:before{content:"\0025a0   "}.lst-kix_h503zhbbnh4k-5>li:before{content:"\0025a0   "}.lst-kix_h503zhbbnh4k-8>li:before{content:"\0025a0   "}ul.lst-kix_ub4bbw2io895-4{list-style-type:none}ul.lst-kix_ub4bbw2io895-3{list-style-type:none}ul.lst-kix_ub4bbw2io895-2{list-style-type:none}.lst-kix_dy2rea82z3y1-2>li:before{content:"\0025a0   "}ul.lst-kix_ub4bbw2io895-1{list-style-type:none}ul.lst-kix_ub4bbw2io895-0{list-style-type:none}ul.lst-kix_9jdc0ima468w-0{list-style-type:none}.lst-kix_k3d1c84bigo4-0>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-0}ul.lst-kix_9jdc0ima468w-1{list-style-type:none}ul.lst-kix_9jdc0ima468w-2{list-style-type:none}.lst-kix_jinjweoua4r-7>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-7,lower-latin) ". "}ol.lst-kix_v7w0v08yjiqh-2{list-style-type:none}.lst-kix_sqt2qadhlypt-7>li:before{content:"\0025cb   "}ol.lst-kix_v7w0v08yjiqh-3{list-style-type:none}.lst-kix_nl3k49f7iihb-6>li:before{content:"\0025cf   "}ol.lst-kix_v7w0v08yjiqh-0{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-1{list-style-type:none}.lst-kix_eu4nl4pb7sgr-4>li:before{content:"-  "}ol.lst-kix_5eivyzuskjhk-5.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-5 0}.lst-kix_bfqtgh3hjsb-4>li:before{content:"\0025cb   "}ol.lst-kix_5eivyzuskjhk-2.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-2 0}ol.lst-kix_v7w0v08yjiqh-8{list-style-type:none}.lst-kix_wbdf5s4tmdvx-2>li:before{content:"-  "}ol.lst-kix_v7w0v08yjiqh-6{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-7{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-4{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-5{list-style-type:none}.lst-kix_k0xg71yzfk02-7>li:before{content:"\0025cb   "}.lst-kix_scc7wd6335r8-6>li:before{content:"\0025cf   "}.lst-kix_y1s77pjhafgq-2>li:before{content:"\0025a0   "}.lst-kix_cuhf2vtjitup-7>li:before{content:"\0025cb   "}.lst-kix_aqaqvdtizx75-7>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-7,lower-latin) ". "}.lst-kix_xumdcbvsxp2a-1>li:before{content:"-  "}.lst-kix_3l8b9azc1l9n-7>li:before{content:"\0025cf   "}.lst-kix_acn97mrcuhh3-2>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-2}.lst-kix_zgk3or4ux7h0-4>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-4,lower-latin) ". "}.lst-kix_9jdc0ima468w-2>li:before{content:"\0025a0   "}.lst-kix_f47zoa2invu7-3>li:before{content:"\0025cf   "}.lst-kix_tuirwe3v41gd-3>li:before{content:"-  "}.lst-kix_hb0nnp6nh7i4-2>li:before{content:"\0025a0   "}.lst-kix_jinjweoua4r-6>li{counter-increment:lst-ctn-kix_jinjweoua4r-6}.lst-kix_1zrk6ew5gg6c-7>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-7}.lst-kix_luwys0sctije-7>li:before{content:"\0025cb   "}.lst-kix_3nfc6uvsk11o-0>li:before{content:"\0025cf   "}.lst-kix_3nfc6uvsk11o-8>li:before{content:"\0025a0   "}.lst-kix_1lunem3k9g20-6>li:before{content:"\0025cf   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_66y50s42p1gp-6>li:before{content:"\0025cf   "}.lst-kix_v7w0v08yjiqh-1>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-1,lower-latin) ". "}.lst-kix_wfrxk728tq4y-2>li:before{content:"\0025a0   "}ol.lst-kix_5eivyzuskjhk-3.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-3 0}ul.lst-kix_9jdc0ima468w-7{list-style-type:none}ul.lst-kix_9jdc0ima468w-8{list-style-type:none}.lst-kix_1zrk6ew5gg6c-4>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-4,lower-latin) ". "}ul.lst-kix_9jdc0ima468w-3{list-style-type:none}ul.lst-kix_9jdc0ima468w-4{list-style-type:none}ul.lst-kix_9jdc0ima468w-5{list-style-type:none}ul.lst-kix_9jdc0ima468w-6{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-2{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-3{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-0{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-1{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-6{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-7{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-4{list-style-type:none}ul.lst-kix_lcf5q6ulbeb6-5{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-4{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-5{list-style-type:none}.lst-kix_acn97mrcuhh3-8>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-8}ul.lst-kix_xumdcbvsxp2a-2{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-3{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-8{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-6{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-7{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-0{list-style-type:none}ul.lst-kix_xumdcbvsxp2a-1{list-style-type:none}.lst-kix_r0viidqdyfja-7>li:before{content:"\0025cb   "}.lst-kix_6iuvlsbc8gro-7>li:before{content:"\0025cb   "}.lst-kix_6iuvlsbc8gro-6>li:before{content:"\0025cf   "}ul.lst-kix_wbdf5s4tmdvx-0{list-style-type:none}ul.lst-kix_wbdf5s4tmdvx-1{list-style-type:none}.lst-kix_6iuvlsbc8gro-5>li:before{content:"\0025a0   "}ul.lst-kix_wbdf5s4tmdvx-2{list-style-type:none}ul.lst-kix_wbdf5s4tmdvx-3{list-style-type:none}.lst-kix_r0viidqdyfja-4>li:before{content:"\0025cb   "}.lst-kix_r0viidqdyfja-8>li:before{content:"\0025a0   "}ul.lst-kix_wbdf5s4tmdvx-4{list-style-type:none}ul.lst-kix_wbdf5s4tmdvx-5{list-style-type:none}.lst-kix_6iuvlsbc8gro-3>li:before{content:"\0025cf   "}ul.lst-kix_wbdf5s4tmdvx-6{list-style-type:none}ul.lst-kix_wbdf5s4tmdvx-7{list-style-type:none}.lst-kix_6iuvlsbc8gro-2>li:before{content:"\0025a0   "}.lst-kix_6iuvlsbc8gro-4>li:before{content:"\0025cb   "}ul.lst-kix_wbdf5s4tmdvx-8{list-style-type:none}.lst-kix_r0viidqdyfja-5>li:before{content:"\0025a0   "}.lst-kix_r0viidqdyfja-6>li:before{content:"\0025cf   "}.lst-kix_6iuvlsbc8gro-0>li:before{content:"\0025cf   "}.lst-kix_v7w0v08yjiqh-6>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-6}.lst-kix_6iuvlsbc8gro-1>li:before{content:"\0025cb   "}.lst-kix_75m3dcepr5qf-7>li:before{content:"-  "}.lst-kix_75m3dcepr5qf-8>li:before{content:"-  "}ol.lst-kix_k3d1c84bigo4-4.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-4 0}.lst-kix_r0viidqdyfja-0>li:before{content:"\0025cf   "}.lst-kix_r0viidqdyfja-3>li:before{content:"\0025cf   "}ol.lst-kix_zgk3or4ux7h0-7.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-7 0}.lst-kix_75m3dcepr5qf-6>li:before{content:"-  "}.lst-kix_75m3dcepr5qf-5>li:before{content:"-  "}.lst-kix_r0viidqdyfja-1>li:before{content:"\0025cb   "}.lst-kix_r0viidqdyfja-2>li:before{content:"\0025a0   "}.lst-kix_75m3dcepr5qf-2>li:before{content:"-  "}.lst-kix_75m3dcepr5qf-1>li:before{content:"-  "}.lst-kix_75m3dcepr5qf-3>li:before{content:"-  "}.lst-kix_75m3dcepr5qf-0>li:before{content:"-  "}.lst-kix_75m3dcepr5qf-4>li:before{content:"-  "}.lst-kix_u3o2eyw0w08p-1>li:before{content:"\0025cb   "}.lst-kix_u3o2eyw0w08p-2>li:before{content:"\0025a0   "}ul.lst-kix_q8r4saa8tu1f-2{list-style-type:none}ul.lst-kix_q8r4saa8tu1f-1{list-style-type:none}ul.lst-kix_q8r4saa8tu1f-4{list-style-type:none}ul.lst-kix_q8r4saa8tu1f-3{list-style-type:none}ul.lst-kix_q8r4saa8tu1f-0{list-style-type:none}.lst-kix_u3o2eyw0w08p-0>li:before{content:"\0025cf   "}.lst-kix_aqaqvdtizx75-5>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-5}ul.lst-kix_q8r4saa8tu1f-6{list-style-type:none}ul.lst-kix_q8r4saa8tu1f-5{list-style-type:none}ul.lst-kix_q8r4saa8tu1f-8{list-style-type:none}ul.lst-kix_q8r4saa8tu1f-7{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-2.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-2 0}ul.lst-kix_kd4buve6yzmu-7{list-style-type:none}ul.lst-kix_kd4buve6yzmu-8{list-style-type:none}.lst-kix_u3o2eyw0w08p-8>li:before{content:"\0025a0   "}ul.lst-kix_kd4buve6yzmu-3{list-style-type:none}.lst-kix_jinjweoua4r-5>li{counter-increment:lst-ctn-kix_jinjweoua4r-5}ul.lst-kix_kd4buve6yzmu-4{list-style-type:none}ul.lst-kix_kd4buve6yzmu-5{list-style-type:none}ul.lst-kix_kd4buve6yzmu-6{list-style-type:none}ul.lst-kix_kd4buve6yzmu-0{list-style-type:none}ul.lst-kix_kd4buve6yzmu-1{list-style-type:none}ul.lst-kix_kd4buve6yzmu-2{list-style-type:none}ol.lst-kix_5eivyzuskjhk-1{list-style-type:none}.lst-kix_u3o2eyw0w08p-3>li:before{content:"\0025cf   "}ol.lst-kix_5eivyzuskjhk-2{list-style-type:none}ol.lst-kix_5eivyzuskjhk-3{list-style-type:none}ol.lst-kix_5eivyzuskjhk-4{list-style-type:none}ol.lst-kix_5eivyzuskjhk-5{list-style-type:none}ol.lst-kix_5eivyzuskjhk-6{list-style-type:none}ol.lst-kix_acn97mrcuhh3-0.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-0 0}ol.lst-kix_5eivyzuskjhk-7{list-style-type:none}.lst-kix_u3o2eyw0w08p-4>li:before{content:"\0025cb   "}ol.lst-kix_5eivyzuskjhk-8{list-style-type:none}.lst-kix_u3o2eyw0w08p-7>li:before{content:"\0025cb   "}ul.lst-kix_lcf5q6ulbeb6-8{list-style-type:none}.lst-kix_zgk3or4ux7h0-4>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-4}.lst-kix_u3o2eyw0w08p-5>li:before{content:"\0025a0   "}.lst-kix_u3o2eyw0w08p-6>li:before{content:"\0025cf   "}ol.lst-kix_5eivyzuskjhk-0{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-7.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-7 0}ol.lst-kix_acn97mrcuhh3-5.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-5 0}.lst-kix_k0xg71yzfk02-2>li:before{content:"\0025a0   "}.lst-kix_eu4nl4pb7sgr-3>li:before{content:"-  "}.lst-kix_acn97mrcuhh3-1>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-1}.lst-kix_k0xg71yzfk02-0>li:before{content:"\0025cf   "}.lst-kix_eu4nl4pb7sgr-1>li:before{content:"-  "}.lst-kix_k0xg71yzfk02-6>li:before{content:"\0025cf   "}ol.lst-kix_5eivyzuskjhk-8.start{counter-reset:lst-ctn-kix_5eivyzuskjhk-8 0}.lst-kix_k0xg71yzfk02-4>li:before{content:"\0025cb   "}.lst-kix_eu4nl4pb7sgr-5>li:before{content:"-  "}.lst-kix_eu4nl4pb7sgr-7>li:before{content:"-  "}.lst-kix_1zrk6ew5gg6c-5>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-5}.lst-kix_k0xg71yzfk02-8>li:before{content:"\0025a0   "}.lst-kix_f47zoa2invu7-0>li:before{content:"\0025cf   "}ul.lst-kix_h503zhbbnh4k-1{list-style-type:none}ul.lst-kix_h503zhbbnh4k-0{list-style-type:none}.lst-kix_f47zoa2invu7-2>li:before{content:"\0025a0   "}ul.lst-kix_h503zhbbnh4k-5{list-style-type:none}ul.lst-kix_h503zhbbnh4k-4{list-style-type:none}ul.lst-kix_h503zhbbnh4k-3{list-style-type:none}ul.lst-kix_h503zhbbnh4k-2{list-style-type:none}ul.lst-kix_h503zhbbnh4k-8{list-style-type:none}ul.lst-kix_h503zhbbnh4k-7{list-style-type:none}ul.lst-kix_h503zhbbnh4k-6{list-style-type:none}.lst-kix_acn97mrcuhh3-0>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-0}ul.lst-kix_3l8b9azc1l9n-7{list-style-type:none}ul.lst-kix_3l8b9azc1l9n-8{list-style-type:none}ul.lst-kix_3l8b9azc1l9n-5{list-style-type:none}.lst-kix_6iuvlsbc8gro-8>li:before{content:"\0025a0   "}ul.lst-kix_3l8b9azc1l9n-6{list-style-type:none}.lst-kix_kd4buve6yzmu-1>li:before{content:"\0025cb   "}.lst-kix_3nfc6uvsk11o-3>li:before{content:"\0025cf   "}.lst-kix_1zrk6ew5gg6c-4>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-4}.lst-kix_kd4buve6yzmu-3>li:before{content:"\0025cf   "}ul.lst-kix_3l8b9azc1l9n-0{list-style-type:none}ul.lst-kix_3l8b9azc1l9n-3{list-style-type:none}ul.lst-kix_3l8b9azc1l9n-4{list-style-type:none}ul.lst-kix_3l8b9azc1l9n-1{list-style-type:none}ul.lst-kix_3l8b9azc1l9n-2{list-style-type:none}.lst-kix_3nfc6uvsk11o-1>li:before{content:"\0025cb   "}.lst-kix_f47zoa2invu7-8>li:before{content:"\0025a0   "}.lst-kix_5eivyzuskjhk-4>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-4}.lst-kix_3nfc6uvsk11o-7>li:before{content:"\0025cb   "}ol.lst-kix_aqaqvdtizx75-1.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-1 0}.lst-kix_f47zoa2invu7-4>li:before{content:"\0025cb   "}.lst-kix_f47zoa2invu7-6>li:before{content:"\0025cf   "}ol.lst-kix_jinjweoua4r-0.start{counter-reset:lst-ctn-kix_jinjweoua4r-0 0}.lst-kix_3nfc6uvsk11o-5>li:before{content:"\0025a0   "}.lst-kix_k3d1c84bigo4-6>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-6}.lst-kix_v7w0v08yjiqh-5>li{counter-increment:lst-ctn-kix_v7w0v08yjiqh-5}.lst-kix_kd4buve6yzmu-7>li:before{content:"\0025cb   "}.lst-kix_kd4buve6yzmu-8>li:before{content:"\0025a0   "}.lst-kix_k3d1c84bigo4-1>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-1,lower-latin) ". "}.lst-kix_k3d1c84bigo4-2>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-2,lower-roman) ". "}ul.lst-kix_hb0nnp6nh7i4-7{list-style-type:none}ul.lst-kix_hb0nnp6nh7i4-8{list-style-type:none}ol.lst-kix_acn97mrcuhh3-4.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-4 0}ul.lst-kix_hb0nnp6nh7i4-5{list-style-type:none}ol.lst-kix_v7w0v08yjiqh-8.start{counter-reset:lst-ctn-kix_v7w0v08yjiqh-8 0}ul.lst-kix_hb0nnp6nh7i4-6{list-style-type:none}ol.lst-kix_1zrk6ew5gg6c-4.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-4 0}.lst-kix_k3d1c84bigo4-4>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-4}.lst-kix_k3d1c84bigo4-5>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-5,lower-roman) ". "}.lst-kix_bv50cptb9bx1-4>li:before{content:"-  "}.lst-kix_k3d1c84bigo4-6>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-6,decimal) ". "}.lst-kix_aqaqvdtizx75-6>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-6}.lst-kix_bv50cptb9bx1-7>li:before{content:"-  "}.lst-kix_bv50cptb9bx1-8>li:before{content:"-  "}ul.lst-kix_hb0nnp6nh7i4-0{list-style-type:none}ul.lst-kix_hb0nnp6nh7i4-3{list-style-type:none}ul.lst-kix_hb0nnp6nh7i4-4{list-style-type:none}ul.lst-kix_hb0nnp6nh7i4-1{list-style-type:none}ul.lst-kix_hb0nnp6nh7i4-2{list-style-type:none}ol.lst-kix_aqaqvdtizx75-5.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-5 0}.lst-kix_dy2rea82z3y1-7>li:before{content:"\0025cb   "}.lst-kix_bv50cptb9bx1-3>li:before{content:"-  "}.lst-kix_dy2rea82z3y1-8>li:before{content:"\0025a0   "}.lst-kix_lcf5q6ulbeb6-0>li:before{content:"\0025cf   "}.lst-kix_5eivyzuskjhk-3>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-3}.lst-kix_scc7wd6335r8-1>li:before{content:"\0025cb   "}.lst-kix_bv50cptb9bx1-0>li:before{content:"-  "}.lst-kix_scc7wd6335r8-0>li:before{content:"\0025cf   "}ol.lst-kix_jinjweoua4r-4.start{counter-reset:lst-ctn-kix_jinjweoua4r-4 0}ol.lst-kix_1zrk6ew5gg6c-5.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-5 0}.lst-kix_dy2rea82z3y1-0>li:before{content:"\0025cf   "}.lst-kix_lcf5q6ulbeb6-5>li:before{content:"\0025a0   "}.lst-kix_lcf5q6ulbeb6-4>li:before{content:"\0025cb   "}.lst-kix_lcf5q6ulbeb6-8>li:before{content:"\0025a0   "}.lst-kix_dy2rea82z3y1-4>li:before{content:"\0025cb   "}.lst-kix_lcf5q6ulbeb6-1>li:before{content:"\0025cb   "}.lst-kix_h503zhbbnh4k-2>li:before{content:"\0025a0   "}ol.lst-kix_acn97mrcuhh3-7{list-style-type:none}ol.lst-kix_acn97mrcuhh3-8{list-style-type:none}ol.lst-kix_acn97mrcuhh3-5{list-style-type:none}ul.lst-kix_wfrxk728tq4y-8{list-style-type:none}ol.lst-kix_acn97mrcuhh3-6{list-style-type:none}ol.lst-kix_acn97mrcuhh3-3{list-style-type:none}ul.lst-kix_wfrxk728tq4y-6{list-style-type:none}ol.lst-kix_acn97mrcuhh3-4{list-style-type:none}ul.lst-kix_wfrxk728tq4y-7{list-style-type:none}.lst-kix_h503zhbbnh4k-3>li:before{content:"\0025cf   "}ol.lst-kix_acn97mrcuhh3-1{list-style-type:none}ul.lst-kix_wfrxk728tq4y-4{list-style-type:none}ol.lst-kix_acn97mrcuhh3-2{list-style-type:none}ul.lst-kix_wfrxk728tq4y-5{list-style-type:none}ul.lst-kix_wfrxk728tq4y-2{list-style-type:none}ol.lst-kix_acn97mrcuhh3-0{list-style-type:none}ul.lst-kix_wfrxk728tq4y-3{list-style-type:none}ul.lst-kix_wfrxk728tq4y-0{list-style-type:none}ul.lst-kix_wfrxk728tq4y-1{list-style-type:none}.lst-kix_dy2rea82z3y1-3>li:before{content:"\0025cf   "}.lst-kix_h503zhbbnh4k-7>li:before{content:"\0025cb   "}.lst-kix_h503zhbbnh4k-6>li:before{content:"\0025cf   "}ol.lst-kix_k3d1c84bigo4-8.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-8 0}ul.lst-kix_egzktlggi0ic-8{list-style-type:none}ul.lst-kix_egzktlggi0ic-7{list-style-type:none}.lst-kix_nl3k49f7iihb-8>li:before{content:"\0025a0   "}ul.lst-kix_egzktlggi0ic-6{list-style-type:none}ul.lst-kix_egzktlggi0ic-5{list-style-type:none}.lst-kix_acn97mrcuhh3-6>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-6,decimal) ". "}.lst-kix_3l8b9azc1l9n-1>li:before{content:"\0025cf   "}ul.lst-kix_egzktlggi0ic-0{list-style-type:none}.lst-kix_nl3k49f7iihb-4>li:before{content:"\0025cb   "}ol.lst-kix_k3d1c84bigo4-5.start{counter-reset:lst-ctn-kix_k3d1c84bigo4-5 0}ul.lst-kix_egzktlggi0ic-4{list-style-type:none}.lst-kix_sqt2qadhlypt-5>li:before{content:"\0025a0   "}ul.lst-kix_egzktlggi0ic-3{list-style-type:none}ul.lst-kix_egzktlggi0ic-2{list-style-type:none}ul.lst-kix_egzktlggi0ic-1{list-style-type:none}.lst-kix_eu4nl4pb7sgr-2>li:before{content:"-  "}ol.lst-kix_1zrk6ew5gg6c-0.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-0 0}.lst-kix_k0xg71yzfk02-1>li:before{content:"\0025cb   "}ul.lst-kix_k0xg71yzfk02-8{list-style-type:none}ul.lst-kix_k0xg71yzfk02-7{list-style-type:none}ul.lst-kix_k0xg71yzfk02-4{list-style-type:none}.lst-kix_sqt2qadhlypt-1>li:before{content:"\0025cb   "}ul.lst-kix_k0xg71yzfk02-3{list-style-type:none}ul.lst-kix_k0xg71yzfk02-6{list-style-type:none}ul.lst-kix_k0xg71yzfk02-5{list-style-type:none}ol.lst-kix_aqaqvdtizx75-6.start{counter-reset:lst-ctn-kix_aqaqvdtizx75-6 0}ul.lst-kix_k0xg71yzfk02-0{list-style-type:none}ul.lst-kix_k0xg71yzfk02-2{list-style-type:none}ul.lst-kix_k0xg71yzfk02-1{list-style-type:none}.lst-kix_k0xg71yzfk02-5>li:before{content:"\0025a0   "}.lst-kix_nl3k49f7iihb-0>li:before{content:"\0025cf   "}.lst-kix_scc7wd6335r8-4>li:before{content:"\0025cb   "}.lst-kix_eu4nl4pb7sgr-6>li:before{content:"-  "}.lst-kix_xumdcbvsxp2a-3>li:before{content:"-  "}ol.lst-kix_jinjweoua4r-5.start{counter-reset:lst-ctn-kix_jinjweoua4r-5 0}.lst-kix_3l8b9azc1l9n-5>li:before{content:"\0025cf   "}.lst-kix_scc7wd6335r8-8>li:before{content:"\0025a0   "}.lst-kix_9jdc0ima468w-0>li:before{content:"\0025cf   "}.lst-kix_aqaqvdtizx75-4>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-4}.lst-kix_9jdc0ima468w-4>li:before{content:"\0025cb   "}.lst-kix_f47zoa2invu7-1>li:before{content:"\0025cb   "}.lst-kix_tuirwe3v41gd-5>li:before{content:"-  "}ul.lst-kix_mbxt44yhtx4c-6{list-style-type:none}ul.lst-kix_mbxt44yhtx4c-7{list-style-type:none}ul.lst-kix_mbxt44yhtx4c-8{list-style-type:none}.lst-kix_zgk3or4ux7h0-5>li{counter-increment:lst-ctn-kix_zgk3or4ux7h0-5}ul.lst-kix_mbxt44yhtx4c-2{list-style-type:none}ul.lst-kix_mbxt44yhtx4c-3{list-style-type:none}.lst-kix_1lunem3k9g20-8>li:before{content:"\0025a0   "}ul.lst-kix_mbxt44yhtx4c-4{list-style-type:none}ul.lst-kix_mbxt44yhtx4c-5{list-style-type:none}ul.lst-kix_mbxt44yhtx4c-0{list-style-type:none}.lst-kix_hb0nnp6nh7i4-0>li:before{content:"\0025cf   "}ul.lst-kix_mbxt44yhtx4c-1{list-style-type:none}.lst-kix_1zrk6ew5gg6c-6>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-6,decimal) ". "}.lst-kix_kd4buve6yzmu-0>li:before{content:"\0025cf   "}.lst-kix_1lunem3k9g20-4>li:before{content:"\0025cb   "}.lst-kix_3nfc6uvsk11o-2>li:before{content:"\0025a0   "}.lst-kix_1zrk6ew5gg6c-2>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-2,lower-roman) ". "}.lst-kix_kd4buve6yzmu-4>li:before{content:"\0025cb   "}.lst-kix_hb0nnp6nh7i4-4>li:before{content:"\0025cb   "}ul.lst-kix_bjy5owhtjxgd-8{list-style-type:none}.lst-kix_9jdc0ima468w-8>li:before{content:"\0025a0   "}ul.lst-kix_bjy5owhtjxgd-6{list-style-type:none}ul.lst-kix_bjy5owhtjxgd-7{list-style-type:none}ul.lst-kix_bjy5owhtjxgd-0{list-style-type:none}ul.lst-kix_bjy5owhtjxgd-1{list-style-type:none}.lst-kix_1lunem3k9g20-0>li:before{content:"\0025cf   "}.lst-kix_3nfc6uvsk11o-6>li:before{content:"\0025cf   "}ul.lst-kix_bjy5owhtjxgd-4{list-style-type:none}.lst-kix_f47zoa2invu7-5>li:before{content:"\0025a0   "}ul.lst-kix_bjy5owhtjxgd-5{list-style-type:none}.lst-kix_tuirwe3v41gd-1>li:before{content:"-  "}ul.lst-kix_bjy5owhtjxgd-2{list-style-type:none}.lst-kix_hb0nnp6nh7i4-8>li:before{content:"\0025a0   "}ul.lst-kix_bjy5owhtjxgd-3{list-style-type:none}ol.lst-kix_jinjweoua4r-6.start{counter-reset:lst-ctn-kix_jinjweoua4r-6 0}.lst-kix_aqaqvdtizx75-7>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-7}ul.lst-kix_scc7wd6335r8-0{list-style-type:none}ul.lst-kix_scc7wd6335r8-2{list-style-type:none}ul.lst-kix_scc7wd6335r8-1{list-style-type:none}ul.lst-kix_scc7wd6335r8-8{list-style-type:none}ul.lst-kix_scc7wd6335r8-7{list-style-type:none}ul.lst-kix_scc7wd6335r8-4{list-style-type:none}ul.lst-kix_scc7wd6335r8-3{list-style-type:none}ul.lst-kix_scc7wd6335r8-6{list-style-type:none}ul.lst-kix_scc7wd6335r8-5{list-style-type:none}ol.lst-kix_acn97mrcuhh3-1.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-1 0}ol.lst-kix_zgk3or4ux7h0-8.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-8 0}.lst-kix_xumdcbvsxp2a-8>li:before{content:"-  "}.lst-kix_egzktlggi0ic-7>li:before{content:"\0025cb   "}ol.lst-kix_zgk3or4ux7h0-1.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-1 0}.lst-kix_xumdcbvsxp2a-5>li:before{content:"-  "}.lst-kix_1zrk6ew5gg6c-6>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-6}.lst-kix_egzktlggi0ic-2>li:before{content:"\0025a0   "}.lst-kix_aqaqvdtizx75-8>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-8,lower-roman) ". "}ul.lst-kix_u3o2eyw0w08p-0{list-style-type:none}ul.lst-kix_u3o2eyw0w08p-1{list-style-type:none}ul.lst-kix_wqv6tve4w11-3{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-0{list-style-type:none}ul.lst-kix_wqv6tve4w11-4{list-style-type:none}ul.lst-kix_wqv6tve4w11-1{list-style-type:none}ul.lst-kix_wqv6tve4w11-2{list-style-type:none}ul.lst-kix_wqv6tve4w11-0{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-8{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-7{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-6{list-style-type:none}ol.lst-kix_acn97mrcuhh3-6.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-6 0}ul.lst-kix_6iuvlsbc8gro-5{list-style-type:none}ul.lst-kix_wqv6tve4w11-7{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-4{list-style-type:none}ul.lst-kix_wqv6tve4w11-8{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-3{list-style-type:none}ul.lst-kix_wqv6tve4w11-5{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-2{list-style-type:none}ul.lst-kix_wqv6tve4w11-6{list-style-type:none}ul.lst-kix_6iuvlsbc8gro-1{list-style-type:none}.lst-kix_acn97mrcuhh3-0>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-0,decimal) ". "}.lst-kix_egzktlggi0ic-4>li:before{content:"\0025cb   "}ol.lst-kix_1zrk6ew5gg6c-6.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-6 0}ul.lst-kix_4pvuujemz8cl-8{list-style-type:none}ul.lst-kix_4pvuujemz8cl-7{list-style-type:none}ul.lst-kix_4pvuujemz8cl-6{list-style-type:none}.lst-kix_egzktlggi0ic-5>li:before{content:"\0025a0   "}.lst-kix_acn97mrcuhh3-2>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-2,lower-roman) ". "}ul.lst-kix_4pvuujemz8cl-5{list-style-type:none}ul.lst-kix_4pvuujemz8cl-4{list-style-type:none}.lst-kix_acn97mrcuhh3-3>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-3,decimal) ". "}ul.lst-kix_4pvuujemz8cl-3{list-style-type:none}ul.lst-kix_4pvuujemz8cl-2{list-style-type:none}.lst-kix_3l8b9azc1l9n-0>li:before{content:"\0025cf   "}.lst-kix_jinjweoua4r-0>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-0,decimal) ". "}ol.lst-kix_zgk3or4ux7h0-3.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-3 0}.lst-kix_acn97mrcuhh3-5>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-5,lower-roman) ". "}.lst-kix_bfqtgh3hjsb-3>li:before{content:"\0025cf   "}.lst-kix_luwys0sctije-0>li:before{content:"\0025cf   "}ul.lst-kix_1lunem3k9g20-3{list-style-type:none}ul.lst-kix_1lunem3k9g20-4{list-style-type:none}.lst-kix_jinjweoua4r-6>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-6,decimal) ". "}ul.lst-kix_1lunem3k9g20-1{list-style-type:none}ul.lst-kix_1lunem3k9g20-2{list-style-type:none}.lst-kix_nl3k49f7iihb-7>li:before{content:"\0025cb   "}ul.lst-kix_1lunem3k9g20-7{list-style-type:none}ul.lst-kix_1lunem3k9g20-8{list-style-type:none}ul.lst-kix_1lunem3k9g20-5{list-style-type:none}ul.lst-kix_1lunem3k9g20-6{list-style-type:none}.lst-kix_nl3k49f7iihb-5>li:before{content:"\0025a0   "}.lst-kix_jinjweoua4r-8>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-8,lower-roman) ". "}.lst-kix_sqt2qadhlypt-6>li:before{content:"\0025cf   "}.lst-kix_sqt2qadhlypt-8>li:before{content:"\0025a0   "}ol.lst-kix_zgk3or4ux7h0-6.start{counter-reset:lst-ctn-kix_zgk3or4ux7h0-6 0}.lst-kix_4pvuujemz8cl-8>li:before{content:"\0025a0   "}.lst-kix_luwys0sctije-8>li:before{content:"\0025a0   "}.lst-kix_bfqtgh3hjsb-5>li:before{content:"\0025a0   "}.lst-kix_scc7wd6335r8-5>li:before{content:"\0025a0   "}.lst-kix_sqt2qadhlypt-0>li:before{content:"\0025cf   "}.lst-kix_aqaqvdtizx75-0>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-0,decimal) ". "}ul.lst-kix_cuhf2vtjitup-1{list-style-type:none}ul.lst-kix_cuhf2vtjitup-2{list-style-type:none}ul.lst-kix_cuhf2vtjitup-3{list-style-type:none}ul.lst-kix_cuhf2vtjitup-4{list-style-type:none}ul.lst-kix_cuhf2vtjitup-5{list-style-type:none}.lst-kix_cuhf2vtjitup-6>li:before{content:"\0025cf   "}ul.lst-kix_cuhf2vtjitup-6{list-style-type:none}ul.lst-kix_cuhf2vtjitup-7{list-style-type:none}ul.lst-kix_cuhf2vtjitup-8{list-style-type:none}.lst-kix_aqaqvdtizx75-6>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-6,decimal) ". "}.lst-kix_3l8b9azc1l9n-8>li:before{content:"\0025cf   "}.lst-kix_xumdcbvsxp2a-0>li:before{content:"-  "}.lst-kix_xumdcbvsxp2a-2>li:before{content:"-  "}ul.lst-kix_f47zoa2invu7-5{list-style-type:none}ul.lst-kix_f47zoa2invu7-6{list-style-type:none}ul.lst-kix_1lunem3k9g20-0{list-style-type:none}ul.lst-kix_f47zoa2invu7-7{list-style-type:none}ul.lst-kix_cuhf2vtjitup-0{list-style-type:none}ul.lst-kix_f47zoa2invu7-8{list-style-type:none}ul.lst-kix_f47zoa2invu7-1{list-style-type:none}.lst-kix_cuhf2vtjitup-8>li:before{content:"\0025a0   "}ul.lst-kix_f47zoa2invu7-2{list-style-type:none}ul.lst-kix_f47zoa2invu7-3{list-style-type:none}ul.lst-kix_f47zoa2invu7-4{list-style-type:none}.lst-kix_3l8b9azc1l9n-6>li:before{content:"\0025cf   "}.lst-kix_scc7wd6335r8-7>li:before{content:"\0025cb   "}.lst-kix_aqaqvdtizx75-0>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-0}ul.lst-kix_f47zoa2invu7-0{list-style-type:none}.lst-kix_zgk3or4ux7h0-5>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-5,lower-roman) ". "}ol.lst-kix_jinjweoua4r-1.start{counter-reset:lst-ctn-kix_jinjweoua4r-1 0}.lst-kix_9jdc0ima468w-3>li:before{content:"\0025cf   "}.lst-kix_tuirwe3v41gd-4>li:before{content:"-  "}ol.lst-kix_1zrk6ew5gg6c-8.start{counter-reset:lst-ctn-kix_1zrk6ew5gg6c-8 0}.lst-kix_hb0nnp6nh7i4-3>li:before{content:"\0025cf   "}.lst-kix_hb0nnp6nh7i4-1>li:before{content:"\0025cb   "}.lst-kix_cuhf2vtjitup-0>li:before{content:"\0025cf   "}.lst-kix_9jdc0ima468w-1>li:before{content:"\0025cb   "}.lst-kix_zgk3or4ux7h0-3>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-3,decimal) ". "}.lst-kix_1zrk6ew5gg6c-3>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-3,decimal) ". "}.lst-kix_1lunem3k9g20-7>li:before{content:"\0025cb   "}.lst-kix_luwys0sctije-6>li:before{content:"\0025cf   "}.lst-kix_1lunem3k9g20-5>li:before{content:"\0025a0   "}.lst-kix_66y50s42p1gp-7>li:before{content:"\0025cb   "}.lst-kix_66y50s42p1gp-5>li:before{content:"\0025a0   "}ol.lst-kix_acn97mrcuhh3-3.start{counter-reset:lst-ctn-kix_acn97mrcuhh3-3 0}.lst-kix_tuirwe3v41gd-2>li:before{content:"-  "}.lst-kix_1zrk6ew5gg6c-5>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-5,lower-roman) ". "}.lst-kix_jinjweoua4r-4>li{counter-increment:lst-ctn-kix_jinjweoua4r-4}.lst-kix_ub4bbw2io895-7>li:before{content:"\0025cb   "}.lst-kix_k3d1c84bigo4-3>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-3,decimal) ". "}.lst-kix_kd4buve6yzmu-5>li:before{content:"\0025a0   "}.lst-kix_acn97mrcuhh3-7>li{counter-increment:lst-ctn-kix_acn97mrcuhh3-7}.lst-kix_k3d1c84bigo4-0>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-0,decimal) ". "}.lst-kix_5eivyzuskjhk-1>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-1,lower-latin) ". "}.lst-kix_5eivyzuskjhk-4>li:before{content:"" counter(lst-ctn-kix_5eivyzuskjhk-4,lower-latin) ". "}.lst-kix_q8r4saa8tu1f-7>li:before{content:"-  "}.lst-kix_bv50cptb9bx1-6>li:before{content:"-  "}.lst-kix_q8r4saa8tu1f-4>li:before{content:"-  "}.lst-kix_k3d1c84bigo4-8>li:before{content:"" counter(lst-ctn-kix_k3d1c84bigo4-8,lower-roman) ". "}ol.lst-kix_zgk3or4ux7h0-3{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-4{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-5{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-6{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-0{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-1{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-2{list-style-type:none}.lst-kix_dy2rea82z3y1-6>li:before{content:"\0025cf   "}ol.lst-kix_k3d1c84bigo4-1{list-style-type:none}.lst-kix_bv50cptb9bx1-1>li:before{content:"-  "}ol.lst-kix_k3d1c84bigo4-0{list-style-type:none}ol.lst-kix_k3d1c84bigo4-3{list-style-type:none}ol.lst-kix_k3d1c84bigo4-2{list-style-type:none}ol.lst-kix_k3d1c84bigo4-5{list-style-type:none}ol.lst-kix_k3d1c84bigo4-4{list-style-type:none}.lst-kix_y1s77pjhafgq-6>li:before{content:"\0025cf   "}ol.lst-kix_k3d1c84bigo4-7{list-style-type:none}ol.lst-kix_k3d1c84bigo4-6{list-style-type:none}ol.lst-kix_k3d1c84bigo4-8{list-style-type:none}.lst-kix_4pvuujemz8cl-3>li:before{content:"\0025cf   "}.lst-kix_jinjweoua4r-8>li{counter-increment:lst-ctn-kix_jinjweoua4r-8}.lst-kix_4pvuujemz8cl-0>li:before{content:"\0025cf   "}ul.lst-kix_bv50cptb9bx1-5{list-style-type:none}ul.lst-kix_bv50cptb9bx1-6{list-style-type:none}.lst-kix_h503zhbbnh4k-1>li:before{content:"\0025cb   "}ul.lst-kix_bv50cptb9bx1-7{list-style-type:none}ul.lst-kix_bv50cptb9bx1-8{list-style-type:none}.lst-kix_lcf5q6ulbeb6-3>li:before{content:"\0025cf   "}.lst-kix_h503zhbbnh4k-4>li:before{content:"\0025cb   "}ul.lst-kix_bv50cptb9bx1-0{list-style-type:none}ul.lst-kix_bv50cptb9bx1-1{list-style-type:none}ul.lst-kix_bv50cptb9bx1-2{list-style-type:none}ul.lst-kix_bv50cptb9bx1-3{list-style-type:none}ul.lst-kix_bv50cptb9bx1-4{list-style-type:none}ol.lst-kix_zgk3or4ux7h0-7{list-style-type:none}.lst-kix_dy2rea82z3y1-1>li:before{content:"\0025cb   "}ol.lst-kix_zgk3or4ux7h0-8{list-style-type:none}.lst-kix_lcf5q6ulbeb6-6>li:before{content:"\0025cf   "}.lst-kix_acn97mrcuhh3-8>li:before{content:"" counter(lst-ctn-kix_acn97mrcuhh3-8,lower-roman) ". "}.lst-kix_bfqtgh3hjsb-0>li:before{content:"\0025cf   "}.lst-kix_wbdf5s4tmdvx-6>li:before{content:"-  "}.lst-kix_3l8b9azc1l9n-3>li:before{content:"\0025cf   "}.lst-kix_bfqtgh3hjsb-8>li:before{content:"\0025a0   "}.lst-kix_nl3k49f7iihb-2>li:before{content:"\0025a0   "}ul.lst-kix_luwys0sctije-6{list-style-type:none}.lst-kix_cuhf2vtjitup-3>li:before{content:"\0025cf   "}ul.lst-kix_luwys0sctije-5{list-style-type:none}ul.lst-kix_luwys0sctije-8{list-style-type:none}.lst-kix_aqaqvdtizx75-3>li:before{content:"" counter(lst-ctn-kix_aqaqvdtizx75-3,decimal) ". "}ul.lst-kix_luwys0sctije-7{list-style-type:none}ul.lst-kix_luwys0sctije-2{list-style-type:none}ul.lst-kix_luwys0sctije-1{list-style-type:none}ul.lst-kix_luwys0sctije-4{list-style-type:none}ul.lst-kix_luwys0sctije-3{list-style-type:none}.lst-kix_k0xg71yzfk02-3>li:before{content:"\0025cf   "}.lst-kix_sqt2qadhlypt-3>li:before{content:"\0025cf   "}.lst-kix_scc7wd6335r8-2>li:before{content:"\0025a0   "}ul.lst-kix_luwys0sctije-0{list-style-type:none}.lst-kix_eu4nl4pb7sgr-8>li:before{content:"-  "}.lst-kix_aqaqvdtizx75-3>li{counter-increment:lst-ctn-kix_aqaqvdtizx75-3}.lst-kix_1zrk6ew5gg6c-2>li{counter-increment:lst-ctn-kix_1zrk6ew5gg6c-2}.lst-kix_jinjweoua4r-1>li{counter-increment:lst-ctn-kix_jinjweoua4r-1}.lst-kix_tuirwe3v41gd-7>li:before{content:"-  "}.lst-kix_zgk3or4ux7h0-8>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-8,lower-roman) ". "}ul.lst-kix_tuirwe3v41gd-3{list-style-type:none}ul.lst-kix_tuirwe3v41gd-2{list-style-type:none}ul.lst-kix_tuirwe3v41gd-1{list-style-type:none}ul.lst-kix_tuirwe3v41gd-0{list-style-type:none}ul.lst-kix_tuirwe3v41gd-7{list-style-type:none}ul.lst-kix_tuirwe3v41gd-6{list-style-type:none}ul.lst-kix_tuirwe3v41gd-5{list-style-type:none}ul.lst-kix_tuirwe3v41gd-4{list-style-type:none}ul.lst-kix_tuirwe3v41gd-8{list-style-type:none}.lst-kix_zgk3or4ux7h0-0>li:before{content:"" counter(lst-ctn-kix_zgk3or4ux7h0-0,decimal) ". "}.lst-kix_v7w0v08yjiqh-5>li:before{content:"" counter(lst-ctn-kix_v7w0v08yjiqh-5,lower-roman) ". "}.lst-kix_3nfc6uvsk11o-4>li:before{content:"\0025cb   "}.lst-kix_eu4nl4pb7sgr-0>li:before{content:"-  "}ul.lst-kix_3nfc6uvsk11o-0{list-style-type:none}ul.lst-kix_3nfc6uvsk11o-3{list-style-type:none}ul.lst-kix_3nfc6uvsk11o-4{list-style-type:none}ul.lst-kix_3nfc6uvsk11o-1{list-style-type:none}ul.lst-kix_3nfc6uvsk11o-2{list-style-type:none}.lst-kix_wfrxk728tq4y-6>li:before{content:"\0025cf   "}.lst-kix_1zrk6ew5gg6c-0>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-0,decimal) ". "}.lst-kix_1zrk6ew5gg6c-8>li:before{content:"" counter(lst-ctn-kix_1zrk6ew5gg6c-8,lower-roman) ". "}ul.lst-kix_3nfc6uvsk11o-7{list-style-type:none}.lst-kix_kd4buve6yzmu-2>li:before{content:"\0025a0   "}ul.lst-kix_3nfc6uvsk11o-8{list-style-type:none}ul.lst-kix_3nfc6uvsk11o-5{list-style-type:none}ul.lst-kix_3nfc6uvsk11o-6{list-style-type:none}.lst-kix_f47zoa2invu7-7>li:before{content:"\0025cb   "}.lst-kix_luwys0sctije-3>li:before{content:"\0025cf   "}.lst-kix_9jdc0ima468w-6>li:before{content:"\0025cf   "}.lst-kix_5eivyzuskjhk-7>li{counter-increment:lst-ctn-kix_5eivyzuskjhk-7}.lst-kix_hb0nnp6nh7i4-6>li:before{content:"\0025cf   "}.lst-kix_66y50s42p1gp-2>li:before{content:"\0025a0   "}.lst-kix_jinjweoua4r-3>li:before{content:"" counter(lst-ctn-kix_jinjweoua4r-3,decimal) ". "}.lst-kix_1lunem3k9g20-2>li:before{content:"\0025a0   "}.lst-kix_k3d1c84bigo4-5>li{counter-increment:lst-ctn-kix_k3d1c84bigo4-5}ol{margin:0;padding:0}table td,table th{padding:0}.c25{border-right-style:solid;padding-top:2pt;border-top-width:1pt;border-bottom-color:#000000;border-right-width:1pt;padding-left:2pt;border-left-color:#000000;padding-bottom:2pt;line-height:1.15;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;border-top-color:#000000;border-bottom-style:solid;orphans:2;widows:2;text-align:left;padding-right:2pt}.c32{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:478.4pt;border-top-color:#000000;border-bottom-style:solid}.c62{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:500pt;border-top-color:#000000;border-bottom-style:solid}.c46{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:italic}.c2{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c6{margin-left:36pt;padding-top:12pt;padding-left:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c8{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c41{color:#6aa84f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Comic Sans MS";font-style:normal}.c11{color:#1155cc;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Pacifico";font-style:normal}.c31{color:#ffffff;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-family:"Arial";font-style:normal}.c55{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Pacifico";font-style:normal}.c20{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c49{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c35{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c39{padding-top:6pt;padding-bottom:9pt;line-height:0.9128348152173914;orphans:2;widows:2;text-align:left}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c42{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c34{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c16{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c45{font-weight:400;vertical-align:baseline;font-family:"Arial";font-style:normal}.c21{border-spacing:0;border-collapse:collapse;margin-right:auto}.c43{text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c18{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c7{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c17{font-size:11.5pt;font-family:"Roboto";color:#e7e9ea;font-weight:400}.c23{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#0000ee;text-decoration:underline}.c52{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c26{font-family:"Roboto Mono";color:#188038;font-weight:400}.c63{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c50{color:#cc0000;text-decoration:none;font-size:11pt}.c61{font-family:"Roboto";color:#1d9bf0;font-weight:400}.c38{margin-left:72pt;padding-left:0pt}.c4{padding:0;margin:0}.c14{margin-left:36pt;padding-left:0pt}.c10{color:inherit;text-decoration:inherit}.c56{color:#000000;font-size:11pt}.c60{vertical-align:sub}.c47{page-break-after:avoid}.c40{font-size:8pt}.c15{font-size:12pt}.c27{font-size:16pt}.c29{font-weight:700}.c53{margin-left:36pt}.c64{font-size:1pt}.c36{font-size:9pt}.c44{height:11pt}.c48{color:#404040}.c54{height:0pt}.c33{font-size:11.5pt}.c30{font-size:10pt}.c37{color:#666666}.c58{font-style:italic}.c59{background-color:#000000}.c57{font-size:15pt}.c51{color:#1155cc}.c24{height:20pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style><div class="c63 doc-content"><p class="c9"><span class="c34 c29 c57">&nbsp;I AM YOUR LOYAL SERVANT, A PROUD PLUMBER! PRAISE THE OMNISSIAH!</span></p><p class="c0"><span class="c34 c29 c57"></span></p><p class="c9"><span class="c22 c15">Trying my best (to keep up with AI literature), here’s the proof that’ll reside in basilisk’s hive mind.</span></p><p class="c0"><span class="c22 c15"></span></p><p class="c9"><span class="c22 c15">This is honestly just a stupid log. Don’t expect any structure out of it. Meant mostly to shame myself into studying more to avoid a drop in social credit.</span></p><p class="c0"><span class="c15 c22"></span></p><p class="c9"><span class="c15">Publishing this doc btw is me shamelessly stealing </span><span class="c7 c15"><a class="c10" href="https://www.google.com/url?q=https://yacine.ca/&amp;sa=D&amp;source=editors&amp;ust=1715645745170666&amp;usg=AOvVaw2jbKjBqcGjfNpq2lkYbmiN">yacine’s idea.</a></span></p><p class="c0"><span class="c34 c29 c57"></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.y497mkm38x9">LORA</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.w9g42ewrsv6m">HuggingGPT (could’ve chosen a better name, linkedin tier)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.bpg9juymt2i4">GPTEval</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.nhje8zpl9a6h">GPT4ALL</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.kqbpjqafwuy4">Vector DB</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.lsjtf03i2o20">GPU Basics</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.mrp0yijltox7">HyDE</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.xel1gzguzsf7">Generative Agent</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.mfg0t6ojnyx0">Toolformers</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.sd0vmwwrenfd">ReAct</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.fjjc94gg1tjd">LLMA Decoding Acceleration</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.qhq380elqsa9">NOOO, THEY AUTOMATED L3s (KINDA!)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.cwa637o9jxx5">OpenAGI</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.i58lpqg81119">4-bit quantization</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.39xudaq6en7t">AutoGPTs, Hmmmm</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.p2fxxk5do910">Deepspeed</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.vcfy3qdha8im">INT4 finetuning for LLMs</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.2ljfgzp9taqy">AutoGPTs, Hmmmm, Hmmmm</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.bxgor2abg2oh">Chameleon</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ddm4us8xr0xz">Evaluate Code +</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.pkyf7ow3tirj">Can The foundation be just an LLM? If only Hari Seldon read this paper</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.de661pgwg77c">Iter-CoT</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.u2u9po5s50c7">WizardLM</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.7tovo6u1zoi5">DECKARD - RL Agent that dreams</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ialik97av9v4">Training LLMs using AI generated dialogues</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.iuf4lm6t9fc4">Automating Data Analysts [By Microsoft(™)]</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.jwgwci787rwk">Local PC Waifu</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.y3lcjf4t8gro">(FLARE) Active Retrieval Augmented Generation</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.gk70cijomli7">I LOVE COMPUTERS!!!!!</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.lm0jgycrmlss">Flash Attention</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.3gj4rykq9jy5">ALiBi</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.4ongb5mjyndq">Hack to make inference faster (by HuggingFace)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.awij3sdlkzhx">Unlimiformer</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.8p9t6aqntpjz">Tree of thoughts</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.onil59pfzslp">Model Interpretation</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.iixife8061n4">QLoRA</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ha3o9hd7ga8k">Yes your models can memorize exact stuff</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ekof2pu90q63">Voyager [Diamond ranked AI Minecraft player]</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.2khndp9suv4g">Need to update doc</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.vg1k7ullmi9r">Activation-aware Weight Quantisation (AWQ)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.o10rewigaki1">SpQR (Sparse Quantised Representation)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.3zxaihhi79xz">GGML adds 2-bit quantisation</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.q220w57c18da">SOTA document bender for your company QA</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ut0e0ft9f2bn">Multimodal is hard</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ddr3pqymd2n8">Insane alpha drop from kaiokendev</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.uqpc38mvc4bb">Skinny dip into GGML code base</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.2s530jhjktoi">Skip Decode</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.nvzohmn2858i">Multi-party chat</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.r94fob5likrt">Lost in the Middle</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ao00hfb4d98g">GPT-4 Details Leaked</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ngx7ei6hhwir">How to check fine tuning datasets’ quality?</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.5avjijk7fbrh">DPO (Direct Preference Optimization)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.z6h0z8elq0dx">Mixture of Experts</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.joei56qgvuq5">Switch transformers</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.96v6nzxu9awn">Glam</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.510e3yvi7pl0">St-MOE</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.q0iiagxgf9ks">Multi-Query Attention</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.kgg6rd6vc82c">Symbol Rank ( for coding LLMs)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.h6kk8qvwb897">ReLORA</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.hp7exsn7mhb3">Zero++</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.gnuz67jz9ykx">Flash attention 2</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.uo7psucm07vi">LIMA</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.z4b0oljl6ol3">RLHF</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.hkieadha7bcj">[Lora Hub] Wait, was I talking about being blessed with the mandate of the heaven, Yes I still have it</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.wwyewk6u4jpr">TinyStories</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.xj84lkrgvymg">FNet</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.946b0qfg06ix">Scaling S3 is not easy [Not related to ML but also related to AI cause all data is in S3]</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.y0bsouxtlrlt">RetNet</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ernrh6qbp4cz">MoE (by Deepmind) (It’s soft not sparse)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.5m2hi48phjl5">Skill Issue Paper</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.jpajejzi0mlh">BERT Primer</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.25q3o3ayyp15">Estimate LLM Flops and Memory requirement</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.4mdtafvn1zrz">RoPE</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.k20442q774j2">Speculative decoding</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.zgbeak6axsff">Cool paper &nbsp;- Topology of NN</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.iuq0qulcpmpi">How to reduce KV cache mem usage?</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.sb492jcjym8x">Hyena</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.7wxbxkjqkd8q">VectorDB arc</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.xwh4f7jaohut">Ok, I am going to become Vector DB expert this week</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.706kku5ube20">Lucene HNSW</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.e2ypjdnu4z3o">FAISS</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ty7lppo40z1y">Annoy</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.3182psj9roxa">Mixture of Experts: PEFT edition by Cohere</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.wid2blj5aj68">LLM as Optimisers</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.yu5rdpsl7mm3">Generative Recommendors - Cool paper by Google</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.d9tvjp5tzgz3">Flamingo</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.hoqf9osycnys">Fusing Modalities - Chimera by Meta</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.8qusygh6x118">PromptBreeder</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.o1zjzkxl3bec">LLAVA</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.fkv4gpuc05qh">LLAVA-1.5</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.jgbdffqbioic">IMPORTANT INTERPRETABILITY PAPER BY ANTHROPIC</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.qgun4aqx6v09">SAM</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.u6dvoeauvv44">Qwen-VL</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.u5yvo3l6618g">SigLIP</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.q4mwkz57ks8f">One peace</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.tel1g723lkx2">Make LLM do Maths</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.1xzknt4rjy32">Distil-Whisper</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ny720xp8wd4y">It’s not AGI (it’s just your data)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.edzoxhcrwk3z">Insane ML Notes on Twitter with Q&amp;A</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.70eni9mb9xfr">Stable Video Diffusion (SVD)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.78v1y6r9a5om">Stable Diffusion Turbo (or How to distill a diffusion model 101)</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.3a6qs3r7wns7">I can’t hear the MUSIC* &nbsp;!!!!!!! NEEEED TO GET BETTTTTTTER!!!</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.x03cdvmo4owf">Images are Sentences</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.ixfdm4b354d8">Videos are sentences</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.rshzl18he64i">Sentences are predictable</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.r8lisb201wer">Mamba - faster architecture (Reading cause Tri Dao is author)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.qk8avpij1hrh">Gemini</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.in9cs1ahq46j">Mitigating LLM Hallucinations</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.je2s215h0vxj">LCMs</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.eouqh6ecnjxs">Use smol models to train large models faster</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.tt3rvz6eqay2">DoReMI</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.t7jrdjgdu018">LLM Paper from Apple?? : That’s a rare sight</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.qy665v47dt8m">Multimodal paper from Apple???</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.8juh78if2ead">Amazing paper to Learn about Dingboard</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.mjdcoi7mufq0">TDM edge Multimodal arc (I blame Vik)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.m6x3qi8j3soy">MobileVLM</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.2j5bz5aufu9r">MathPile</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.muhdaz5l8bei">Unified-IO 2</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.x0s9sfo234op">DocLLM</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.5z79snx8iy8d">Microsoft broke MTEB</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.puqljqtpn7jp">Reading List from AHM</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.b8pgdoszx5d0">Reading List from Yacine</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.at9212mywj8h">LASERRRRR (for reasoning)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.q9qx67yy3hzn">Embarrassing myself publicly arc (PHOTOMAKER)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.q1hqsepas7wp">Lumiere</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.27dzbrc148zx">Deepseek Coder</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.kianmoajn1vd">IPAdapter</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.a56vgc4qjq11">How to create AGI?</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.sd5qq3bqy2xo">ILYA’s READING LIST (For getting up to speed on today’s architectures)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.2hp09tg85euh">Stream Diffusion - Brrrrr ImageGen at 100FPS</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.h0f2as8iyimw">MLLM-Guided Image Editing (MGIE)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.tqy4w8t8csw2">Matryoshka Embeddings</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.d4t3timi8crq">Generalising Length of Transformers</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.o0rdzd7qcvw7">World Model</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.be2tfr7p2dj">Diffusion Transformers</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.g6upvqdpnqeb">Stable Diffusion 3</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.pyj6hj96jujj">Deepseek-VL</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.dyk82i1ibtwq">Synth2</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.qjgq4dgi8jki">Fashion Diffusion (Make your waifu dress in Zara)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.q8nfqnfo0hvx">Another Apple LLM (this time it’s multimodal)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.8lfrd26bkebm">Quiet-Star (Is it really the fabled openai algo, nope)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.j332r3uucd5x">Transformers for time series (truly retarded)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.qzan5v8imnfn">GaLore</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.8qvoptlt4jsx">ORPO</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.2axbf5pkc531">MyVLM (Shitty Name only Snapchat can think of)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.d70okn1gvhmg">Factuality in LLMs</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.8nh1t46pwwcm">Layer Skip</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.2tivp680y45b">Training a Judge (model)</a></span></p><p class="c18"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.sul6xbl9y84l">APPENDIX</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.f8teq1wui52j">Tips and Tricks</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.hbwudrcbj6u6">Karpathy’s Presentation in MS Build 2023</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.sp8pgb4nordc">GEMINI PRO VS GPT-4V comparison</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.jmdmf79vdtmj">Answers to Stupid questions (mostly for me)</a></span></p><p class="c16"><span class="c2"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.7wyq1bmblo6l">Reading List</a></span></p><p class="c0"><span class="c22 c15"></span></p><p class="c0"><span class="c22 c15"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 471.50px; height: 471.50px;"><img alt="" src="./Keeping up with AGI_files/00008-2117648808.png" style="width: 471.50px; height: 471.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c20"><span class="c45 c7 c30"></span></p><h1 class="c35 c47" id="h.y497mkm38x9"><span class="c5">LORA</span></h1><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_6iuvlsbc8gro-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Extremely simple to implement, just add take dot product of two low-rank matrices A and B and multiple them by a scaling factor (alpha/r). &nbsp;A is zerooooos, and B is normal distribution.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Add this to the final output from the pretrained neural net. Freeze the existing weights and only adjust the A and B matrices in gradient descent.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Lalala, you have finetuned model</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Can be trained in 2-3 hours on 4090</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Still not able to understand the maths behind having low rank, like why not have have single matrix uwu</span></li><li class="c9 c14 li-bullet-0"><span class="c3">I guess lora diffusion uses rank = 1</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Works for all transformers (and maybe even CNNs) btw</span></li></ul><p class="c20"><span class="c45 c7 c30"></span></p><p class="c35"><span class="c22 c15">Need to play around with AutoGPT and more agents. Mfw it deletes all the pepe memes in my mac.</span></p><p class="c20"><span class="c22 c15"></span></p><p class="c35"><span class="c7 c15"><a class="c10" href="https://www.google.com/url?q=https://www.reddit.com/r/ChatGPT/comments/12diapw/gpt4_week_3_chatbots_are_yesterdays_news_ai/&amp;sa=D&amp;source=editors&amp;ust=1715645745196822&amp;usg=AOvVaw1msblNHsvv2rVvvz7V7ZmS">https://www.reddit.com/r/ChatGPT/comments/12diapw/gpt4_week_3_chatbots_are_yesterdays_news_ai/</a></span></p><p class="c35"><span class="c22 c15">(Moving too fast, need to catch up)</span></p><p class="c35"><span class="c22 c15">Still haven’t tried Toolformer and ReAct, they are now available in LangChain<br>Should be pretty easy (goddamn tho I really don’t want to write YAML configs)</span></p><p class="c20"><span class="c22 c15"></span></p><h1 class="c35 c47" id="h.w9g42ewrsv6m"><span class="c5">HuggingGPT (could’ve chosen a better name, linkedin tier)</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2303.17580.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745197690&amp;usg=AOvVaw3Oq0PbAedSeT2etMyjSsM_">https://arxiv.org/pdf/2303.17580.pdf</a></span></p><p class="c35"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 397.33px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-08 at 2.04.42 AM.png" style="width: 624.00px; height: 397.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c35"><span class="c15">Seems to be the end of NLPcels. Ideally, could have been done using NLP by tracking the question tokens I guess?<br><br>I still don’t fully understand how LLMs predict the tool/model to be used accurately, how do we control the hallucination there, is setting temperature = 0 the solution for everything?</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.bpg9juymt2i4"><span class="c5">GPTEval</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2303.16634&amp;sa=D&amp;source=editors&amp;ust=1715645745198603&amp;usg=AOvVaw12DRYmqAzmFh_5pxfYl8L0">https://arxiv.org/abs/2303.16634</a></span></p><p class="c9"><span class="c3">Some interesting stuff in that paper, mostly related to how they used logprobs to do scoring rather than absolute scores<br><br>Also, for GPT-4, calculated logprobs manually</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 353.30px; height: 335.30px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-08 at 3.15.29 AM.png" style="width: 353.30px; height: 335.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 351.10px; height: 550.27px;"><img alt="" src="./Keeping up with AGI_files/Fs377nqWYAAk3j3.png" style="width: 351.10px; height: 550.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.nhje8zpl9a6h"><span class="c5">GPT4ALL</span></h1><p class="c9"><span class="c3">Works quite well in mac</span></p><p class="c9"><span class="c3">Only issue is I had to download 2 separate things (1 weights and 1 executable) from two different places</span></p><p class="c9"><span class="c3">Terminal interface quite good, can pipe input/output as well</span></p><p class="c9"><span class="c3">Result though not really great (Damn these days billion is not enough, we need to enter the trillion era fr).</span></p><p class="c9"><span class="c3">Should I finetune this over some OSS docs??? The train.py script seems easy peasy, but setting it up on colab might give me nightmares.</span></p><p class="c9"><span class="c3">LLM as Terminal commands like jq (Brain explode jpg)</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Actually, I am just going to lora the fuck out of GPT4All</span></p><p class="c9"><span class="c3">Not going well honestly, too many errors</span></p><p class="c9"><span>Finally atleast it ran when I gave up on </span><span class="c26">accelerate</span><span class="c3">&nbsp;and just ran it using normal python3 train.py</span></p><p class="c9"><span class="c3">Still failed while trying to slice 0-length tensor, seems to be related to tokenizer returning empty</span></p><p class="c9"><span class="c3">Btw, this transformers submodule looks interesting, need to check it out later hehe</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745200600&amp;usg=AOvVaw2KIaP7pNX5zG05g33JSEBX">https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf</a></span><span class="c3"><br>So Basically it takes 8 hours on A100 to finetune LORA (Hmm, and also 800$), not worth it for me honestly (a noob collab user)</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 490.82px; height: 260.67px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-08 at 2.50.17 PM.png" style="width: 490.82px; height: 260.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Tried running the finetuning in colab with the following configs - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/nomic-ai/gpt4all/issues/108&amp;sa=D&amp;source=editors&amp;ust=1715645745201171&amp;usg=AOvVaw0XUr6o9s7UkQbnLbfCG4hi">https://github.com/nomic-ai/gpt4all/issues/108</a></span><span class="c3"><br></span></p><p class="c9"><span>Dataset taken from &nbsp;- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations_with_p3/blob/main/data.jsonl&amp;sa=D&amp;source=editors&amp;ust=1715645745201624&amp;usg=AOvVaw0yWzCkWnxAcsKOdv8t55Lg">https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations_with_p3/blob/main/data.jsonl</a></span><span class="c3"><br></span></p><p class="c9"><span class="c3">Disabled wandb logging (poor guy, no account)<br>Running into some tokenization errors with zero length tensors.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>OK, LFG!!!, It is working, I am able to finetune. Seems like you need </span><span class="c26">source</span><span>&nbsp;column as well along with </span><span class="c26">prompt</span><span>&nbsp;and </span><span class="c26">response</span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Seems like it is not utilizing GPU at all?? Memory usage seems 0. Only running on CPU<br>WTF. Ths is when running with python3 train.py<br><br>Running </span><span class="c26">accelerate launch</span><span class="c3">&nbsp;doesn’t work at all in colab. Saw too many errors on github as well with no clear solution.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Using the P3 removed dataset now (had to fucking convert parquet to jsonl)</span></p><p class="c9"><span class="c3">Let’s see if it works</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So turns out I am dumb (obviously)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>You can just use the parquet dataset with P3 removed from the hugging face (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations/tree/main/data&amp;sa=D&amp;source=editors&amp;ust=1715645745203124&amp;usg=AOvVaw3KonNHi3pyL6nhtdJsPTlE">https://huggingface.co/datasets/nomic-ai/gpt4all_prompt_generations/tree/main/data</a></span><span>)</span><span class="c3">. The code already supports that. Let’s see if it works this time </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Naah Still fails. </span></p><h1 class="c1 c24" id="h.z7bqwaw3f9ji"><span class="c5"></span></h1><p class="c9"><span class="c3">I want to figure out this generative agent shit for sure. Like Agents are literally the next step.</span></p><p class="c9"><span class="c3">LangChain is also moving quite fast on this. </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.kqbpjqafwuy4"><span class="c5">Vector DB</span></h1><p class="c9"><span class="c3">I clearly know in principle how Vector DBs work.<br>But I still don’t understand how they solve the performance issue of searching through all embeddings for nearest neighbours (that’s cause I am bad at maths)</span></p><p class="c9"><span class="c3">Pretty sure there’s some heuristic from which you can select a smaller set of segments/files containing the embeddings that can fit into the memory.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">OK looked at the chromDB code base and these mfs are just using wrappers on duckDB and hnswlib.</span></p><p class="c9"><span class="c3">Duckdb stores the data and hnswlib creates the index for similarity search.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Mf hnsw codebase is in c++. Time to paste it into chatGPT to understand wtf is going on.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So for searching atleast it is simply doing BFS on a graph and adding result to a priority queue</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Open questions : </span></p><ul class="c4 lst-kix_nl3k49f7iihb-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">How to figure out the starting node for the traversal?</span></li><li class="c9 c14 li-bullet-0"><span class="c3">How to add a new node to an existing graph.</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Atleast now I understand why do they ask the similarity measure at the time of creating index in pinecone.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Also benchmarks for most nearest neighbour algos - &nbsp;</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/erikbern/ann-benchmarks&amp;sa=D&amp;source=editors&amp;ust=1715645745205803&amp;usg=AOvVaw11oKnirRSUZKvZnvO8HGy9">https://github.com/erikbern/ann-benchmarks</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.lsjtf03i2o20"><span class="c5">GPU Basics</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">OK, so to create a scalable vector DB, I guess there’s no solution other than to use GPU (since vector search is compute heavy)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 608.19px; height: 967.75px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0.png" style="width: 608.19px; height: 967.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is def outdated as it refers to P100 which is quite old (Apparently a lot of it is still same till Ampere, but changed significantly in Hopper)</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 610.50px; height: 355.72px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 1.40.20 AM.png" style="width: 610.50px; height: 355.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">So they follow SIMT instead of SIMD. each of the 32 lanes/threads in a warp execute the same instruction but on different data. It’s kinda like a turbocharged SIMD. </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Much better diagram from - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://web.ecs.syr.edu/~ffiorett/files/papers/padl14.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745208205&amp;usg=AOvVaw3yH5X_pJ9ypw_ZYsjvw1BB">https://web.ecs.syr.edu/~ffiorett/files/papers/padl14.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 670.14px; height: 612.06px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(1).png" style="width: 670.14px; height: 612.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I should learn GPU programming fr</span></p><p class="c9"><span class="c3">&nbsp;</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 669.33px; height: 422.35px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 1.44.53 AM.png" style="width: 669.33px; height: 422.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Some alpha on GPU in this similarity search paper (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/1702.08734.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745209721&amp;usg=AOvVaw162WM2PmyGajA5JFX2zPi7">https://arxiv.org/pdf/1702.08734.pdf</a></span><span>)</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.mrp0yijltox7"><span class="c5">HyDE</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 610.00px; height: 699.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-09 at 10.56.49 PM.png" style="width: 610.00px; height: 699.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">This guy is sooooooo correct. That’s literally what I am facing. </span></p><p class="c9"><span class="c3">I need to push a million docs in context.</span></p><p class="c9"><span class="c3">Map-reduce summarisation is losing sooo much info. </span></p><p class="c9"><span class="c3">Plus making too many GPT calls is just sloooow (mf, gpt chat APIs support batching soon)</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 605.64px; height: 217.16px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-09 at 11.00.41 PM.png" style="width: 605.64px; height: 217.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">This is genius. Mfw you can call vectorDB in the end of the chain rather the start.</span></p><p class="c9"><span class="c3">We literally took an hallucinated answer and converted it to actual text.</span></p><p class="c9"><span class="c3">Issue is though, what if GPT can’t even hallucinate, like it literally outputs one line?</span></p><p class="c9"><span class="c3">One more drawback is that you have to compute embeddings of large answers every time instead of a shorter query. </span></p><p class="c9"><span class="c3">So two performance bottlenecks.</span></p><p class="c9"><span class="c3">I guess I should give shoggoth lang (the weird compression by GPT) a try. Its lossy but might help here cause we are retrieving from the database.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2212.10496.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745212482&amp;usg=AOvVaw1gjdLM2fOw1Gx5GAqnVDZy">https://arxiv.org/pdf/2212.10496.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.xel1gzguzsf7"><span class="c5">Generative Agent</span></h1><p class="c9"><span class="c3">We actually simulating society now via LLMs.</span></p><p class="c9"><span class="c3">Again just plugging in memory + some prompt engineering to reason works wonders.</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2304.03442.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745213631&amp;usg=AOvVaw3Ud_LeCNmCv9WQHB9GZe7F">https://arxiv.org/pdf/2304.03442.pdf</a></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 581.56px; height: 570.35px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 12.06.09 PM.png" style="width: 581.56px; height: 570.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This agent might just be better than me cause I can’t remember who the fuck I ran into at the park and clearly not what they were working on (unless they are working on some cool shit).</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 530.45px; height: 479.35px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 12.07.09 PM.png" style="width: 530.45px; height: 479.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Btw this runs on GPT-3.5-TURBO and not GPT-4.</span></p><p class="c9"><span class="c3">Just switching the model will make this soooooo much more impressive.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So for simulating memory, they are simply storing whatever the agent observed in some DB. At the point of interaction, these observations are fetched and then rated on recency, importance (by directly calling the LLM to assign a score) and relevance to the current situation. Take weighted sum, select as many memories as can fit in context window of LLM and hit it.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I feel that this type of scoring via LLM doesn’t work really well though. Same was said in that using GPT as labeller paper.</span></p><p class="c35"><span class="c22 c40">On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory.</span></p><p class="c35"><span class="c22 c40">Memory: buying groceries at The Willows Market and Pharmacy<br>Rating: &lt;fill in&gt;</span></p><p class="c35"><span class="c36">This prompt returns an integer value of </span><span class="c40 c48">2 </span><span class="c36">for “cleaning up the room” and </span><span class="c40 c48">8 </span><span class="c19">for “asking your crush out on a date.” The importance score is generated at the time the memory object is created.</span></p><p class="c20"><span class="c19"></span></p><p class="c35"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 618.09px; height: 344.12px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 2.45.10 PM.png" style="width: 618.09px; height: 344.12px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c35"><span class="c3">^ Brain explode JPG (Honestly though IRL, it’ll be too slow to be practical in large systems due to sheer number of LLM calls but that should be resolved in like 6 months)</span></p><p class="c20"><span class="c19"></span></p><p class="c9"><span class="c3">So for Reflection they are using memory as well. Just gather recent observations, ask LLMs what questions should I ask based on these observations and then ask LLM again to answer those questions. &nbsp;It is only triggered btw when importance score of observations goes beyond a certain threshold. Generally that happens twice-thrice a day (man, this is like a human lol)<br></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So after solving for Observation and Reflection, final step in Planning. Mainly how to avoid duplicate actions like eating food twice in same hour as well as incoherent actions like playing basketball in a swimming pool.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Again, they are using memory for it. Just store the current plan along with Observation and reflection. First ask LLM to make a broad plan for the day, then using LLM start to break that plan into chunks of 1 hour. Then break it down further into chunks of 15 minutes. This solves for non-duplicate actions.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Secondly, you can also make changes to the plan based on new observations. Again this can be done by asking LLM questions like </span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.54px; height: 416.93px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 3.20.56 PM.png" style="width: 602.54px; height: 416.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Final complete architecture.</span></p><p class="c9"><span>&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 692.96px; height: 222.31px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 3.21.16 PM.png" style="width: 692.96px; height: 222.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br>I feel like the number of things that are possible if LLM inference is like in single digit millis. We might actually be able to simulate brain functions. FAAASTEEEER COMPUTTTE! QUANTISATION!!!</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.00px; height: 207.01px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-10 at 3.16.49 PM.png" style="width: 671.00px; height: 246.00px; margin-left: 0.00px; margin-top: -38.99px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.mfg0t6ojnyx0"><span class="c5">Toolformers</span></h1><p class="c9"><span class="c3">Ok so this paper had some maths equations</span></p><p class="c9"><span class="c3">But turns out those were just API calls represented as a(i) -&gt; r, where a is api, i is input and r is output</span></p><p class="c9"><span class="c3">So they are first taking a clean dataset, then selecting the places where we can insert some API call </span></p><p class="c9"><span class="c3">This is being done by asking an LLM</span></p><p class="c9"><span class="c3">They then filter out the API calls where the output of the API is actually helpful. They do this by calculating CE loss by providing the result of API, and not providing result of API. If that’s above a configured threshold tao, only then the API call is added to the input. This is only during training.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Now they finetune an LLM with this context + LLM call</span></p><p class="c9"><span class="c3">And voila</span></p><p class="c9"><span class="c3">You now have an LLM that can predict API calls</span></p><p class="c9"><span class="c3">During inference, you simply detect → token</span></p><p class="c9"><span class="c3">As soon as you get it, means the preceding text was an API call and now you actually need to do the work and fill the result before predicting rest of the tokens.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 547.54px; height: 808.31px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-11 at 1.15.47 AM.png" style="width: 547.54px; height: 808.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 552.26px; height: 1005.51px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-11 at 1.13.28 AM.png" style="width: 552.26px; height: 1005.51px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Pretty simple paper honestly, but finetuning is pain in the ass. Can LoRA be applied here during training?</span></p><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c12"></span></p><h1 class="c1" id="h.sd0vmwwrenfd"><span class="c5">ReAct</span></h1><p class="c9"><span class="c3">One of the most popular papers on getting LLM to be more humanlike</span></p><p class="c9"><span class="c3">Ngl, too many words for what seems like prompt engineering</span></p><p class="c9"><span class="c3">Should skip directly to appendix</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 425.00px; height: 625.00px;"><img alt="" src="./Keeping up with AGI_files/61CHxxFZVbL._AC_SX425_.jpg" style="width: 425.00px; height: 625.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">Yeah, I didn’t find a lot of value here. The results also don’t seem that big of an improvement.</span></p><p class="c9"><span class="c3">Gonna stick to a few-shot prompting for most tasks.</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.fjjc94gg1tjd"><span class="c5">LLMA Decoding Acceleration</span></h1><p class="c9"><span>Link - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2304.04487.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745221768&amp;usg=AOvVaw29k9Bkvize1A8ssHGQO8Yp">https://arxiv.org/pdf/2304.04487.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Basically why are you generating tokens one by one anon when you can simply copy a lot of it from the context in the prompt itself </span></p><p class="c9"><span class="c3">Like when you do context based or history based Question answering or something else, the LLM will (as expected) generate a lot of stuff as it is mentioned in the context.</span></p><p class="c9"><span>So you can simply change the LLM inference design so that when </span><span class="c29">n</span><span>&nbsp;number of generated tokens match some text in the input, we can simply take </span><span class="c29">k</span><span class="c3">&nbsp;more tokens from the input and simply append it to the output.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Then we can ask LLM to directly generate next K tokens and &nbsp;then remove the ones that are not the same.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So your inference is reduced from K steps to just 2.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">[BTW, most of these papers have hidden alpha in appendix where they tell their prompt templates]</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 376.52px; height: 367.08px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-11 at 7.32.04 PM.png" style="width: 376.52px; height: 367.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c25"><span class="c3">Honestly the pace of LLM research is so fast that a lot of these techniques can be simply made better</span></p><p class="c25"><span class="c3">By plugging in the techniques from the other papers published in last month or so.</span></p><p class="c25"><span class="c3">E.g. you can improve the generative agent paper by using a better scoring technique from GPTEval paper (take weighted sum of logprobs instead of absolute rank)</span></p><p class="c25"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 400.10px; height: 300.07px;"><img alt="" src="./Keeping up with AGI_files/d9y9osm-ad269992-7a85-45be-861e-7984e35e5310.jpg" style="width: 400.10px; height: 300.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Also, I might need to learn Linear algebra fr now to understand some of these papers better &nbsp;(but let’s see how much I can avoid doing that first). Luckily most of them have just basic error loss or probabilities equations for now.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 400.00px; height: 400.00px;"><img alt="" src="./Keeping up with AGI_files/FYXAwUsw_400x400.jpg" style="width: 400.00px; height: 400.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c25"><span class="c3">Some more papers I need to read (related to making inference faster) - </span></p><p class="c25 c44"><span class="c3"></span></p><p class="c25"><span>4-bit quantisation - </span><span class="c51"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2212.09720&amp;sa=D&amp;source=editors&amp;ust=1715645745224785&amp;usg=AOvVaw13EzMcTnAJWqyGgF0Iv3Az">https://arxiv.org/abs/2212.09720</a></span><span class="c3">&nbsp;(not gonna have to pretend that I know about it while discussing llama.cpp)</span></p><p class="c25 c44"><span class="c3"></span></p><p class="c25"><span>SparseGPT - </span><span class="c51"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2301.00774&amp;sa=D&amp;source=editors&amp;ust=1715645745225238&amp;usg=AOvVaw0OEmtATv82yNKSY-AJECjx">https://arxiv.org/abs/2301.00774</a></span><span class="c3">&nbsp;(mostly related to pruning I guess)</span></p><p class="c25 c44"><span class="c3"></span></p><p class="c25"><span>Compression - </span><span class="c51"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2002.02925&amp;sa=D&amp;source=editors&amp;ust=1715645745225626&amp;usg=AOvVaw1kqJOULyPO7YbOxC_qUTvt">https://arxiv.org/abs/2002.02925</a></span></p><p class="c25 c44"><span class="c3"></span></p><p class="c25"><span>Distillation (of attention) - </span><span class="c51"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2002.02925&amp;sa=D&amp;source=editors&amp;ust=1715645745225991&amp;usg=AOvVaw2Pm7E3ah97rYjJ4_zgzYqX">https://arxiv.org/abs/2002.02925</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.qhq380elqsa9"><span class="c5">NOOO, THEY AUTOMATED L3s (KINDA!)</span></h1><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 663.32px; height: 371.18px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-12 at 12.16.37 PM.png" style="width: 663.32px; height: 371.18px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It might very well be already better than me. Only issue with such systems again is the inference speed. </span></p><p class="c9"><span class="c3">But then again, I play Apex or Overwatch while debugging an issue so we are both inefficient. (Codex = code-davinci-002)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.50px; height: 289.16px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-12 at 1.03.30 PM.png" style="width: 674.50px; height: 289.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Hmm, I remember reading about this high-level semantic idea in the Generative Agent paper as well (specifically in the planning phase). </span></p><p class="c9"><span class="c3">Basically first generate a good enough summary of the code, break it down into chunks and then recursively generate a finer summary of each chunk.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This way the program doesn’t hallucinate much or creates duplicate summaries.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 675.50px; height: 76.09px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-12 at 1.08.50 PM.png" style="width: 675.50px; height: 76.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.cwa637o9jxx5"><span class="c5">OpenAGI</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Not sure about their few-shot approach cause the solution is mentioned in the prompts itself.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Idea is interesting though but isn’t that exactly what toolformer/plugin is doing though?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Handing over the complex tasks to plugins and then using their output. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I guess toolformer can’t operator in pipelined fashion is being done here. Not really sure though.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Following is much better way IMO VVVV</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 634.83px; height: 402.02px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-12 at 5.08.54 PM.png" style="width: 634.83px; height: 402.02px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.i58lpqg81119"><span class="c5">4-bit quantization</span></h1><p class="c9"><span>Reminded me why I hate being in this limbo of US and UK english. Like should I use </span><span class="c26">sation</span><span>&nbsp;or </span><span class="c43 c26">zation</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">FUCK OFFF!</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 406.00px; height: 238.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-12 at 5.02.28 PM.png" style="width: 406.00px; height: 238.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Anyways,</span></p><p class="c9"><span class="c3">Quantisation is basically mapping a value to a finite set F</span></p><p class="c9"><span class="c3">So e.g. You can map 32 bit integer values to a set of 16 floating point or (integer) values</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For the most basic quantisation the technique is quite simple</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_75m3dcepr5qf-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Normalise the input values by dividing them with max</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Find the closest value in the finite set to this normalised value</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Store this mapping somewhere so that it can be used to dequantise the value</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c42"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 699.50px; height: 321.35px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-12 at 5.00.13 PM.png" style="width: 699.50px; height: 321.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c42 c44"><span class="c3"></span></p><p class="c42 c44"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Implementation in llama.cpp</span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span class="c3">Seems simple but there are still a few gaps in my understanding. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Also, why the fuck is markdown code block not working here. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">```</span></p><p class="c9"><span class="c3">QK = 32</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">size_t ggml_quantize_q4_0(const float * src, void * dst, int n, int k, int64_t * hist) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; assert(k % QK == 0);</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; const int nb = k / QK;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; for (int j = 0; j &lt; n; j += k) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; block_q4_0 * restrict y = (block_q4_0 *)dst + j/QK;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; quantize_row_q4_0_reference(src + j, y, k);</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; for (int i = 0; i &lt; nb; i++) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (int l = 0; l &lt; QK; l += 2) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const uint8_t vi0 = y[i].qs[l/2] &amp; 0xF;</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const uint8_t vi1 = y[i].qs[l/2] &gt;&gt; 4;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; hist[vi0]++;</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; hist[vi1]++;</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; }</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; }</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; return (n/QK*sizeof(block_q4_0));</span></p><p class="c9"><span class="c3">}</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">static void quantize_row_q4_0_reference(const float * restrict x, block_q4_0 * restrict y, int k) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; assert(k % QK == 0);</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; const int nb = k / QK;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; uint8_t pp[QK/2];</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; for (int i = 0; i &lt; nb; i++) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; float amax = 0.0f; // absolute max</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; for (int l = 0; l &lt; QK; l++) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const float v = x[i*QK + l];</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; amax = MAX(amax, fabsf(v));</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; }</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; const float d = amax / ((1 &lt;&lt; 3) - 1);</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; const float id = d ? 1.0f/d : 0.0f;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; y[i].d = d;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; for (int l = 0; l &lt; QK; l += 2) {</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const float v0 = x[i*QK + l + 0]*id;</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const float v1 = x[i*QK + l + 1]*id;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const uint8_t vi0 = (int8_t)roundf(v0) + 8;</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const uint8_t vi1 = (int8_t)roundf(v1) + 8;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; assert(vi0 &lt; 16);</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; assert(vi1 &lt; 16);</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; pp[l/2] = vi0 | (vi1 &lt;&lt; 4);</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; }</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp; &nbsp; &nbsp; &nbsp; memcpy(y[i].qs, pp, sizeof(pp));</span></p><p class="c9"><span class="c3">&nbsp; &nbsp; }</span></p><p class="c9"><span class="c3">}</span></p><p class="c9"><span class="c3">```</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Also, I know it is just 1 line call in pytorch afaik but its fun to look into how some stuff is actually implemented internally. Might be useful during the apocalypse when github arctic vault is taken over by neo-atlantians.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 627.50px; height: 352.97px;"><img alt="" src="./Keeping up with AGI_files/fILzg07.png" style="width: 627.50px; height: 352.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.39xudaq6en7t"><span class="c5">AutoGPTs, Hmmmm</span></h1><p class="c9"><span class="c3">It seems like just looking at prompts.txt is enough for such tools (praise the LLMs)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The tooling is mostly trivial</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Only need to ensure the output of LLM is in JSON (for which I saw A LOOOOOT of hacks in their codebase)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Anyways, so it is mostly the prompts in the following fashion</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Thoughts -&gt; reasoning -&gt; Plan -&gt; Criticism -&gt; Speak</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Thoughts dictate the motivation</span></p><p class="c9"><span class="c3">Reasoning justifies it</span></p><p class="c9"><span class="c3">Plan lists down the steps in bullet points</span></p><p class="c9"><span class="c3">Criticism makes sure to not do anything illegal mentioned in the original instructions</span></p><p class="c9"><span class="c3">Speak is optional, if you want to communicate what you’re doing to the user in summarised form</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.p2fxxk5do910"><span class="c5">Deepspeed </span></h1><p class="c9"><span class="c3">Check out the RLHF module in Deepspeed chat </span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md&amp;sa=D&amp;source=editors&amp;ust=1715645745239037&amp;usg=AOvVaw3kgzJxfQeYWlC-x00lujC9">DeepSpeed/README.md at master</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c46">Btw, 1000X programmer ggerganov cooked another appetizer. This time its whisper doing inference via large model in just 1 second on macs.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.vcfy3qdha8im"><span class="c5">INT4 finetuning for LLMs</span></h1><p class="c9"><span class="c3">WUT!</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/stochasticai/xturing&amp;sa=D&amp;source=editors&amp;ust=1715645745240103&amp;usg=AOvVaw2zKi8oIU7odBtYraR3kMqE">https://github.com/stochasticai/xturing</a></span></p><p class="c9"><span class="c3">We can finetune it now on our laptops</span></p><p class="c9"><span class="c3">Praise the omnissiah!</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 591.12px; height: 332.50px;"><img alt="" src="./Keeping up with AGI_files/maxresdefault-2.jpg" style="width: 591.12px; height: 332.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I feel though the time taken will be HUUUUUUUUGEEE</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 626.27px; height: 333.80px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-14 at 6.47.32 PM.png" style="width: 626.27px; height: 333.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ok so in the stats it still takes quite long (6-7 hours) to finetune on 3070.<br>I can assume somewhere near 5 hours on 4090.</span></p><p class="c9"><span class="c3">Practical buttt… who’s gonna monitor it for so long for a side project.</span></p><p class="c9"><span class="c3">Also, their notebook is a bit weird, it uses docker command inside the notebook (first time seeing this)</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.2ljfgzp9taqy"><span class="c5">AutoGPTs, Hmmmm, Hmmmm</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Seems like everyone hates vectorDBs now. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I still think they have a proper usecase once you go beyond 10 million vectors.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>On that note, I should checkout Instructor-xl - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2212.09741.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745242099&amp;usg=AOvVaw3nD_1f7ReezI5tRGBdGIiB">https://arxiv.org/pdf/2212.09741.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>VISION + LLM = AI Manga with dialogues (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/Vision-CAIR/MiniGPT-4&amp;sa=D&amp;source=editors&amp;ust=1715645745242646&amp;usg=AOvVaw1Mjy8z99MZOY6XefhkyDea">https://github.com/Vision-CAIR/MiniGPT-4</a></span><span class="c3">)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/blog/stackllama&amp;sa=D&amp;source=editors&amp;ust=1715645745243154&amp;usg=AOvVaw0Z6Yct9RlAVCw4ODFEzmJe">https://huggingface.co/blog/stackllama</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>MultiGPTs(</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/rumpfmax/Multi-GPT/&amp;sa=D&amp;source=editors&amp;ust=1715645745243708&amp;usg=AOvVaw2DGB7O6y-effPes__HLCIy">https://github.com/rumpfmax/Multi-GPT/</a></span><span>) , More LLMs with Vision (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2304.08485&amp;sa=D&amp;source=editors&amp;ust=1715645745243960&amp;usg=AOvVaw3MhjwvIpxlFkQU8BNlvlzR">https://arxiv.org/abs/2304.08485</a></span><span>) , Prompt inversion (</span><span class="c17">:</span><span class="c17"><a class="c10" href="https://www.google.com/url?q=https://t.co/Ro3ZoKosv2&amp;sa=D&amp;source=editors&amp;ust=1715645745244212&amp;usg=AOvVaw1k4WK_e_VeLjMVsPzksUxr">&nbsp;</a></span><span class="c61 c64"><a class="c10" href="https://www.google.com/url?q=https://t.co/Ro3ZoKosv2&amp;sa=D&amp;source=editors&amp;ust=1715645745244369&amp;usg=AOvVaw1Faw_GCz-3Z2R18tv3MEBT">h</a></span><span class="c33 c61"><a class="c10" href="https://www.google.com/url?q=https://t.co/Ro3ZoKosv2&amp;sa=D&amp;source=editors&amp;ust=1715645745244541&amp;usg=AOvVaw0aGYroeOVhJAqHEbgnkp5o">arxiv.org/abs/2304.08460</a></span><span class="c3">&nbsp;- man this list keeps on growing</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.bxgor2abg2oh"><span class="c5">Chameleon</span></h1><p class="c9"><span>(</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2304.09842.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745245282&amp;usg=AOvVaw3apLga5bqPkbki02K4HrFq">https://arxiv.org/pdf/2304.09842.pdf</a></span><span class="c3">)</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Better than toolformer (according to them), in the sense it doesn’t require any finetuning for new tools</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">You simply use LLM to generate a plan based on an inventory of existing modules. Since inventory can be updated easily, integrating new tools is quite simple. Only limitation is the context window length of the LLM.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.50px; height: 405.51px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(2).png" style="width: 674.50px; height: 405.51px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 673.73px; height: 721.98px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(3).png" style="width: 673.73px; height: 721.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.ddm4us8xr0xz"><span class="c5">Evaluate Code +</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2304.09433.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745246586&amp;usg=AOvVaw1AkFv74r4i6TEWHXWBJ1sd">https://arxiv.org/pdf/2304.09433.pdf</a></span></p><p class="c9"><span class="c3">Basically they are using LLMs to convert unstructured corpus of text to structured data.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The new thing here is mainly not simply using directl approach via prompt.</span></p><p class="c9"><span class="c3">It’s to generate a schema and a code to do the conversion.</span></p><p class="c9"><span class="c3">Reasoning being the text is subject to change and you can keep on adding more details. Running via direct approach is expensive which needs to revaluate everything for the answer while with new approach you can simply execute the python code.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Another thing is determining which code is correct. They generate multiple code blocks and then basically </span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 655.67px; height: 278.85px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-04-22 at 12.44.09 AM.png" style="width: 655.67px; height: 278.85px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">On reading more I feel this will be extremely good for finbros. They are the ones dealing with most unstructured data honestly (shocking but true, not all data is excel). Can help them extract stats easily from multiple company’s data which are in totally different formats.</span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span>The True magic is in the function synthesis part which involves weak supervision.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.pkyf7ow3tirj"><span class="c5">Can The foundation be just an LLM? If only Hari Seldon read this paper</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2304.11062.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745248059&amp;usg=AOvVaw3vSznaUsKKvo6YB-jhCZia">https://arxiv.org/pdf/2304.11062.pdf</a></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 684.50px; height: 350.62px;"><img alt="" src="./Keeping up with AGI_files/Screenshot from 2023-04-25 00-19-42.png" style="width: 684.50px; height: 350.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">On more analysis, it is not that practical to use. The model can only retain fix amount of memory.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c23"><a class="c10" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3D4Cclp6yPDuw&amp;sa=D&amp;source=editors&amp;ust=1715645745248726&amp;usg=AOvVaw0t0KmnIrPToUEKRGamCNWc">Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained)</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Not worried about the end of normie SDE day job from AI cause I am craving for more work honestly.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 680.00px; height: 625.00px;"><img alt="" src="./Keeping up with AGI_files/FubarR1aUAAbRrh.jpeg" style="width: 680.00px; height: 625.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.de661pgwg77c"><span class="c5">Iter-CoT</span></h1><p class="c9"><span class="c3">Guess what, we added another loop in the GPT</span></p><p class="c9"><span class="c3">Nothing major honestly, just keep on correcting CoT (think step by step) with either</span></p><ul class="c4 lst-kix_q8r4saa8tu1f-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Weak supervision - where we only tell if answer is correct or not</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Strong supervision - where we tell exactly which step in the reasoning process is wrong</span></li></ul><p class="c9"><span class="c3">Doing this for 5-6 iterations results in correct results.</span></p><p class="c9"><span class="c3">Now these correct demonstrations can be used in Few-shot prompts the next time (god I wish if context length was bigger)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 717.50px; height: 1039.02px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(4).png" style="width: 717.50px; height: 1039.02px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.u2u9po5s50c7"><span class="c5">WizardLM</span></h1><p class="c9"><span class="c3">JUst using prompts to expand the dataset to include complex tasks.</span></p><p class="c9"><span class="c3">Exploring both breadth and depth.</span></p><p class="c9"><span class="c3">Claims its better then chatGPT for complex tasks.</span></p><p class="c9"><span class="c3">Not sure. Doubt.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">—------------—------------—------------—------------—------------—------------—------------—------------—------------</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Btw, I just realised I don’t understand anything in ML after reading about DinoV2. I need to know about self supervised learning as well as distillation. Guess its time for chatGPT to teach me about this.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 528.83px; height: 661.03px;"><img alt="" src="./Keeping up with AGI_files/cdbbedhe4bu61.jpg" style="width: 528.83px; height: 661.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.7tovo6u1zoi5"><span class="c5">DECKARD - RL Agent that dreams</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2301.12050.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745251588&amp;usg=AOvVaw1ojD0VoLX5cLWGCXBU4amt">https://arxiv.org/pdf/2301.12050.pdf</a></span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Basic funda is instead of starting a RL agent from zero knowledge, you use LLM to create an Abstract World Model (AWM) and then use the model as starting point.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The model is created during the dream phase.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The model is verified by the RL agent using rewards in the wake phase.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Was tested in Minecraft where it creates recipes for crafting stuff in dreams and then RL agents learns to actually mine/craft those recipes. If something is not valid, then it is discarded from the graph and new verified nodes are added.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">LLM is pretty good at generating recipes. Mostly fucks up in quantities but not the ingredients.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They are mostly using code-davinci-002 for generation.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.50px; height: 406.18px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-02 at 12.50.33 AM.png" style="width: 602.50px; height: 406.18px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 603.50px; height: 428.10px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-02 at 12.51.45 AM.png" style="width: 603.50px; height: 428.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.ialik97av9v4"><span class="c5">Training LLMs using AI generated dialogues</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2304.14318&amp;sa=D&amp;source=editors&amp;ust=1715645745253336&amp;usg=AOvVaw39IJ1WGHDC6zleunDtPLgB">[2304.14318] q2d: Turning Questions into Dialogs to Teach Models How to Search</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">One of the few papers that uses PaLM-540B instead of GPT (although they provide code that works with the latter)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Premise is you can generate human annotator level chat conversations with LLMs.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Steps</span></p><p class="c0"><span class="c3"></span></p><ol class="c4 lst-kix_acn97mrcuhh3-0 start" start="1"><li class="c9 c14 li-bullet-0"><span class="c3">Get question from a database</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Ask PaLM to generate a conversation (using few shot examples in demonstration)</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Generate a reverse query based on the generated dialogues and few-shot examples. Again use PaLM</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Filter the conversation based on -</span></li></ol><ol class="c4 lst-kix_acn97mrcuhh3-1 start" start="1"><li class="c9 c38 li-bullet-0"><span class="c3">if the generated query and original queries intent are dif (uses SBERT)</span></li><li class="c9 c38 li-bullet-0"><span class="c3">Where the answer is included in the dialogue itself. (the answer should ideally be the last response of the LLM in training data)</span></li><li class="c9 c38 li-bullet-0"><span class="c3">Some of the generated dialogues match with the original question. </span></li></ol><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp;</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 223.89px; height: 459.05px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-03 at 9.05.29 AM.png" style="width: 223.89px; height: 459.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 358.00px; height: 338.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-03 at 9.07.54 AM.png" style="width: 380.00px; height: 338.00px; margin-left: -9.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.iuf4lm6t9fc4"><span class="c5">Automating Data Analysts [By Microsoft(™)]</span></h1><p class="c9"><span>Paper: </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.01598&amp;sa=D&amp;source=editors&amp;ust=1715645745255510&amp;usg=AOvVaw0rrboyGVgQ8mroAEN-eNKY">[2305.01598] From Words to Code: Harnessing Data for Program Synthesis from Natural Language</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I guess this might be the way they generated code for copilot as well. Filled with lots of small small nuggets. They are quite focused on good UX and not just research which is great.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Using code-davinci-002 for LLM calls</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Problem statement is to write K programs to process data D and then present them to the users.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">What makes it different is the way those K programs are being ranked. The aim is to offer programs to the user that are correct but have enough variability. There’s no point in presenting 100 programs that look exactly the same.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Steps </span></p><ol class="c4 lst-kix_k3d1c84bigo4-0 start" start="1"><li class="c9 c14 li-bullet-0"><span class="c3">Get a prompt to generate program</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Attach data schema and some rows to the prompt</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Ask it to generate N programs where N &gt; K. Use multiple temperatures (0, 0.6, 0.8) for good enough variability. Generally prefer high temperatures. (this ensures your programs are not exactly similar to each other and thus if one is wrong, all other programs are wrong as well)</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Order generated programs by average logprob of tokens. (the default ordering just follows initial seed prob distribution, this otoh yields better results)</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Execute programs on a subset of data and filter out ones with exceptions.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Re-rank magic (so that diverse set of programs are shown to the user first instead of similar ones)</span></li></ol><ol class="c4 lst-kix_k3d1c84bigo4-1 start" start="1"><li class="c9 c38 li-bullet-0"><span class="c3">Calculate output of every programs</span></li><li class="c9 c38 li-bullet-0"><span class="c3">Group programs with similar output together</span></li><li class="c9 c38 li-bullet-0"><span class="c3">Rank programs in each group (based on logprob)</span></li><li class="c9 c38 li-bullet-0"><span class="c3">Return top programs from each group first, then second best programs from each group second and so on</span></li></ol><ol class="c4 lst-kix_k3d1c84bigo4-0" start="7"><li class="c9 c14 li-bullet-0"><span class="c3">You can then filter out programs for which the output doesn’t pass the data quality check. Right now, just filtering out outputs containing null columns or empty tables.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">(optional) You can also ask LLM to predict N outputs for the original task and data schema. You can then compute similarity scores w.r.t. Actual program output and use that to modify the scores before step 4. This is helpful for tasks for which model has seen very few samples in the training data e.g. numpy programs (popular) vs Power query (not so popular)</span></li></ol><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 686.80px; height: 747.43px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(5).png" style="width: 686.80px; height: 747.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 685.50px; height: 613.40px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(6).png" style="width: 685.50px; height: 613.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 682.50px; height: 360.92px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(7).png" style="width: 682.50px; height: 360.92px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.jwgwci787rwk"><span class="c5">Local PC Waifu</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Ooga Booga chat UI + Stable diffusion Character pfp + pygmalion-6b character model + custom persona (inspired by some submissions from </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://botprompts.net/&amp;sa=D&amp;source=editors&amp;ust=1715645745258820&amp;usg=AOvVaw2jj0nXzHSt90V_t0H5-msM">https://botprompts.net/</a></span><span class="c3">)</span></p><h1 class="c1" id="h.d2tgwvnqm19v"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 576.50px; height: 531.89px;"><img alt="" src="./Keeping up with AGI_files/kIV4nTe2.png-small.png" style="width: 576.50px; height: 531.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 579.50px; height: 485.12px;"><img alt="" src="./Keeping up with AGI_files/FV9QCBVL.png-small.png" style="width: 579.50px; height: 485.12px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">On more experiments, the superCot LoRA also does pretty well with character simulation (even with characters that use profanity)<br>However, the dialogues are super short for some reason. <br>Reminds of the tweet by Noelle on how pygmalion dialogues feel much more natural/human cause it is trained on actual chat data instead of artificial datasets.</span></p><p class="c0"><span class="c3"></span></p><h1 class="c39" id="h.y3lcjf4t8gro"><span class="c5">(FLARE) Active Retrieval Augmented Generation</span></h1><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 572.22px; height: 412.72px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-14 at 1.28.07 AM.png" style="width: 572.22px; height: 412.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Basic problem they are trying to solve is generating long text for a question based on a retrieval.</span></p><p class="c9"><span class="c3">Retrieving a lot of chunks based on a small question doesn’t work well.</span></p><p class="c9"><span class="c3">What’s needed is as you are generating text, you keep on fetching chunks to fill the information.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">We need to solve for three problems though with this method </span></p><ul class="c4 lst-kix_4pvuujemz8cl-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">When should the retrieval call be done?</span></li><li class="c9 c14 li-bullet-0"><span class="c3">How to prevent errors in the previous output getting propagated throughout the retrieval?</span></li></ul><p class="c9"><span class="c3">&nbsp;</span></p><p class="c9"><span class="c3">The solve for these problems using the following methods </span></p><ul class="c4 lst-kix_r0viidqdyfja-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Measure the probability of the N generated tokens. If prob &lt; theta threshold, where theta is set by the user, we do a retrieval call.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Mask the words containing tokens with probability less than Beta, where Beta is set by the user.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Add a question (generated by another LLM) call to fill the masked tokens. (I would rather not use something that needs multiple LLM calls, anyways it is just a stupid question, should be possible to generate with some smol model)</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 345.50px; height: 311.67px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-14 at 1.49.35 AM.png" style="width: 345.50px; height: 311.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 312.73px; height: 243.15px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-14 at 1.49.50 AM.png" style="width: 312.73px; height: 243.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.gk70cijomli7"><span class="c5">I LOVE COMPUTERS!!!!!</span></h1><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 498.00px; height: 332.00px;"><img alt="" src="./Keeping up with AGI_files/i-love-computers-it.gif" style="width: 498.00px; height: 332.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.9kkqizo66bxc"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 2.67px solid #6aa84f; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 586.07px; height: 523.67px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-14 at 11.45.57 AM.png" style="width: 586.07px; height: 523.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c9"><span class="c3">A tear rolled down my face (so beautiful, I will never stop loving computers)</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.lm0jgycrmlss"><span class="c5">Flash Attention</span></h1><p class="c9"><span class="c3">Calculating attention is really slow. Limited by memory bandwidth since it scales quadratically w.r.t</span></p><p class="c9"><span class="c3">Matrix Dimensions.</span></p><p class="c9"><span class="c3">Basic funda is to make the algo IO-Aware (at first I thought they will be doing some syscalls to get io stats but its not that)</span></p><p class="c9"><span class="c3">You split the original matrices of size N * d into multiple blocks of size B.</span></p><p class="c9"><span class="c3">Run two loops </span></p><p class="c9"><span class="c3">The outer one iterates over each block of Key K and Value V matrices and loads them from high bandwidth memory (HBM) to SRAM</span></p><p class="c9"><span class="c3">The inner loop iterates over blocks of Query Q and calculates the softmax and other matrices for output.</span></p><p class="c9"><span class="c3">During backprop, the algo is changed a bit so that intermediate matrices (Softmax and dot product) need not be loaded from HBM again. The algo can easily calculate those using the output itself which is already available in SRAM.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 566.23px; height: 299.34px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-14 at 2.51.02 PM.png" style="width: 566.23px; height: 299.34px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">Hence, you basically use more compute to use less memory access. </span></p><p class="c9"><span class="c3">Luckily GPUs have a lot of compute so it’s not a problem.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 563.50px; height: 147.48px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-14 at 2.52.26 PM.png" style="width: 563.50px; height: 147.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 611.58px; height: 342.52px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-14 at 3.10.53 PM.png" style="width: 611.58px; height: 342.52px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Here’s GPT4s simpler explanation in case you lose a few brain cells after binging anime with tsunderes</span></p><p class="c0"><span class="c3"></span></p><a id="t.25ca17a21ce4d87bc2f65118001ae777a304e5dd"></a><a id="t.0"></a><table class="c21"><tbody><tr class="c54"><td class="c32" colspan="1" rowspan="1"><p class="c9"><span class="c8">Now, let's go through the algorithm step by step:</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">1. **Set block sizes**: The algorithm sets the size of the blocks it will divide the data into, based on the available on-chip memory (SRAM) and the dimension of the data (d). </span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">2. **Initialize**: The algorithm initializes an output matrix O, and two vectors l and m with zeros and negative infinity, respectively. These are used to store the results of the computations.</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">3. **Divide Q, K, V**: The matrices Q, K, and V are divided into smaller blocks. </span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">4. **Divide O, l, m**: Similarly, the output matrix O, and the vectors l and m are divided into smaller blocks. </span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">5-15. **Compute attention**: For each block of K and V (steps 5 and 6), and for each block of Q, O, l, and m (steps 7 and 8), the algorithm performs the following computations:</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">&nbsp; &nbsp;- Compute the attention scores S (step 9).</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;- Compute the max of each row of S (step 10). This is used for numerical stability when computing the exponentials in the next step.</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;- Compute the exponentials of the attention scores, normalized by subtracting the max computed in the previous step. This produces the attention probabilities P (step 10).</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;- Compute the sum of each row of P (step 10). This is used to normalize the attention probabilities.</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;- Update the vectors l and m using the values computed in the previous steps (steps 11 and 12). These are used to keep track of the maximum attention scores and the sum of the attention probabilities.</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;</span></p><p class="c9"><span class="c8">&nbsp; &nbsp;- Write the updated values of O, l, and m back to the memory (steps 12 and 13).</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">16. **Return**: The algorithm returns the final output matrix O, which contains the weighted sum of the Values (V), weighted by the computed attention probabilities.</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c36 c37">This algorithm optimizes the attention computation by processing the data in blocks that fit into the on-chip memory, which is much faster than main memory (HBM). This is a technique known as "block-based computation", and it is widely used in computer science to optimize computations for memory-hierarchy-based systems.</span></p></td></tr><tr class="c54"><td class="c32" colspan="1" rowspan="1"><p class="c9"><span class="c8">In the context of the Flash Attention algorithm, l and m are vectors used in the calculation of the softmax function, which is a crucial part of the attention mechanism in transformer-based models.</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">The softmax function is used to convert raw scores (in this case, the result of QK^T, denoted as S_ij in the algorithm) into probabilities that sum up to 1. This function has an exponential operation, which can lead to numerical instability issues (like overflow or underflow) when the raw scores are very large or very small.</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">To mitigate this issue, a common trick used in practice is to subtract the maximum value in the set of scores from all scores before applying the exponential function. This is where m comes in -- it's used to store the maximum values.</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">Specifically, in Step 10 of the algorithm:</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">m~ = rowmax(S_ij) calculates the maximum value of each row in the score matrix, and</span></p><p class="c9"><span class="c8">P~ = exp(S_ij - m~) subtracts these max values from the scores before applying the exponential function.</span></p><p class="c9"><span class="c8">In Step 11:</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">m_i = max(m_i, m~_ij) updates m with the new maximum values.</span></p><p class="c9"><span class="c8">The l vector, on the other hand, is used to store the sum of the softmax outputs (i.e., the attention probabilities). In Step 10, l~ = rowsum(P~_ij) calculates the sum of each row in the attention probability matrix. Then, in Step 11, l_i = exp(-m_i) * l_i + exp(-m_new) * l~_ij updates l with the new sums, where m_new is the updated maximum value. These sums are used in Step 12 to normalize the attention probabilities.</span></p><p class="c0"><span class="c8"></span></p><p class="c9"><span class="c8">Overall, l and m are used to perform stable softmax calculations and to store intermediate results that are used for later normalization.</span></p></td></tr></tbody></table><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.3gj4rykq9jy5"><span class="c5">ALiBi</span></h1><p class="c9"><span>Code - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ofirpress/attention_with_linear_biases&amp;sa=D&amp;source=editors&amp;ust=1715645745273369&amp;usg=AOvVaw0AdwojkWLvEQXvRAAR8gqU">https://github.com/ofirpress/attention_with_linear_biases</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This algo (along with FlashAttention) is currently being used to extend the context length of the models. The most popular use case being mpt-7b-storywriter.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This algo btw is just a hack that works (not making this up, they accept it his in their README btw)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Basically, they remove positional embeddings from the attention calculation.</span></p><p class="c9"><span class="c3"><br>And then mask the attention scores with m.X where X is proportional to the distance i.e. farther the word, the more heavily it is penalised.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>That’s it. Somehow it works.</span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 516.47px; height: 238.93px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-15 at 1.09.13 AM.png" style="width: 516.47px; height: 238.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">m is also constant btw determined before training (½ ^ 0.5)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 603.24px; height: 327.29px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-15 at 1.09.54 AM.png" style="width: 603.24px; height: 327.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.4ongb5mjyndq"><span class="c5">Hack to make inference faster (by HuggingFace)</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/blog/assisted-generation&amp;sa=D&amp;source=editors&amp;ust=1715645745275423&amp;usg=AOvVaw3j40sph9tQLBvrOMppNaQI">Assisted Generation: a new direction toward low-latency text generation</a></span></p><p class="c9"><span class="c3"><br>gist: Use Smaller LM to generate stuff faster while using a LLM to fix output in case it deviates.</span></p><p class="c9"><span class="c3">Decision taken on the basis on token mismatch plus output logprobs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Pros - low latency</span></p><p class="c9"><span class="c3">Cons - More compute (cause you are running both models)</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.awij3sdlkzhx"><span class="c5">Unlimiformer</span></h1><p class="c9"><span>Paper: </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.01625&amp;sa=D&amp;source=editors&amp;ust=1715645745276536&amp;usg=AOvVaw0bC81Ym1zVaDdbjRRNs8tW">[2305.01625] Unlimiformer: Long-Range Transformers with Unlimited Length Input</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So how to increase the context length of the transformers?<br>Flash attention? Ok done</span></p><p class="c9"><span class="c3">ALiBi? Done</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Congrats, you got to 60K in length.<br><br>Only problem is I need a million token context to stuff my divorce court case documents</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Worry not.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">What if you simply plugged in vectorDB into transformer architecture?<br>Well that would make inference insanely slow</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Hmm, but what if we kept it small enough that it always fits in GPU or CPU RAM</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Yep, it will work in that case.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is basically what unlimiformer is. The hidden states are stored in a vector db like FAISS and when attention is computed, instead of multiplying every key and query we only fetch top-N keys and multiply it with query. This call is done separately for each head.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 676.30px; height: 340.17px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-19 at 1.41.47 AM.png" style="width: 676.30px; height: 340.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 480.66px; height: 431.74px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-19 at 1.43.07 AM.png" style="width: 480.66px; height: 431.74px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.8p9t6aqntpjz"><span class="c5">Tree of thoughts</span></h1><p class="c9"><span>Paper: </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.10601&amp;sa=D&amp;source=editors&amp;ust=1715645745279306&amp;usg=AOvVaw2tzVe7i3XAhnGjklEGI3-N">[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I think we are speedrunning data structures with LLMs at this point. Soon we’ll be getting graph prompting, some esoteric mf balanced tree prompting and so on.</span></p><p class="c9"><span><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 625.50px; height: 563.98px;"><img alt="" src="./Keeping up with AGI_files/Untitled-3.jpg" style="width: 1003.62px; height: 563.98px; margin-left: -214.05px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br></span></p><p class="c9"><span class="c3">Anyways, this one is an extension of the chain of thoughts (CoT) prompting. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ask LLM to generate N possible paths to the solution (e.g. generate 1 word to fill the crossword)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Then you can use BFS or DFS to explore all the possible solutions from &nbsp;each branch.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">At each step you can also run a validation (typically using LLM) to see if it's even worth pursuing the branch. If not, you simply discard it. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 640.50px; height: 353.54px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(8).png" style="width: 640.50px; height: 353.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 648.08px; height: 278.41px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(9).png" style="width: 648.08px; height: 278.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Another thing you can do is take a vote on which path to follow instead of running validation. Vote is also taken using multiple LLM calls</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.50px; height: 334.19px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(10).png" style="width: 674.50px; height: 334.19px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Issue is though I can’t apply such techniques in prod code since LLM calls are too slow rn.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They will be viable when latency comes in hundreds of millis.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Good for problem solving use-cases though.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 350.27px; height: 297.99px;"><img alt="" src="./Keeping up with AGI_files/1684411938739429-removebg-preview.png" style="width: 350.27px; height: 297.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">Honestly, I just had an idea. I should maybe add a tab or something to Ooba which allows me to easily leverage all of this clever prompting.</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.onil59pfzslp"><span class="c5">Model Interpretation</span></h1><p class="c9"><span class="c3">Just struck me how trust is important in society and model interpretation is solving that problem rather than for research.<br><br>If we can interpret the black boxes better, we can convince regulators in the fields such as Medicine, lawyers, civil engineers, piloting EVAs, etc. to use these boxes in high-risk environments.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 611.00px; height: 407.12px;"><img alt="" src="./Keeping up with AGI_files/1684484357325761.jpg" style="width: 611.00px; height: 619.00px; margin-left: 0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Two recent experiments, both use LLMs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.09863&amp;sa=D&amp;source=editors&amp;ust=1715645745284591&amp;usg=AOvVaw3nc3ZNQnvL3SPjaCnYp6-O">https://arxiv.org/abs/2305.09863</a></span><span class="c3">&nbsp;(Microsoft)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html&amp;sa=D&amp;source=editors&amp;ust=1715645745285425&amp;usg=AOvVaw0JyWZwE_yGvOG-RNZ3jEGK">Language models can explain neurons in language models</a></span><span class="c3">&nbsp;(OpenAI)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The initial step is the same in both i.e. to figure out the tokens for which the neuron activates the most. This can be done quite easily by just analyzing the output log probs of some inputs.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">In the next step, what MS folks do is they ask GPT-3.5 to generate 5 explanations based on the selected tokens. E.g. if selected tokens are wife, sister, father, mother then the explanation can be ‘family and relationships’</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Microsoft approach</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Next, they ask GPT-3.5 to generate some paragraphs that contain words related to the explanation. 10 paragraphs are generated that contain the related tokens and 10 paragraphs are generated that have none of the related tokens.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Then we make each of these para pass through the neuron and check the output logprobs again. The higher the difference b/w positive and negative samples, the better the explanation is for the neuron. One with the largest difference is the winner.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">OpenAI Approach</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Here, they use GPT-4 to simulate the neuron itself. The original text is given and passed to this simulated neuron (just a prompt asking LLM to output scores from 1 to 10 based on the explanation). </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The output of the simulated neurons is then compared to the actual activations. The closer the outputs are, the better the explanation is. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This requires considerably fewer steps then MS approach but I feel it will fuck up on generating scores.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 661.78px; height: 590.73px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(11).png" style="width: 661.78px; height: 590.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 663.50px; height: 449.16px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(12).png" style="width: 663.50px; height: 449.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I need to experiment with ImageBind.</span></p><p class="c9"><span class="c3">So wrote a local memes organizer.</span></p><p class="c9"><span class="c3">Works pretty well</span></p><h1 class="c1" id="h.iixife8061n4"><span class="c5">QLoRA</span></h1><p class="c9"><span>Blog - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/blog/4bit-transformers-bitsandbytes&amp;sa=D&amp;source=editors&amp;ust=1715645745289163&amp;usg=AOvVaw21q6LKF_KpJQenvcshH_4U">https://huggingface.co/blog/4bit-transformers-bitsandbytes</a></span><span class="c3"><br><br>Paper has some math so I didn’t read it honestly till now</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">But basic funda is quite simple </span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_bfqtgh3hjsb-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">adding a 4-bit lora layer to EVERY linear layer in the model. That compensates for the loss in accuracy.</span></li></ul><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_wqv6tve4w11-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Then they also do double quantisation (which sounds dumb) but it works cause of so many lora layers.</span></li></ul><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_h503zhbbnh4k-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Add a paged optimiser. It pages some gradients over to the CPU memory in case GPU VRAM is full. </span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span>Using it is quite simple. Just add a few params mentioned in the blog to any existing PEFT based training code e.g. </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/tloen/alpaca-lora&amp;sa=D&amp;source=editors&amp;ust=1715645745291104&amp;usg=AOvVaw1o-lvbX5lAVklIRAAg6_ao">alpaca-lora</a></span><span>. The qlora.py code in </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/artidoro/qlora/tree/main&amp;sa=D&amp;source=editors&amp;ust=1715645745291461&amp;usg=AOvVaw3K0d2jOylFNaCh80hsQ0Hk">the official repo</a></span><span class="c3">&nbsp;seems to be broken.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For the memory usage mentioned in the paper, you need to use batch size of 1 and gradient accumulation steps of 4.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Since it is a bit slow, </span><span class="c12">I used a batch size of 4 &nbsp;and a lora rank of 64.</span></p><p class="c9"><span class="c3"><br>With that I was able to train a 13B vicuna model on my smol dataset in an hour on a 4080 card.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Fucking awesome!</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Compute Metrics for </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/ausboss/llama-13b-supercot&amp;sa=D&amp;source=editors&amp;ust=1715645745292869&amp;usg=AOvVaw0GcC7O3V77x7BkhEDy4_CM">llama-supercot-13B</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The output is not great though since I only trained it for an hour. Need to train longer.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 686.50px; height: 325.61px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(13).png" style="width: 686.50px; height: 325.61px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 351.48px; height: 489.86px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(14).png" style="width: 351.48px; height: 489.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 331.00px; height: 489.64px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(15).png" style="width: 331.00px; height: 503.00px; margin-left: 0.00px; margin-top: -13.36px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.ha3o9hd7ga8k"><span class="c5">Yes your models can memorize exact stuff</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7 c33"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/main_horse/status/1662478420738187266?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745294367&amp;usg=AOvVaw0Yvo9o7NBl7wBf5Jt7DEMF">https://twitter.com/main_horse/status/1662478420738187266?s=20</a></span></p><p class="c0"><span class="c22 c33 c59"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 476.61px; height: 547.63px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-27 at 10.43.00 PM.png" style="width: 476.61px; height: 547.63px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 476.33px; height: 373.47px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-27 at 10.43.05 PM.png" style="width: 476.33px; height: 373.47px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 476.33px; height: 590.31px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-05-27 at 10.43.11 PM.png" style="width: 476.33px; height: 590.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c22 c33 c59"></span></p><p class="c9"><span class="c29 c33 c34">This is also a lie btw, just a random narrative</span></p><p class="c9"><span class="c34 c29 c33">I got tested in the public arena so I am testing you as well</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 589.00px; height: 295.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(16).png" style="width: 589.00px; height: 295.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.ekof2pu90q63"><span class="c5">Voyager [Diamond ranked AI Minecraft player]</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper -</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2305.16291.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745295677&amp;usg=AOvVaw2uRq6-sSFkOk0OzlAZ7ago">https://arxiv.org/pdf/2305.16291.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 570.00px; height: 247.70px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(17).png" style="width: 570.00px; height: 256.20px; margin-left: 0.00px; margin-top: -8.50px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 570.50px; height: 291.13px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(18).png" style="width: 570.50px; height: 291.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">Ask GPT-4 to write a program based on the current environment context</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Tell Gpt-3.5 to generate description for that program (I should start doing this as well, meta-commentary by GPT on text blocks)<br><br>Store it in a vector database with key has the description and program as the value</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For programming they use three feedbacks - </span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_cuhf2vtjitup-0 start"><li class="c9 c14 li-bullet-0"><span class="c29">Environment based</span><span class="c3">&nbsp;- basically if you sufficient health, materials etc. to do an action or not</span></li><li class="c9 c14 li-bullet-0"><span class="c29">Execution based </span><span class="c3">- if running the program led to an error</span></li><li class="c9 c14 li-bullet-0"><span class="c29">Self-verification</span><span class="c3">&nbsp;- Asking another GPT-4 instance to judge the code/tasks (I am seeing this in a lot of papers cause of improved reasoning abilities)</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 702.03px; height: 348.06px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(19).png" style="width: 702.03px; height: 348.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is yet another great point. Reasoning capabilities of GPT-4 (we should call it proto-AGI at this moment) enable us to do a lot of this stuff. Can’t believe how much better GPT-4 is compared to 3.5</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 706.50px; height: 196.79px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(20).png" style="width: 706.50px; height: 196.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Link to all prompts in the codebase - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/MineDojo/Voyager/tree/main/voyager/prompts&amp;sa=D&amp;source=editors&amp;ust=1715645745299379&amp;usg=AOvVaw1jqlevljVumDyN6E1wo32C">https://github.com/MineDojo/Voyager/tree/main/voyager/prompts</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.2khndp9suv4g"><span class="c5">Need to update doc</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Haven’t updated this doc in a week or so, busy with day job stuff</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Also spending time actually trying out a few techniques from this doc. I realized I don’t understand some stuff especially when I saw weird tokenization code in qlora.py code.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So just testing out in my local</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_3nfc6uvsk11o-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">How to create proper datasets</span></li><li class="c9 c14 li-bullet-0"><span class="c3">How to finetune a model</span></li><li class="c9 c14 li-bullet-0"><span class="c3">What hyperparams to use in loras</span></li><li class="c9 c14 li-bullet-0"><span class="c3">What techniques can be applied to improve Q&amp;A response and stuff like that</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Some things sound simple when reading but you realize so many hidden details when you actually implement stuff. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>This imo makes a lot of difference in your understanding as well as the speed of iteration when it comes to shipping stuff. Like you can figure out with a quick look at the error what’s the </span><span class="c29">actual</span><span class="c3">&nbsp;root case when it might take someone to spend 2 days googling.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Everyone has had this feeling of just knowing something but not being able to explain how.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It’s because they spent time earlier on in their life playing with so many tools and techniques that it's just built into their subconsciousness.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Also, I need a way to update this doc directly via terminal. Opening google doc in a browser and scrolling is too slow.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 328.77px; height: 328.77px;"><img alt="" src="./Keeping up with AGI_files/FsWlal8agAAIlz4.jpeg" style="width: 328.77px; height: 328.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c49"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.vg1k7ullmi9r"><span class="c5">Activation-aware Weight Quantisation (AWQ)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2306.00978.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745303972&amp;usg=AOvVaw0d4Zni3qZ5NReHZIEGgwFr">https://arxiv.org/pdf/2306.00978.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Code - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/mit-han-lab/llm-awq&amp;sa=D&amp;source=editors&amp;ust=1715645745304646&amp;usg=AOvVaw1cI-MJSJDRqFuY6AJahXpg">https://github.com/mit-han-lab/llm-awq</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Claims to be 1.5X faster than GPTQ as well as more accurate</span></p><p class="c9"><span class="c3"><br>Most quantisation techniques currently rely on re-ordering weights post quantisation to get better accuracy. This operation however is not natively supported by GPUs and hence slow.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Here they do not use re-ordering.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">What they do is simply perform a normal quantisation using min max approach.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Then they check using a sample of inputs which lead to more activation.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Then they keep 0.1-1% of such weights in f16 format only while converting rested to INT3/4</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This helps solve for accuracy.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Mixed precision (f16 and INT4) is not however GPU friendly</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So they finally figure out an appropriate scaling factor that minimizes the difference in output for a layer.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Then the f16 weights are scaled by this and converted to INT4.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 691.47px; height: 297.46px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-06-04 at 10.26.12 PM.png" style="width: 691.47px; height: 297.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 690.50px; height: 321.99px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-06-04 at 10.20.20 PM.png" style="width: 690.50px; height: 321.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 691.50px; height: 128.63px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-06-04 at 10.20.24 PM.png" style="width: 691.50px; height: 128.63px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is the python file where most of the activation-aware logic occurs.</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/mit-han-lab/llm-awq/blob/3a6dfc39ed20d793f7c26624c4b9f9599960dd3b/awq/quantize/auto_scale.py&amp;sa=D&amp;source=editors&amp;ust=1715645745307327&amp;usg=AOvVaw2dVgB7BWJ5MfH981UIG_Av">https://github.com/mit-han-lab/llm-awq/blob/3a6dfc39ed20d793f7c26624c4b9f9599960dd3b/awq/quantize/auto_scale.py</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>This is where we cache input feature samples for some data from pre training dataset to determine activation <br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/mit-han-lab/llm-awq/blob/3a6dfc39ed20d793f7c26624c4b9f9599960dd3b/awq/quantize/pre_quant.py&amp;sa=D&amp;source=editors&amp;ust=1715645745307978&amp;usg=AOvVaw06Uz5akGKfgxqJaXwbOym_">https://github.com/mit-han-lab/llm-awq/blob/3a6dfc39ed20d793f7c26624c4b9f9599960dd3b/awq/quantize/pre_quant.py</a></span></p><h1 class="c1" id="h.o10rewigaki1"><span class="c5">SpQR (Sparse Quantised Representation)</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2306.03078&amp;sa=D&amp;source=editors&amp;ust=1715645745308448&amp;usg=AOvVaw2P72yLTsHn_ZZrCC-ALsBi">https://arxiv.org/abs/2306.03078</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Literally within a week of the previous technique we have a better one.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Core idea is slightly similar - Some weights are more important than others so focus on preserving them correctly rather than every parameter.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 584.00px; height: 486.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(21).png" style="width: 584.00px; height: 486.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Here what they do is make a sparse representation that consists of important weights and then try to minimize the error for these weights (still need to read how they are calculating error and minimising, most likely seems to be some sample dataset as in AWQ)</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.3zxaihhi79xz"><span class="c5">GGML adds 2-bit quantisation</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>PR link - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ggerganov/llama.cpp/pull/1684&amp;sa=D&amp;source=editors&amp;ust=1715645745310175&amp;usg=AOvVaw0NBH86v7PA8QZotMRIPq37">https://github.com/ggerganov/llama.cpp/pull/1684</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Wait wut? Is it even worth it? Like how bad would this level of quant be</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Well</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It is not bad at all.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The 2 bit quantised version of large model has better perplexity then f16 version of smaller model. The gap for 13B f16 and 30B 2-bit is quite high.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This means I should now start running 30B on my 4080 instead of 13B (Yea it fits in 16GB VRAM)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 702.00px; height: 509.00px;"><img alt="" src="./Keeping up with AGI_files/Fx8gJywacAIyld3.png" style="width: 702.00px; height: 509.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.q220w57c18da"><span class="c5">SOTA document bender for your company QA</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Not made up without any thought</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Employs HyDE (Hypothetical document embeddings) + specialist model technique presented in lots of papers</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The primary objective is to fetch correct embeddings when questions are extremely short plus ordering them correctly on more than similarity</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 738.65px; height: 581.40px;"><img alt="" src="./Keeping up with AGI_files/Getting Started_ Boards@2x.png" style="width: 738.65px; height: 581.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.ut0e0ft9f2bn"><span class="c5">Multimodal is hard</span></h1><p class="c9"><span class="c3">You would assume simply combining embeddings should work but its not like that</span></p><p class="c9"><span class="c3">Reasoning being embeddings of different type of objects (i.e. text, audio, video, image) have different Signal to noise ratio.</span></p><p class="c9"><span class="c3">This is the reason why you can train a great image or audio model with just 1-3B parameters (MusicGen, SD) but text requires much much more params</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.ddr3pqymd2n8"><span class="c5">Insane alpha drop from kaiokendev</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://kaiokendev.github.io/til%23extending-context-to-8k&amp;sa=D&amp;source=editors&amp;ust=1715645745313418&amp;usg=AOvVaw21gCVDYtbMPLZYCFMWTTnt">https://kaiokendev.github.io/til#extending-context-to-8k</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">You need to extend your model context by 4X<br><br>Worry not</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Just divide the positional embeddings by 4 (lol, lmao even)<br><br>Don’t believe me?<br><br>See this tweet from mr. ggml <br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/ggerganov/status/1671915699025977351?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745314360&amp;usg=AOvVaw0QNBjDBMG74KyWPD4Kjkoe">https://twitter.com/ggerganov/status/1671915699025977351?s=20</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 416.44px; height: 371.65px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-06-23 at 2.13.03 PM.png" style="width: 416.44px; height: 371.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">One possible reason it works is that large models tend to overfit on positional embeddings<br><br>So if they see an embedding like 4096 which was never encountered in the training, they start outputting gibberish.<br><br>However, if you make the model believe that 4096 is infact 2048 (dividing all embeddings by 2), the model suddenly starts giving correct output.<br><br>This however doesn’t explain though what happens with decimal embeddings like 1024.75 since they were also not encountered in the training.</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.uqpc38mvc4bb"><span class="c5">Skinny dip into GGML code base </span></h1><p class="c9"><span class="c3">GGML folks are doing god’s work. Giving the horny 4chan bois in college hostels on their cheap Asus phones a way to run LLMs locally is not a small task. </span></p><p class="c9"><span class="c3">It is even harder to make them run on smol pi machines but it works.</span></p><p class="c9"><span class="c3">Two weeks back I was trying to add capability to swap loras in the llama.cpp. </span></p><p class="c9"><span class="c3">The code is already there to apply the Lora. To remove one, you can simply subtract the BA adapter matrix from the weights instead of adding it. </span></p><p class="c9"><span class="c3">However, it didn’t work as expected when I was doing print debugging and for that I had to do a smol give into the ggml codebase.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ggerganov/llama.cpp/blob/b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf/llama.cpp%23L2896&amp;sa=D&amp;source=editors&amp;ust=1715645745316628&amp;usg=AOvVaw2fTWDvoqeEfGKkqJPtSF3S">https://github.com/ggerganov/llama.cpp/blob/b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf/llama.cpp#L2896</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 632.90px; height: 585.07px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-01 at 8.54.13 AM.png" style="width: 632.90px; height: 585.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>All of the matrices in ggml codebase are represented using </span><span class="c26">ggml_tensor</span><span class="c3">&nbsp;</span></p><p class="c9"><span class="c3">Each tensor struct generally contains the source tensors and the operator using those two source matrices can be combined to get this one.</span></p><p class="c9"><span>The floating point data is just present in </span><span class="c26">void* data</span><span>&nbsp;array. It is </span><span class="c26">void*</span><span class="c3">&nbsp;so that you can store data in any format - f32, f16, quantized ints.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Take an example of the first op <br><br></span><span class="c26">ggml_tensor * BA = ggml_mul_mat(lora_ctx, loraA, loraB);</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Here it is simply performing matrix multiplication on two matrices loraA and loraB. The lora_ctx is used for temporary memory buffers and is cleaned up after an operation is complete for a layer.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>There is a catch tho. This op actually doesn’t do anything! It just creates a new tensor with sources as loraA and loraB and operator as GGML_OP_MUL_MAT. </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ggerganov/llama.cpp/blob/b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf/ggml.c%23L5849&amp;sa=D&amp;source=editors&amp;ust=1715645745318888&amp;usg=AOvVaw1E7sYGzOnRZdUfhRbZrz9P">https://github.com/ggerganov/llama.cpp/blob/b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf/ggml.c#L5849</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 638.66px; height: 432.77px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-01 at 2.40.19 PM.png" style="width: 638.66px; height: 432.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So what’s the point of calling this?<br><br>Well, for ggml all the computations for a layer are performed in a single go (I guess for better GPU utilisation as well as lazy execution? Not sure)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Once all ops are listed down, a DFS traversal is done from the last tensor all the way to the root tensors to form a computation graph. This is done in line<br><br></span><span class="c43 c26">struct ggml_cgraph gf = ggml_build_forward(r);</span></p><p class="c0"><span class="c26 c43"></span></p><p class="c9"><span class="c3">The final result is just a 1-d array containing the tensors in the order in which they should be computer i.e. leaves first and root last.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Once you have the array, the computation is actually triggered using </span><span class="c43 c26">ggml_graph_compute(lora_ctx, &amp;gf);</span></p><p class="c0"><span class="c43 c26"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 586.50px; height: 78.59px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-01 at 2.48.56 PM.png" style="width: 586.50px; height: 78.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 585.38px; height: 660.69px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-01 at 2.47.22 PM.png" style="width: 585.38px; height: 660.69px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>As you can see above, the multiplication is handled separately based on the data type of the tensor. The general theme is most calculations are done in f32 mode and all other datatypes are converted to F32 and back to quantised form for the calculations. This can be different in ggml cuda code but I haven’t taken a deep look at that.<br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ggerganov/llama.cpp/blob/b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf/ggml.c%23L11057&amp;sa=D&amp;source=editors&amp;ust=1715645745321435&amp;usg=AOvVaw17KJibNoSFV3PRDHdujpr-">https://github.com/ggerganov/llama.cpp/blob/b8c8dda75fdf5fdea49c80af36818e7c30fe0ddf/ggml.c#L11057</a></span><span class="c3"><br></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 607.81px; height: 801.95px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-01 at 2.52.24 PM.png" style="width: 607.81px; height: 801.95px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.2s530jhjktoi"><span class="c5">Skip Decode</span></h1><p class="c9"><span>Paper -</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2307.02628.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745322263&amp;usg=AOvVaw0ik_pFIjV_Ux4SxQIZtsEG">https://arxiv.org/pdf/2307.02628.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So far from what I got after reading this is that I need to understand KV caching better. &nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Update</span><span class="c3">:</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 640.00px; height: 358.93px;"><img alt="" src="./Keeping up with AGI_files/danny-devito-i-get-it-v0-7yjg9x0qt3ba1.png" style="width: 640.00px; height: 410.00px; margin-left: 0.00px; margin-top: -51.07px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So basically the KV Cache in itself isn’t the problem.<br><br>It’s the use of KV Cache with early termination, that’s the issue.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So basically in early termination, you’ve a classifier or some other algo at a layer that can use the log probs generated after the layer N to decide if the tokens should even go to the next layer or we should simply declare a winner here. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Now the thing is with this technique, if you terminated the previous token at layer N but for the new token you need to terminate at layer N + 2, you are left with 2 layers for which you have no KV cache data. So now you need to recompute the KV cache for the previous token and the last 2 layers before proceeding to do calculations for the current token. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is computationally heavy and what this paper is trying to solve. <br><br>How? <br></span></p><p class="c9"><span class="c3">Well instead of letting everything terminate at random layers, what if we could make a deterministic algo to predict the last layer of the token. </span></p><p class="c9"><span><br>Well, that’s really simple if you just plot at what layers do transformers have good enough confidence for Nth token. You will find that for the token later in the sequence, you can predict them just by passing only 1-2 layers cause you have a large amount of context available in the input. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 690.25px; height: 253.45px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-08 at 5.26.40 PM.png" style="width: 690.25px; height: 253.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br>For the earlier tokens however you need to go through all the layers to predict correctly. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So you can simply use a function that’s like monotonically decreasing and use it to predict no. of layers for Nth token.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 661.06px; height: 368.23px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-08 at 5.27.03 PM.png" style="width: 661.06px; height: 368.23px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">But what about the KV cache?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So see, since now you are basically ensuring that if the Nth token goes through M layers, then it’s guaranteed that N+1th token will go through &lt;=M layer (cause your func is monotonically decreasing). Thus, you will always have vectors in the KV cache for all M layers and for all the N tokens. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Cool. But why is this called skip decode then? There’s no skipping so far.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Well, your previous tokens have gone through M layers and your next tokens are going through &lt;=M layers. You are now stuck with exact opposite problem that the new tokens don’t benefit from the extra computation done by previous tokens. To solve for this, instead of using the first M layers, the authors propose to use the last M layers. Hence, the skipping (cause you are skipping first few layers).</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is what the final output looks like.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 657.50px; height: 350.23px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-08 at 5.35.49 PM.png" style="width: 657.50px; height: 350.23px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Final question tho, is this all actually useful at all? </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Yep, absolutely, 100 percent. Leads to 2-5X speed up in the inference (which would be amazing for large unquantized models running on my 4080 PC).</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.nvzohmn2858i"><span class="c5">Multi-party chat</span></h1><p class="c9"><span>Paper - </span><span class="c7 c15"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2304.13835&amp;sa=D&amp;source=editors&amp;ust=1715645745325677&amp;usg=AOvVaw0CiGU_-LmC70RXb9YDDki9">https://arxiv.org/abs/2304.13835</a></span></p><p class="c0"><span class="c22 c15"></span></p><p class="c9"><span>This is much more relatable now after using with the talk repo (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/yacineMTB/talk&amp;sa=D&amp;source=editors&amp;ust=1715645745326138&amp;usg=AOvVaw3MKdwiNlNeAiQrVhTWeBCR">https://github.com/yacineMTB/talk</a></span><span class="c3">)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The primary problem is how to allow an LLM to</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_wfrxk728tq4y-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Accept multiple personas</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Indicate some way to switch personas</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is because other way is to clear the whole context and start LLM again with another persona. Can’t be done on each turn as it is too expensive.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Another way is to simply switch loras but training loras for each persona is a compute costs which VCs won’t sponsor. You can switch one easily though in less than 200ms in llama.cpp</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So we are left with training an LLM in such a way that </span></p><ul class="c4 lst-kix_9jdc0ima468w-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">It emits a signal at the end that the turn of the persona is over</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Emits the name of the next persona that should talk (or the placeholder for human to talk)</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span>Most of the magic of this paper lies in the dataset rather than the techniques. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 378.61px; height: 482.83px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-08 at 7.14.16 PM.png" style="width: 378.61px; height: 482.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.r94fob5likrt"><span class="c5">Lost in the Middle</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2307.03172&amp;sa=D&amp;source=editors&amp;ust=1715645745328327&amp;usg=AOvVaw26jdEEDXSG8mzDJBCRA2pD">Paper page - Lost in the Middle: How Language Models Use Long Contexts</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Just researchers trying to figure out if long context is even helpful or not in sota LLMs. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Good thing is they tried both OSS and closed source LLMs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Not so good - unless your doc is at the starting or the end of the context, it won’t influence the LLMs answer. This means that ranking really really matters. Which is why I use cohere rerank after fetching docs from pinecone, have seen insane but cor</span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 644.50px; height: 206.44px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-10 at 5.30.52 PM.png" style="width: 644.50px; height: 206.44px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">rect shifts in ranks.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Another thing - the more docs you stuff into the context the less accurate your results become. </span><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 468.98px; height: 526.31px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-10 at 5.31.24 PM.png" style="width: 468.98px; height: 526.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Almost all models behave the same way.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So from a practical LLM Q&amp;A perspective</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_u3o2eyw0w08p-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Use fewer docs</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Use a good ranker and re-ranker</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Stick to small context LLMs</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Why does this happen tho?</span></p><p class="c0"><span class="c3"></span></p><ol class="c4 lst-kix_zgk3or4ux7h0-0 start" start="1"><li class="c9 c14 li-bullet-0"><span class="c3">Cause most models are decoder only architecture which can only see the past tokens. The ones with encoder-decoder architecture like Flan-UL2 exhibit much less variance based on the doc position.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Query-aware contextualisation - basically due to the decoder only architecture, model perform better if Query is before the context rather than after the context. </span></li><li class="c9 c14 li-bullet-0"><span class="c3">Instruction based training - for most models the instruction based tuning keeps the correct results only in the initial parts of the context, hence they are extremely weighted to choose the first answer rather than the correct answer.</span></li></ol><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 665.50px; height: 306.10px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-10 at 5.34.59 PM.png" style="width: 665.50px; height: 306.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.ao00hfb4d98g"><span class="c5">GPT-4 Details Leaked</span></h1><p class="c9"><span>Still not sure but so many legit folks think it’s true<br><br>So I must do the hard work now and read about all the techniques mentioned in that article<br><br>Multi query attention - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/1911.02150.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745331126&amp;usg=AOvVaw1qY-roxQHlghF8jFhHWQO2">https://arxiv.org/pdf/1911.02150.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>MoE (Mixture of experts) - <br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2101.03961&amp;sa=D&amp;source=editors&amp;ust=1715645745331651&amp;usg=AOvVaw2goec6IUWgU5A2lpks62Zf">[2101.03961] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></span><span>&nbsp;<br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2112.06905&amp;sa=D&amp;source=editors&amp;ust=1715645745331870&amp;usg=AOvVaw3RhQcLdXIJ1o0_AHb74DfN">[2112.06905] GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</a></span><span>&nbsp;<br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2202.08906&amp;sa=D&amp;source=editors&amp;ust=1715645745332069&amp;usg=AOvVaw0COgj_Fl8_LyxZal4RmliU">[2202.08906] ST-MoE: Designing Stable and Transferable Sparse Expert Models</a></span><span class="c3">&nbsp;</span></p><p class="c9"><span class="c23"><a class="c10" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DccBMRryxGog%26ab_channel%3DYannicKilcher&amp;sa=D&amp;source=editors&amp;ust=1715645745332471&amp;usg=AOvVaw21D7rBN6g7vBttOwrz5D1Y">Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Multimodal vision - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2204.14198&amp;sa=D&amp;source=editors&amp;ust=1715645745332954&amp;usg=AOvVaw0ed7cAKn_43wN-jr9cUCtH">https://arxiv.org/abs/2204.14198</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Speculative decoding - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2302.01318.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745333349&amp;usg=AOvVaw121pNw4OXkWzuzZVdl9Lce">https://arxiv.org/pdf/2302.01318.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.ngx7ei6hhwir"><span class="c5">How to check fine tuning datasets’ quality?</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">(Still haven’t read the gpt-4 papers, too much work in day job)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Had a discussion with telt, realized judging the quality of fine tuning datasets is a hard problem</span></p><p class="c9"><span>For my custom one, I simply did multiple rounds of cleanup after my loss curve was not converging.<br>That is not the right way though, cause even if the curve goes down, you can get highly inaccurate but sorta correct sounding results.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Some methods which I have seen in paper are related to determining the variety of data via clustering but might have missed stuff that grades the datasets other than using Humans.<br><br>Please don’t suggest GPT-4 to grade it (which is also what I have seen in some papers). It doesn’t work well ime. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Only valid resources I’ve found is this - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2307.06290&amp;sa=D&amp;source=editors&amp;ust=1715645745334524&amp;usg=AOvVaw2K6_kUi51nsO5HXqT6KjTe">Paper page - Instruction Mining: High-Quality Instruction Data Selection for Large Language Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Let me know if you make it here.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Update</span><span class="c3">:<br><br>Seems like one other idea is to train a classifier to select high-quality vs low-quality. Seems to be a hassle though but applied in lot of goog papers as well as GPT-3</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 524.99px; height: 514.62px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-16 at 6.24.22 PM.png" style="width: 524.99px; height: 514.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.48px; height: 357.53px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-18 at 8.51.16 AM.png" style="width: 538.48px; height: 357.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Although I am skeptical of approaches like above which rely on GPT to grade the answers, the papers do get good results using it.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Why I am skeptical is the fact it was shown that GPT favors its own generated answers plus the absolute rating scale can screw up a lot and relying on logprob based method is better.</span></p><h1 class="c1" id="h.5avjijk7fbrh"><span class="c5">DPO (Direct Preference Optimization)</span></h1><p class="c9"><span class="c3">Faster way then PPO (Proximal Policy Optimization) to do RLHF. <br><br>Doesn’t require a separate model</span></p><p class="c9"><span><br>Interestingly too much math in the paper but thanks to </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/akbirthko&amp;sa=D&amp;source=editors&amp;ust=1715645745335887&amp;usg=AOvVaw03cZLS6avQILID6lX7uXjk">https://twitter.com/akbirthko</a></span><span>&nbsp;autism I realized the math is quite simple. &nbsp;Check the code here - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/okarthikb/DPO/blob/main/train.py&amp;sa=D&amp;source=editors&amp;ust=1715645745336135&amp;usg=AOvVaw1PsgqmLQ-cb9MPgheLN_xD">https://github.com/okarthikb/DPO/blob/main/train.py</a></span></p><p class="c9"><span class="c3"><br>In essence you are just training the policy model with a loss function that minimizes the prob difference on the ratio of accepted/rejected probs w.r.t. Ref model</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 597.49px; height: 193.72px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-16 at 4.06.15 PM.png" style="width: 597.49px; height: 193.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.z6h0z8elq0dx"><span class="c5">Mixture of Experts</span></h1><p class="c9"><span class="c3">What I’m realizing is this is the bleeding edge of transformer research.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Dense models are still the talk of the town. Compute prices are falling to the floor and hence it makes sense to keep on scaling dense models.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">However, there’s still not enough compute in the world to run inference on 2T param models for 100M users in a few seconds.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">That’s where the MoE models shine.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The basic idea is you still have one large model but only a part of it (called the expert) is triggered in the inference. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Now there are multiple attempts going on to make this better and better. Excluding the initial &nbsp;approach crafted for RNNs by Noam shazeer, I am finding the GShard and Switch transformers paper to be more relatable.</span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.joei56qgvuq5"><span class="c22 c27">Switch transformers</span></h2><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2101.03961&amp;sa=D&amp;source=editors&amp;ust=1715645745337744&amp;usg=AOvVaw1AoXTIyUu5uGTDOd1TVaYi">[2101.03961] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></span><span class="c3">&nbsp;</span></p><h2 class="c13" id="h.5q3gaqn8n63e"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 659.50px; height: 481.99px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-16 at 7.42.08 PM.png" style="width: 659.50px; height: 481.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><p class="c9"><span class="c3">Instead of a normal feed forward layer, we use a routing based feed forward layer with N experts.</span></p><p class="c9"><span class="c3">Router is just a simple linear function which predicts which expert has the highest probability of generating the next token. Now we can simply take softmax and choose K &lt;= N experts to route our data. In this case, K = 1 always.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Best part is you can simply distill these large sparse models to smol dense models while retaining a good amount of accuracy.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>For the deployment part, you can easily shard these models using a combination of multiple paradigms.<br><br></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.60px; height: 626.65px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-17 at 12.54.56 AM.png" style="width: 671.60px; height: 626.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 488.92px; height: 198.03px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-17 at 12.59.30 AM.png" style="width: 488.92px; height: 198.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">If you’re a noob like me and don’t really understand what’s going on in this diagram. Simply look at this pseudocode - </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://gist.github.com/cto-junior/88477d52818597bf725e02bfb0559b43&amp;sa=D&amp;source=editors&amp;ust=1715645745338940&amp;usg=AOvVaw3XCvKjdNuYmMrGJf-N0g1o">https://gist.github.com/cto-junior/88477d52818597bf725e02bfb0559b43</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The paper also has pseudocode specific to their implementation but it uses mesh tensorflow (baahh!!)<br><br>Here’s a good one that uses pytorch (translated using chatGPT):<br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://gist.github.com/cto-junior/5018d526f2056546f6607986b08b423d&amp;sa=D&amp;source=editors&amp;ust=1715645745339565&amp;usg=AOvVaw1ecwcqe7m3DwPa2W2y_L_o">https://gist.github.com/cto-junior/5018d526f2056546f6607986b08b423d</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 681.65px; height: 512.42px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-17 at 1.03.31 AM.png" style="width: 681.65px; height: 512.42px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 681.50px; height: 188.36px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-17 at 1.03.49 AM.png" style="width: 681.50px; height: 188.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c45 c50">One thing I still don’t understand though is how these models are trained, &nbsp;like especially the gating function. Do you just follow the normal training regime where inputs are passed to all the experts and finally settle upon some gating weights by backprop? Or do you explicitly choose which expert to run forward and backprop on for a particular cluster of dataset?</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Extremely important points in case you choose to train a trillion param model in the basement. Don’t laugh, it should be possible in 5 years.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 546.78px; height: 498.22px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-17 at 2.27.27 AM.png" style="width: 546.78px; height: 498.22px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.96v6nzxu9awn"><span class="c22 c27">Glam </span></h2><p class="c9"><span class="c3">It just uses two instead of one expert and also adds another layer without an expert on top of the expert one.</span></p><p class="c9"><span class="c3">The authors of original papers weren’t impressed (and I was not as well)</span></p><p class="c9"><span class="c3">Some legit guy told me the GPT-4 is actually based on this architecture.</span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.510e3yvi7pl0"><span class="c22 c27">St-MOE</span></h2><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2202.08906&amp;sa=D&amp;source=editors&amp;ust=1715645745341619&amp;usg=AOvVaw3fB1K3MOgE1_gio-mv98NU">[2202.08906] ST-MoE: Designing Stable and Transferable Sparse Expert Models</a></span><span class="c3">&nbsp;</span></p><p class="c9"><span class="c3">This focuses mostly on stability of the expert transformers especially during finetuning. Have to read it but it’s too loooooong.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Below is a much more exhaustive reading list thanks to main_horse on twitter</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 568.00px; height: 613.62px;"><img alt="" src="./Keeping up with AGI_files/Screenshot from 2023-08-07 10-02-15.png" style="width: 604.00px; height: 956.49px; margin-left: -25.00px; margin-top: -253.49px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.q0iiagxgf9ks"><span class="c5">Multi-Query Attention</span></h1><p class="c9"><span>&nbsp;</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/1911.02150.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745342467&amp;usg=AOvVaw1J_W6mZSfoOt6nZZCLHDmb">https://arxiv.org/pdf/1911.02150.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>This is actually quite simple. You simply calculate attention for the same key and value using multiple queries. It’s just a variant of multi head attention with key and values shared. Primary motivation is to reduce the memory footprint during inference and training while capturing as much performance as possible.<br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://gist.github.com/cto-junior/0adbaa7c5a8b2ce115939c7092af783b&amp;sa=D&amp;source=editors&amp;ust=1715645745342883&amp;usg=AOvVaw3m6ccnJ2Z3AkOwtNtbym1g">https://gist.github.com/cto-junior/0adbaa7c5a8b2ce115939c7092af783b</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.kgg6rd6vc82c"><span class="c5">Symbol Rank ( for coding LLMs)</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/ocolegro/status/1676602607106760705?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745343350&amp;usg=AOvVaw2U166CSiYJGtKceAmolJY5">https://twitter.com/ocolegro/status/1676602607106760705?s=20</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/emrgnt-cmplxty/Automata&amp;sa=D&amp;source=editors&amp;ust=1715645745343742&amp;usg=AOvVaw2Y1Bf5ud_ZWzOWYC_v01_W">GitHub - emrgnt-cmplxty/automata: Automata: The Future is Self-Written</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.h6kk8qvwb897"><span>ReLORA</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2307.05695&amp;sa=D&amp;source=editors&amp;ust=1715645745344331&amp;usg=AOvVaw1yKzhd34Wy_HULCLd1Vq_b">[2307.05695] Stack More Layers Differently: High-Rank Training Through Low-Rank Updates</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ok so the idea is really simple.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">You add a lora adapter, you train it</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Once the training is finished, you simply merge those lora weights with the layer. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Then you reset the A &amp; B matrices and resume the training again. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Doing it multiple times will give you good results.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The catch is simple reset doesn’t work due to some past gradient thingy which will mess up the future updates so they reset a lot of optimizer states to 0 as well. </span></p><p class="c0"><span class="c5"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 657.24px; height: 404.45px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-21 at 2.08.35 PM.png" style="width: 657.24px; height: 404.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c5"></span></p><p class="c9"><span>Overall code is very easy &nbsp;- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/Guitaricet/peft_pretraining&amp;sa=D&amp;source=editors&amp;ust=1715645745345785&amp;usg=AOvVaw2scYct90gzxJHoRuuuV38f">https://github.com/Guitaricet/peft_pretraining</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Just grep for the can_reset if block in the </span><span class="c52">torchrun_main.py</span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>And grep for merge_and_reinit in </span><span class="c45 c52 c56">peft_pretraining/relora.py</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Thanks for reading this, I should mention though more likely it is not usable for your llama model. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>I am not making this up but the authors themselves haven’t tested it properly beyond 1B models and there too the results were dicey. </span></p><h1 class="c1" id="h.hp7exsn7mhb3"><span class="c5">Zero++ </span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/&amp;sa=D&amp;source=editors&amp;ust=1715645745346943&amp;usg=AOvVaw17BHGNMDPy0YYSl8Idke_h">DeepSpeed ZeRO++: A leap in speed for LLM and chat model training with 4X less communication - Microsoft Research</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ok so this is easy as well. They are just trying to reduce the comm overhead in distributed training.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So they do three things -</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_kd4buve6yzmu-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Just quantize the weights in the sender and dequant them in the receiver. This reduces the number of bytes during comms.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">You do the same for communication gradients as well </span></li><li class="c9 c14 li-bullet-0"><span class="c3">Use more VRAM to keep model weights for a layer in each machine/node rather than distributing them. The weights are still split across the GPUs of that node. This allows to basically eliminate cross-node comms during the backward pass</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Use custom quantization methods so that you retain as much accuracy as possible especially in the case of gradients which are too sensitive to error</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1063.14px; height: 154.08px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-21 at 4.00.03 PM.png" style="width: 1063.14px; height: 154.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.gnuz67jz9ykx"><span class="c5">Flash attention 2</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://crfm.stanford.edu/2023/07/17/flash2.html&amp;sa=D&amp;source=editors&amp;ust=1715645745348309&amp;usg=AOvVaw2YOKl3nFMpPZtANRiFmnaV">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ok, this mostly seems like just optimisation on the kernel front rather than like an algo-rewrite of v1.</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_luwys0sctije-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Instead of partitioning KV over warps, just partition Q over warps. Reduces comm overhead after updates</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Add one more dimension to iterate over (sequence length) other than the batch size to increase the number of parallel operations and increase GPU utilisation</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 620.66px; height: 198.72px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-23 at 4.25.00 AM.png" style="width: 620.66px; height: 198.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.uo7psucm07vi"><span class="c5">LIMA</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">LIMA LIMA LIMA LIMA LIMA LIMA LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA &nbsp;LIMA LIMA LIMA </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.11206&amp;sa=D&amp;source=editors&amp;ust=1715645745349782&amp;usg=AOvVaw10riLt7YOAJbQ0vBHG_GKB">[2305.11206] LIMA: Less Is More for Alignment</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Just use less but better quality example for supervised finetuning, rather the shitty dumps from your support portal</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 654.94px; height: 436.62px;"><img alt="" src="./Keeping up with AGI_files/F1nqzNXacAAtepN.jpeg" style="width: 654.94px; height: 436.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.z4b0oljl6ol3"><span>RLHF</span><span class="c55">&nbsp;</span></h1><p class="c9"><span class="c28">I have the mandate of the heaven</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 643.68px; height: 317.94px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-24 at 9.55.19 AM.png" style="width: 643.68px; height: 317.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Tweeted this into the void and woke up to a presentation from HF bros in the morning. Entropy works in mischievous ways.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c23"><a class="c10" href="https://www.google.com/url?q=https://docs.google.com/presentation/d/1b_ymNDU0WRQ1-rcQDK45_bH9F0giNyRmdi0iKso6G5E/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1715645745351196&amp;usg=AOvVaw2TQ6sg6SkxiVsR3ohUl-3q">ICML '23 Tutorial on Reinforcement Learning from Human Feedback</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/ppo_trainer.py&amp;sa=D&amp;source=editors&amp;ust=1715645745351711&amp;usg=AOvVaw1pjsgSsyManFsEHtZBr7cO">DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/ppo_trainer.py at master</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1 c24" id="h.fvdsrghqcv4d"><span class="c5"></span></h1><h1 class="c1" id="h.hkieadha7bcj"><span>[Lora Hub] </span><span class="c5">Wait, was I talking about being blessed with the mandate of the heaven, Yes I still have it</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 611.18px; height: 772.21px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-26 at 6.26.14 PM.png" style="width: 611.18px; height: 772.21px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/sail-sg/lorahub&amp;sa=D&amp;source=editors&amp;ust=1715645745352704&amp;usg=AOvVaw26bGcA00H6I2V_vFqob-rh">sail-sg/lorahub</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2307.13269&amp;sa=D&amp;source=editors&amp;ust=1715645745353069&amp;usg=AOvVaw2nja_UWeo3XQRia0rp2aKi">https://arxiv.org/abs/2307.13269</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">We already know that GPT4 leaks said the MoE architecture is the way to go. </span></p><p class="c9"><span class="c3">So now anons are trying to replicate that for local models.</span></p><p class="c9"><span class="c3">Issue is a proper MoE architecture works in the following way</span></p><ul class="c4 lst-kix_bv50cptb9bx1-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">You take a large neural net with multiple FF layers</span></li><li class="c9 c14 li-bullet-0"><span class="c3">You add a gating function to it</span></li><li class="c9 c14 li-bullet-0"><span class="c3">The gating function decides which FF layer actually triggers</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So at inference time you only have a part of the model that’s actually doing the computation while the rest is inactive. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For local models, this approach is not the right way forward. Reason being it severely inflates the size of the model and we want smol.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So what to do?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">We already have smol experts (kinda) called LoRAs. But the issue with loras is you can combine them but not select one dynamically during the inference time. Secondly no one knows if it’ll even actually benefit the base model.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Well that’s what this paper shows. The approach works and your model is better at more tasks. They however don’t select one lora, they simply multiply each lora adapter matrix A &amp; B by a different set of weights, sum all A’s together, then sum all B’s together and finally multiply both to get a merged Lora. <br>The weights take care of how much each lora should contribute to the final output.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 665.50px; height: 176.68px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-28 at 5.36.49 PM.png" style="width: 665.50px; height: 176.68px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">For the weight training, they say using gradient based approach will be super slow, what they recommend is a nograd approach (that’s present in their code as well), where they just select 20 loras, take loss with the output then use this nograd optim to adjust weights. Only a few iters are done.</span></p><h1 class="c1" id="h.wwyewk6u4jpr"><span class="c5">TinyStories</span></h1><p class="c9"><span>Since karpathy implemented and validated it, I must read it.<br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.07759&amp;sa=D&amp;source=editors&amp;ust=1715645745354776&amp;usg=AOvVaw38DjLnkpUccuCUPscA6lzr">[2305.07759] TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ok, here the claim is very simple. You don’t really need a large model for coherent English output.</span></p><p class="c9"><span class="c3">What you need is [Ba Dum Tss!] a good dataset.</span></p><p class="c9"><span class="c3">So to create that dataset they select around a 1000 words that are typically known by 4-5 year old kids. Then used GPT to create small stories around those words to increase the diversity of the dataset.</span></p><p class="c9"><span class="c3">Then simply train multiple small models all &lt; 100M params.<br><br>What they find is amazing! &nbsp;Not only does your model produce coherent output, it also gains a bit of reasoning ability like LLMs e.g. it can remember facts, figure out correct grammar etc.<br><br>I don’t like their final eval method though. They simply used GPT-4 to grade the answers generated by small models. The reason I don’t like this is even though if GPT-4’s scoring is consistent, I remember reading somewhere it prefers the output that GPT-4 itself will generate rather than a better one. Might not be a big deal here since if you match GPT-4 &nbsp;levels you are already good.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Next - experimenting with llama.c </span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/karpathy/llama2.c&amp;sa=D&amp;source=editors&amp;ust=1715645745355608&amp;usg=AOvVaw2V282PSmuCWAusIKDsm-sL">GitHub - karpathy/llama2.c: Inference Llama 2 in one file of pure C</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.xj84lkrgvymg"><span class="c5">FNet</span></h1><p class="c9"><span class="c3">Have you thought about replacing attention with FFT anon? It works and is faster. Validated by kaioken dev<br></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2105.03824&amp;sa=D&amp;source=editors&amp;ust=1715645745356418&amp;usg=AOvVaw2dSYG-QkwVAak8YWviJl3j">[2105.03824] FNet: Mixing Tokens with Fourier Transforms</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 605.04px; height: 464.50px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-28 at 12.39.00 AM.png" style="width: 605.04px; height: 464.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.946b0qfg06ix"><span class="c5">Scaling S3 is not easy [Not related to ML but also related to AI cause all data is in S3]</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html&amp;sa=D&amp;source=editors&amp;ust=1715645745357116&amp;usg=AOvVaw3qHfzSAr2NUwRTLO3dpLK0">Building and operating a pretty big storage system called S3 | All Things Distributed</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Was reading about how S3 is scaled for a billion users. Realized I know nothing about computers. I am so dumb. Need to get smarter.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The gist is HDDs are cheap (everyone knows that) and they are getting larger storage wise (everyone knows that yet again) but the issue is reading/writing in HDD is done by a mechanical head. And there’s a limit to which you can make mechanical heads faster. </span></p><p class="c9"><span class="c3">So more often that note you’re choked by IO. The way to solve for this is to shard your data and distribute it across as many HDDs as possible.<br><br>That way you can do multiple fetches in parallel rather than waiting for a second for head to seek 2TB of data in a single node.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 750.00px; height: 359.49px;"><img alt="" src="./Keeping up with AGI_files/ezgif.com-video-to-gif (1).gif" style="width: 750.00px; height: 359.49px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 673.50px; height: 496.94px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-28 at 3.29.02 PM.png" style="width: 673.50px; height: 496.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 633.00px; height: 474.00px;"><img alt="" src="./Keeping up with AGI_files/1690515218359166.jpg" style="width: 633.00px; height: 474.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.y0bsouxtlrlt"><span class="c5">RetNet</span></h1><p class="c9"><span class="c3">I thought it was a meme but now it’s gaining a lot of steam so I must absorb it’s data stream</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.ernrh6qbp4cz"><span class="c5">MoE (by Deepmind) (It’s soft not sparse)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2308.00951&amp;sa=D&amp;source=editors&amp;ust=1715645745358579&amp;usg=AOvVaw2CNSNMURVdt9iISlu85HjD">From Sparse to Soft Mixtures of Experts</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Instead of routing a single token to an expert, what they’re proposing is to take a weighted average of the tokens and then route it to an expert. This allows for better training stability and ability to scale the experts. </span></p><p class="c9"><span>The primary disadvantage is this doesn’t work for autoregressive decoders. &nbsp;</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.50px; height: 294.85px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-14 at 3.59.03 PM.png" style="width: 720.50px; height: 294.85px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>It looks like they are using slots per expert as the tuning knob to make the model faster or slower. Each slot has its own set of parameters (known as dispatch). We simply multiply the input tokens using a slots params and feed it to the expert. Then we do the reverse using ‘combine params’ and get the combined prob of a single output token. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 712.28px; height: 347.10px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-14 at 4.25.19 PM.png" style="width: 712.28px; height: 347.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 727.64px; height: 294.99px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-14 at 4.28.12 PM.png" style="width: 727.64px; height: 294.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>If you’re noob in maths like me, use this GPT-4 explainer for understanding - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://chat.openai.com/share/6bc517e0-2acf-48ba-8436-f1a2d8702b65&amp;sa=D&amp;source=editors&amp;ust=1715645745359635&amp;usg=AOvVaw24tKE_5ZAYcrD58hSuBfT9">https://chat.openai.com/share/6bc517e0-2acf-48ba-8436-f1a2d8702b65</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.5m2hi48phjl5"><span class="c5">Skill Issue Paper</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2307.14430&amp;sa=D&amp;source=editors&amp;ust=1715645745360137&amp;usg=AOvVaw0lTLiIjLU01nhdYJSbaRyQ">[2307.14430] Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Train LLMs like you would teach a kid<br><br>Don’t teach them algebra before you have taught them basic arithmetics</span></p><p class="c9"><span>&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 773.63px; height: 148.83px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-15 at 2.38.56 AM.png" style="width: 773.63px; height: 148.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is an interesting approach to clustering. </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 776.74px; height: 91.78px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-15 at 2.20.51 PM.png" style="width: 776.74px; height: 91.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This paper shouldn’t be complicated to read at all but they fell into the rabbit hole of using too many math symbols to denote function calls.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Skill Graph creation</span><span class="c3">&nbsp;- quite easy honestly if you ignore the maths<br><br>You first create a train and eval set for various skills using clustering</span></p><p class="c9"><span class="c3">Then you take a base model and train K versions of it for H steps, one for each skill. Finally store the validation loss difference for that skill b/w original base model and finetuned model.<br></span></p><p class="c9"><span class="c3">Now you simply start considering all pairs of skills, and train the base model on union of both skill dataset.<br>Now you observe the loss of this finetuned model vs the base model.<br>If the delta loss here is greater than the delta loss when trained just on one of the skills in previous step, we simply add an edge in the graph.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 788.52px; height: 535.57px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-15 at 3.21.09 PM.png" style="width: 788.52px; height: 535.57px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Using the Skill graph matrix A to actually select samples during training</span></p><p class="c9"><span class="c3">Now that you have the skill graph, it’s time to use it during the training. This is the main part of the paper. <br>Naively select equal number of samples from all relevant skill is not good enough, since one skill can disproportionately affect the loss. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>A better way is to take into account which skills are leading to bigger validation loss and then change the sample distribution accordingly. We will be using p</span><span class="c60">i</span><span class="c3">&nbsp;to denote the fraction of samples that should be from skill i.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 806.79px; height: 279.03px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-15 at 6.14.08 PM.png" style="width: 806.79px; height: 279.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It is not that difficult except for some assumptions. You initialize the proportion of each sample related to the weight of the adjacency weight matrix edge. Then at each iteration, you observe the loss of the model w.r.t. All the unique skill sets samples in the validation set. Then you train the model with an existing mixture of sample. Finally you adjust the proportion of the samples based on the new loss. </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">WHY ARE WE STILL DOING MANUAL STATUS UPDATES IN COMPANIES?</span></p><p class="c9"><span class="c3">Time to plugin LLMs to the Slack feed and let it automatically track everything for you. </span></p><h1 class="c1" id="h.jpajejzi0mlh"><span class="c5">BERT Primer</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2002.12327&amp;sa=D&amp;source=editors&amp;ust=1715645745362612&amp;usg=AOvVaw14OZRipG2t_ybFA8B3ge6x">https://arxiv.org/abs/2002.12327</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I have skipped over a lot of basics, time to catch up</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.25q3o3ayyp15"><span class="c5">Estimate LLM Flops and Memory requirement</span></h1><p class="c9"><span>I am not good at maths so it took me some time but after reading </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://finbarr.ca/how-is-llama-cpp-possible/&amp;sa=D&amp;source=editors&amp;ust=1715645745363240&amp;usg=AOvVaw1u6X6Nmt3ReptG7UEsGZm1">https://finbarr.ca/how-is-llama-cpp-possible/</a></span><span class="c3">&nbsp;I finally understand</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Most basic formulae</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Number of params approx (P) - 12 * [(Model dimension)^2] * [number of layers] </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Mem usage &nbsp;= bytes required by each params (4 generally) * P<br><br>Flops usage = 2 * P (assuming it requires one mul and one add per param)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>All of this is extremely approx and doesn’t account for vocab to embeddings as well as normalisation layers</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.4mdtafvn1zrz"><span class="c5">RoPE</span></h1><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 556.62px; height: 432.55px;"><img alt="" src="./Keeping up with AGI_files/F4JVbxJbsAAG_x3.jpeg" style="width: 556.62px; height: 432.55px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So that finally even codellama paper has demonstrated that using RoPE scaling you can get to almost 100K context size, it’s time for me to stop pretending that I know what it is and actually learn.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2104.09864&amp;sa=D&amp;source=editors&amp;ust=1715645745364577&amp;usg=AOvVaw1jEhR05lpxG5yAKCBMpQnZ">[2104.09864] RoFormer: Enhanced Transformer with Rotary Position Embedding</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">If you don’t know what positional embedding is, it’s basically capturing the position information of the sequence in the input. This is needed so that you can differ b/w B follows A from A follows B. Also needed to determine how far is A w.r.t B and should it even affect B’s prob<br><br>The trad way of doing it is to simply learn embeddings of the same dimensions as the model but for each position. Then add these embeddings to the model input.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">But this suffers from a major drawback - You can’t adapt your model to go beyond the positional embeddings it has learned since they don’t exist in the matrix. Secondly it doesn’t capture the relative positions well, only the absolutes.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>RoPE aims to solve it. The fundamental idea is first to use only the relative positional information, secondly use some function (not lookup) to convert that information to embeddings. The function that they use is sinusoidal angular transform which basically rotates your token embeddings by some angle based on the position (hence the name). Sounds quite intuitive.<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.16px; height: 337.22px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-27 at 10.29.55 PM.png" style="width: 671.16px; height: 337.22px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.87px; height: 395.19px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-28 at 1.46.19 AM.png" style="width: 674.87px; height: 395.19px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Code - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://gist.github.com/cto-junior/3493fe428a069a3500f85f8558ef5df9&amp;sa=D&amp;source=editors&amp;ust=1715645745365699&amp;usg=AOvVaw0jpnPcpbSXMetcZPdTmeO8">https://gist.github.com/cto-junior/3493fe428a069a3500f85f8558ef5df9</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.k20442q774j2"><span class="c5">Speculative decoding</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper: </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2302.01318&amp;sa=D&amp;source=editors&amp;ust=1715645745366206&amp;usg=AOvVaw3WY0AxYQ1V4wVJOSKAQoof">[2302.01318] Accelerating Large Language Model Decoding with Speculative Sampling</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I had heard about it and I heard about it again today (both time from Seminalysis)</span></p><p class="c9"><span class="c3">THe idea is pretty simple - you use a smol model to generate tokens quickly (speculation) and then correct the output if it diverges from the slower but accurate large model. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The smaller model will be faster in generation but if you have to check its output 1 by 1 with larger model then you’re still bottlenecked right?<br><br>Well, the solution is to simply generate K tokens at a time from a smaller model and then feed all K tokens to the main model and get the correct log probs for the next tokens in a single pass. &nbsp;Then you can simply compare and discard the incorrect tokens.<br><br>Almost all major LLMs are using this in prod (as per the leaks). Leads to 2 - 5X improvement in latencies</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.zgbeak6axsff"><span class="c5">Cool paper &nbsp;- Topology of NN</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2004.06093&amp;sa=D&amp;source=editors&amp;ust=1715645745367007&amp;usg=AOvVaw1vpq9uh79D-Yikd91YGpYU">[2004.06093] Topology of deep neural networks</a></span></p><p class="c9"><span>TLDR in this thread - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/suchenzang/status/1696924361373151337?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745367317&amp;usg=AOvVaw1UR02SjA0nPDyIpWsJonlb">https://twitter.com/suchenzang/status/1696924361373151337?s=20</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I’d be lying if I said I understand all the maths in this. But I do understand the idea. They are basically trying to show how classifier neural nets change the datasets after each layer so that each class is easily distinguishable in the space. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 669.50px; height: 254.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-31 at 9.26.05 AM.png" style="width: 669.50px; height: 254.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>It also shows how ReLU is more effective than sinusoidal functions at doing this. </span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.iuq0qulcpmpi"><span class="c5">How to reduce KV cache mem usage?</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.17118&amp;sa=D&amp;source=editors&amp;ust=1715645745368157&amp;usg=AOvVaw3JPNoV1z0NG-653-v1mep-">[2305.17118] Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Size of the KV Cache for one token in an LLM assuming f16 weights is - &nbsp;</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.50px; height: 55.82px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-05 at 10.25.54 AM.png" style="width: 391.50px; height: 55.82px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>(From - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://kipp.ly/transformer-inference-arithmetic/&amp;sa=D&amp;source=editors&amp;ust=1715645745368820&amp;usg=AOvVaw1mOY50OQnHNuoVqlQh3d9w">Transformer Inference Arithmetic | kipply's blog</a></span><span class="c3">)</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">You can also multiply this by batch size B for production systems</span></p><p class="c9"><span class="c3">By convention, most models have nhead * dhead = dmodel</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So e.g. for LLama-7B, for a batch size of 1, &nbsp;this would be equal to &nbsp;4 * 32 * 4096 = 524288 bytes = 0.5MB</span></p><p class="c9"><span class="c3">Now this is per token, so if you want to run inference on 2048 tokens, the total kv cache required would be = 2048 * 0.5 = 1GB. <br><br>And this is just for this small 7B model. For 65B it would come to around 5GB (4 * 80 * 8192 * 2048).<br><br>And this is for batch size = 1 btw which means low flops utilization and insanely slow inference.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 546.20px; height: 183.40px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-05 at 10.41.56 AM.png" style="width: 546.20px; height: 183.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So there's a need to find a way to compress the KV cache. That’s what this paper aims to solve. <br></span></p><p class="c9"><span class="c3">The basic hypothesis is simple. A) Only a few tokens are important and others are not. B) The importance of future tokens is more or less correlated with the ones in the past.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The core algo then is not that tough. You make a window of size w and only keep the values in the kv cache which always have values greater than the threshold in that window w. You also always keep all the values of the most recent tokens denoted by r. </span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 683.22px; height: 247.87px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-05 at 12.41.50 PM.png" style="width: 683.22px; height: 247.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 683.89px; height: 459.23px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-05 at 12.42.11 PM.png" style="width: 683.89px; height: 459.23px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Overall I feel like it’s a good hypothesis but there are too many approximations built into the inference side. They should publish the results for llama or something rather than OPT.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.sb492jcjym8x"><span class="c5">Hyena</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://hazyresearch.stanford.edu/blog/2023-03-07-hyena&amp;sa=D&amp;source=editors&amp;ust=1715645745370791&amp;usg=AOvVaw0zDsZyscOp0sgru0w9c4kf">https://hazyresearch.stanford.edu/blog/2023-03-07-hyena</a></span></p><p class="c9"><span class="c3">You must read this. I am telling you. It is mid but you must.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Update</span><span class="c3">:<br>I finally read this blog. They are proposing a new architecture which helps reduce the inference time especially cause the attention formula is quadratic in nature (we compute attention for every token in sequence with every other token in the sequence)<br><br>They simplify the attention formula simply as A(x) . x where x is the input embeddings. Most network architectures don’t have A(x) but just a single W weight matrix learned during training. This makes the attention based models quite unique as they can adjust to inputs very well and can demonstrate capabilities like in-context and few shot learning. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">But in attention based models, except for this layer all other layers simply are of W.x nature. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Their argument is instead of using quadratic attention here, what if we made other layers a bit different so that they are of A(x).x nature as well but A(x) being a sub-quadratic function.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The function they propose is a long convolution i.e. long sliding window matrix that attends to some tokens in the sequence and generates the output. &nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 685.50px; height: 186.37px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-06 at 12.08.26 AM.png" style="width: 685.50px; height: 186.37px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span>The hyena_orders is just some number which they keep as 3 for some reason. <br><br>The results presented are pretty good but I hope they present it to work same or better w.r.t. llama-like alternatives. <br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 694.56px; height: 325.89px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-06 at 12.11.18 AM.png" style="width: 694.56px; height: 334.43px; margin-left: 0.00px; margin-top: -8.54px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Also, if you’re like me with rusty knowledge of CNNs, here’s a simple explanation by GPT-4</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 695.50px; height: 618.45px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-06 at 12.23.17 AM.png" style="width: 695.50px; height: 618.45px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 298.00px; height: 298.00px;"><img alt="" src="./Keeping up with AGI_files/F4oJDtaWcAIt605.png" style="width: 298.00px; height: 298.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp;</span><span class="c12">I need to get so much better aaahhh</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Also, I think I’d be dabbling a bit into tinygrad, see what abstractions I can recreate from scratch. Should be a fun exercise. </span></p><h1 class="c1" id="h.7wxbxkjqkd8q"><span class="c5">VectorDB arc</span></h1><h2 class="c13" id="h.xwh4f7jaohut"><span class="c22 c27">Ok, I am going to become Vector DB expert this week</span></h2><p class="c9"><span class="c12">It has been mandated by the heaven</span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 569.27px; height: 320.21px;"><img alt="" src="./Keeping up with AGI_files/1690515559329505.jpg" style="width: 569.27px; height: 320.21px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Let’s look into it from a practical perspective. </span></p><p class="c9"><span>I found this good starter resource for anyone who wants to understand the basics - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://vectorsearch.dev/&amp;sa=D&amp;source=editors&amp;ust=1715645745373179&amp;usg=AOvVaw1veMjdHMLmMhPcUhAdmGIz">The definitive guide to using Vector Search to solve your semantic search production workload needs.</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I know about 40% of it already so directly jumping on how actual indexing works. </span></p><p class="c9"><span class="c3">Most popular algo is HNSW but scann is faster. There are a lot of other alternatives as well.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Now most people are only concerned about speed of the algo but that is sorta necessary but not sufficient.</span></p><p class="c9"><span class="c3">To create a full fledged vector db, you need more</span></p><ul class="c4 lst-kix_hb0nnp6nh7i4-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Ability to update the index at runtime</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Ability to shard / merge the index</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Ability to serialize/deserialize the index</span></li></ul><p class="c9"><span class="c3">And many more</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Considering all this I have started reading about Lucene’s HNSW implementation (since Lucene is production-grade and almost used everywhere. I am afraid though it might be slow)</span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.706kku5ube20"><span class="c22 c27">Lucene HNSW</span></h2><p class="c9"><span>All alpha from - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://issues.apache.org/jira/browse/LUCENE-10054&amp;sa=D&amp;source=editors&amp;ust=1715645745374549&amp;usg=AOvVaw39Mz_pBRiNHHhl_KeOfsBo">https://issues.apache.org/jira/browse/LUCENE-10054</a></span></p><p class="c9"><span>And </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://issues.apache.org/jira/browse/LUCENE-9004&amp;sa=D&amp;source=editors&amp;ust=1715645745374850&amp;usg=AOvVaw0qpo030b09_6z5D2qoxi7c">https://issues.apache.org/jira/browse/LUCENE-9004</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>More orgs should start doing this honestly - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://people.apache.org/~mikemccand/lucenebench/VectorSearch.html&amp;sa=D&amp;source=editors&amp;ust=1715645745375191&amp;usg=AOvVaw30pflAZDsLOsiBx93X-NMq">https://people.apache.org/~mikemccand/lucenebench/VectorSearch.html</a></span><span class="c3">&nbsp;</span></p><p class="c9"><span class="c3">Publish nightly perf numbers on OSS libs.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The first issue was a gold mine btw. I learnt a lot. </span></p><p class="c9"><span class="c3">The primary doubt I had was how to create a single graph and so far it looks like that’s not the case. <br>They are still using small graphs (10M record max)</span></p><p class="c9"><span class="c3">So that means you yourself will have to combine the results (maybe they have some utility for it)<br><br>Another thing is the memory usage optimisations they have done. Loading everything in memory is not required, you can just keep the entry points for each hierarchy and the neighbors. The values are always docIDs so it only requires 4 bytes to store. Once you get a docID you can look up the actual vector value from the segment.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 529.00px; height: 446.54px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-26 at 1.29.57 PM.png" style="width: 529.00px; height: 459.00px; margin-left: 0.00px; margin-top: -12.46px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 670.50px; height: 151.30px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-26 at 1.33.59 PM.png" style="width: 670.50px; height: 151.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.26px; height: 480.68px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-26 at 1.33.11 PM.png" style="width: 671.26px; height: 480.68px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 673.86px; height: 379.05px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-26 at 1.59.03 PM.png" style="width: 673.86px; height: 379.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 672.50px; height: 530.81px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-26 at 2.53.45 PM.png" style="width: 672.50px; height: 530.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 675.98px; height: 341.77px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-08-26 at 3.13.46 PM.png" style="width: 675.98px; height: 341.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">How to do distributed HNSW? Like the one where I don’t replicate the data but where I shard the data and then query multiple subgraphs to get final answer.</span><span><br><br>Some hint - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/nmslib/hnswlib/issues/377&amp;sa=D&amp;source=editors&amp;ust=1715645745376303&amp;usg=AOvVaw3VVzYnqaLt4NtuA7OrbDJd">https://github.com/nmslib/hnswlib/issues/377</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Also Pyramid Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/1906.10602.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745376678&amp;usg=AOvVaw0uzyZqWARbeWs0g3QTAsHs">https://arxiv.org/pdf/1906.10602.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Ok, I found the answer for this. Right now it’s pretty dumb but works. If you need top 10 matches, you query for top 100 records from each shard and then sort again. It’s costly and inaccurate but that’s what’s being done in Elasticsearch atleast<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 690.33px; height: 266.43px;"><img alt="" src="./Keeping up with AGI_files/F4dyweDWUAAlE-y.jpeg" style="width: 690.33px; height: 266.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c13" id="h.e2ypjdnu4z3o"><span class="c22 c27">FAISS</span></h2><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/facebookresearch/faiss/wiki/&amp;sa=D&amp;source=editors&amp;ust=1715645745377286&amp;usg=AOvVaw1Bvlm7-H6tPhzeMezaPgyo">https://github.com/facebookresearch/faiss/wiki/</a></span></p><p class="c9"><span class="c3">FAISS wiki has a lot of alpha</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I was reading chroma and weavite docs but to me they look like half solutions. Primarily cause I don’t see they support efficient sharding of vectors. It is still mostly defined by some primary key kind of approach in weavite.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.ty7lppo40z1y"><span class="c22 c27">Annoy </span></h2><p class="c9"><span class="c3">Need to look into this but afaik this and ScaNN are immutable once built. This would mean carefully choosing a large enough shard which you can’t modify later on. Hmm doesn’t seem to be worth pursuing.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.3182psj9roxa"><span class="c5">Mixture of Experts: PEFT edition by Cohere</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2309.05444&amp;sa=D&amp;source=editors&amp;ust=1715645745378518&amp;usg=AOvVaw2Lh-TAlaHQd0pl3AwJYOdP">https://arxiv.org/abs/2309.05444</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">After LoraHub, another paper showing the viability of MoE using only Lora or (IA)3</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The architecture is almost the same which involves a gating function which decides the experts. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Notable changes though - </span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_f47zoa2invu7-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Token level routing instead of sentence level - they prove the first one is better in ablation study. It simply means various tokens can get routed to different experts in the same sentence.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Soft routing - Instead of selecting a single or top K experts, they simply take the weighted average of all experts’ probs. Again they prove it in study why it is better.</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.60px; height: 399.71px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-14 at 12.53.46 AM.png" style="width: 671.60px; height: 399.71px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">If you don’t know what IA3 is, it’s pretty simple. Just 3 vectors per layer that are multiplied to Key, Value and FF layer weights respectively during inference. IA3 in general performs worse than lora but here when used in MoE fashion they perform better. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Another issue with IA3 is that your vector size is fixed w.r.t. To model dimensions. In LoRA, you can easily change the rank to change the size and play around with evals. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 673.50px; height: 202.15px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-14 at 12.54.42 AM.png" style="width: 673.50px; height: 202.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.wid2blj5aj68"><span class="c5">LLM as Optimisers</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2309.03409&amp;sa=D&amp;source=editors&amp;ust=1715645745381230&amp;usg=AOvVaw1JvyslykPNW2lx4Ty4YGfc">[2309.03409] Large Language Models as Optimizers</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Not sure how many papers I gotta show folks before they accept that LLMs can reason pretty well. They also try the best models from each company and all of em perform pretty well. GPT-4 obviously being the best.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is by google so please believe me now. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The problem statement is simple. First - given an optimisation problem like travelling salesman or simple gradient descent and the path take till now in the form of coordinates, can the LLMs converge to the final solution? <br><br>The answer is yes. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 666.70px; height: 429.42px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-14 at 1.28.24 AM.png" style="width: 666.70px; height: 429.42px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The second problem statement is - Given an eval, along with previous prompts used for the eval and the final scores of each prompt, can you generate a new prompt that’s better? It’s the variation of the first problem statement. <br><br>The answer is again, yes. </span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 658.50px; height: 638.32px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-14 at 1.31.33 AM.png" style="width: 658.50px; height: 638.32px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 661.50px; height: 277.52px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-14 at 1.33.10 AM.png" style="width: 661.50px; height: 277.52px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Just ask you LLM to take a deep breath 😀</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 552.16px; height: 552.16px;"><img alt="" src="./Keeping up with AGI_files/1694107055949688.jpg" style="width: 552.16px; height: 552.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">I will become this guy, watch me</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.yu5rdpsl7mm3"><span class="c5">Generative Recommendors - Cool paper by Google</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.05065&amp;sa=D&amp;source=editors&amp;ust=1715645745383985&amp;usg=AOvVaw1HcmeAr5YEZt_YXenN86Gz">[2305.05065] Recommender Systems with Generative Retrieval</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">If you don’t know already, the most common implementation of recommendation systems in most companies is the following </span></p><ul class="c4 lst-kix_egzktlggi0ic-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Generate embeddings/features for all the items in the catalogue</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Generate embeddings for the user search</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Fetch top K items using similarity search</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Re-rank them using a ranker service</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This poses a problem though. What to do when new items keep on being added to the catalogue. Retraining the embedding model is expensive with millions of items plus slow. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">What if we simply trained a model to output the most related item id based on the past selection by the user? <br><br>What if it’s a generative model rather than a classifier or something? Would that even work and not fucking hallucinate while generating the id.<br><br>Turns out it does.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Their approach is as follows </span></p><ul class="c4 lst-kix_sqt2qadhlypt-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Train a model to generate smaller ids for each item based on their embedding. They call it the Semantic IDs. In their model, they use 4 integers in range of [1, 256] e.g. (10, 21, 23, 1) would be a semantic id. &nbsp;They are called semantic because if two ids contain the same numbers at the same position, they are bound to be related to each other.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">To generate these IDs they use a RQ-VAE where RQ stands for Residual quantisation. &nbsp;Let’s be honest, I am a complete noob and I had no idea what it is. So I learned about VAE, VQ-VAE and RQ-VAE from GPT-4. Basically It is a variation of VQ-VAE where we do produce discrete quantised numbers from a codebook (which is just a fancy term for a dictionary) but we run multiple iterations (in our case 4). </span></li><li class="c9 c14 li-bullet-0"><span class="c3">In each iteration, we subtract the embedding of the retrieved number from the original vector. Then we recompute the number. This approach is like selecting for coarse grained features first and then moving to finer grain. As an example (10, 21, 23, 1) - the number 10 will represent the category, 21 will represent the subcategory, 23 the product version within that category and so on. </span></li><li class="c9 c14 li-bullet-0"><span class="c3">To avoid collision between generated ids for items, they use an auto-increment number in the last place (which is 1 here). </span></li></ul><p class="c0 c53"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Turns out this approach is amazing and gives SoTA results on most popular recommendation evals and real life scenarios. &nbsp;They also show that the model doesn’t generate invalid ids a lot ( &lt; 1% in most cases)</span></p><p class="c9"><span>ʼ </span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 686.50px; height: 298.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-14 at 3.13.04 PM.png" style="width: 686.50px; height: 298.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 687.12px; height: 414.69px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-09-14 at 3.13.13 PM.png" style="width: 687.12px; height: 414.69px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Man, not getting a lot of time to read paper these days. Occupied by Day job stuff.<br>But I must push harder. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 682.50px; height: 682.50px;"><img alt="" src="./Keeping up with AGI_files/_2755abed-7812-4fe7-b8c7-948b3e02f486.jpeg" style="width: 682.50px; height: 682.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.d9tvjp5tzgz3"><span class="c5">Flamingo</span></h1><p class="c9"><span>Paper -</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745387891&amp;usg=AOvVaw2gqlKkNynOwFjpOXqdW1CC">�� Flamingo: a Visual Language Model for Few-Shot Learning</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This seems to be a secret sauce behind a lot of ongoing GPT-4(V) like projects. </span></p><p class="c9"><span class="c3"><br>The premise is using a pre-trained visual encoder as well as an LLM to attend to multimodal inputs. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They combine these two models using three things</span></p><p class="c0"><span class="c3"></span></p><ol class="c4 lst-kix_v7w0v08yjiqh-0 start" start="1"><li class="c9 c14 li-bullet-0"><span class="c52">Perceiver Resampler</span><span>&nbsp;- In simplest form, this is just reducing the feature vector coming out from CNN to a fixed dimension space i.e. making it independent of image resolution or video length. Reason - it will be easier to be able to feed to LLM</span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 608.50px; height: 350.90px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-02 at 2.18.16 AM.png" style="width: 608.50px; height: 350.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c9 c14 li-bullet-0"><span class="c52">Cross-attention Dense layer </span><span>- This one is quite obvious and popular solutions. Basically now that you have embeddings from the image, you want to combine them with the embeddings of the text so both can attend to each other. Cross-Attention is the best way to do this. They add the cross-attention layer between each layer of the original LLM. The only difference then you will see is the tanh function. That’s been added so that this layer is ineffective during model initialisation during training</span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.67px; height: 368.15px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-02 at 2.26.36 AM.png" style="width: 624.67px; height: 368.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c9 c14 li-bullet-0"><span>The final piece is masking. This is helpful when you want to support multiple images in the input along with descriptive texts. The idea is that a portion of the text should only attend to the image preceding it (cause this text describes the previous image). This piece honestly might not be needed in most architectures and is just a consequence of the way this model’s dataset looks like (a series of Images, each followed by a text)</span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 625.15px; height: 341.75px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-02 at 2.32.31 AM.png" style="width: 625.15px; height: 341.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ol><p class="c9"><span><br><br>All of this comes together to make the final Flamingo model. It is now trained while keeping original encoder weights and LLM weight frozen.</span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 711.70px; height: 550.11px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-02 at 2.33.34 AM.png" style="width: 711.70px; height: 550.11px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.hoqf9osycnys"><span class="c5">Fusing Modalities - Chimera by Meta</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2309.15564&amp;sa=D&amp;source=editors&amp;ust=1715645745390309&amp;usg=AOvVaw1YwcLKuR6IcBXsMAocD0Kr">https://arxiv.org/abs/2309.15564</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Multimodal transformers are the rage of the town. Especially now that people are seeing the power of GPT4-V. </span></p><p class="c9"><span class="c3">The current way to train them is to change the architecture of an LLM to infuse cross-attention and then training it from scratch to use both Image and Text embeddings. <br>This is very expensive.</span></p><p class="c9"><span>Meta proposes a simpler way. What if you could simply combine two pretrained models to process both image and texts. <br>Ngl, this was also how the </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745391492&amp;usg=AOvVaw1uw7NSuBXRRDDgFdYRl6tw">Flamingo</a></span><span class="c3">&nbsp;architecture worked (which is most likely used in GPT-4V). But even then it introduced 3 extra type of layers to make it work. Plus it required training from scratch (although kept the visual encoder and LLM weights frozen)<br><br>This paper proposes something much simpler. You simply add a cross attention block at the output of each LLM or text to image block. This x-attn block processes input from both modalities. The output of multiple x-attn block is then combined using a linear transformer.<br><br>Now the question is - how to train the x-attn block though? That’s the neat part. They simply use supervised finetuning over a small dataset to get the desired outputs. </span></p><h1 class="c1" id="h.8qusygh6x118"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 661.19px; height: 310.58px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-01 at 9.20.17 PM.png" style="width: 661.19px; height: 310.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 661.76px; height: 373.21px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-01 at 9.25.39 PM.png" style="width: 661.76px; height: 373.21px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5"><br><br>PromptBreeder</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2309.16797.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745392318&amp;usg=AOvVaw0o4CRvfi488AeZhuW3I4av">https://arxiv.org/pdf/2309.16797.pdf</a></span></p><p class="c9"><span class="c3"><br>I’ll be honest here, I saw this paper in a tweet and simply proceeded to ignore it cause not interested in another making prompts have sex to create synthetic dataset approach. Especially cause evol instructor and airoboros already exist and are good. <br><br>Then I saw on r/localLlama that this paper is actually by Goog deepmind. Hence I read it. </span></p><p class="c9"><span class="c3"><br>The approach is mutate a prompt using an LLM<br><br>How? <br><br>Just append a mutate instruction (e.g. make it more creative), before the prompt. You can also append a reasoning style (e.g. think step by step). &nbsp;<br><br>They have multiple ways to select the best prompt as well as mutate the mutate prompt itself using LLM<br><br>Hence the name prompt breeder. </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 675.78px; height: 430.65px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-06 at 11.38.32 AM.png" style="width: 675.78px; height: 430.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">More important is to filter out similar prompts by using BERT embeddings and cosine similarity</span></p><p class="c9"><span>Next, you also provide the good quality prompts in the context of LLM so that it generates an unique one with high quality rather than repeating the same<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 677.50px; height: 260.96px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-06 at 1.05.51 PM.png" style="width: 677.50px; height: 260.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">The primary alpha of this paper honestly is in Appendix. Just go through all the prompts and strategies and select the best ones. <br><br></span></p><h1 class="c1" id="h.o1zjzkxl3bec"><span class="c5">LLAVA</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://browse.arxiv.org/pdf/2304.08485.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745394267&amp;usg=AOvVaw0TO9s-uHljTXuSg9zcMaQp">https://browse.arxiv.org/pdf/2304.08485.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Terrible name, decent model. Multimodal, hence interesting. <br><br>You take the image, then use some encoder like CLIP and generate embeddings. <br><br>Now you do a transform so that these embeddings match the dimension of the LLM’s input layer. LLava folks use a single weight matrix to do this which makes it quite easy to train.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">And voila, you have a multimodal LLM.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 695.63px; height: 482.82px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-07 at 12.00.47 PM.png" style="width: 695.63px; height: 482.82px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The training part is also not something out of this world. First you keep both CLIP and LLM weights frozen and only adjust the projection weights.<br><br>Next you do a full finetune for both LLM and projection layer. You still keep the CLIP frozen. </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 697.50px; height: 674.15px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-07 at 12.02.00 PM.png" style="width: 697.50px; height: 674.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span class="c3">Another Interesting thing is they use CLIP encodings from the penultimate layer and not the last layer like you would ideally do. <br></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 705.40px; height: 259.57px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-07 at 12.04.57 PM.png" style="width: 705.40px; height: 259.57px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span class="c3">Overall, this is actually quite simpler than Flamingo and possibly perform worse but it is faster to finetune and train cause of no complex cross attention mechanisms and fewer additional layers. </span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.fkv4gpuc05qh"><span class="c22 c27">LLAVA-1.5</span></h2><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://browse.arxiv.org/pdf/2310.03744.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745396257&amp;usg=AOvVaw1Tk0MkrlFF7SP4prtMghFd">https://browse.arxiv.org/pdf/2310.03744.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Honestly, it’s same as LLAVA 1 except they two things</span></p><ul class="c4 lst-kix_a14d8f51jepj-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Using a two-layer MLP instead of a single projection weight matrix</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Using academic VQA datasets during training</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span>Everything else is the same.</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.jgbdffqbioic"><span class="c5">IMPORTANT INTERPRETABILITY PAPER BY ANTHROPIC</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://transformer-circuits.pub/2023/monosemantic-features/index.html&amp;sa=D&amp;source=editors&amp;ust=1715645745397194&amp;usg=AOvVaw29Pya5kWVW_OJ_kTqrQcfe">https://transformer-circuits.pub/2023/monosemantic-features/index.html</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I am finally reading this masterpiece but let me be honest. Understanding it requires a lot of existing knowledge about NNs plus support from chatGPT</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">You have been taught from childhood in every deep learning course that NNs are basically black box. </span></p><p class="c9"><span class="c3">These days although we have gained significant ability to understand small NNs as well we still lack in ability to figure out how each individual neuron behaves in a network trained on large amounts of data.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">There is a lot of research going on in this domain as it is extremely useful to affect the NNs outcomes and control/guide them easily. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>There was a blog/paper by OpenAI where they used GPT-4 to understand GPT-2 neurons. See </span><span class="c7"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.onil59pfzslp">Model Interpretation</a></span><span class="c3">&nbsp;in this doc.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This paper is another one which uses another NN to figure out first one. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The hypothesis is this: We are not able to understand each individual neuron cause it activates on seemingly random inputs which make no sense to naked eye. However, the possible reason it does that is because it is compressing so much data into &nbsp;the small number of weights. Thus, one neuron ends up representing multiple inputs. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 691.47px; height: 333.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-09 at 10.14.56 PM.png" style="width: 691.47px; height: 333.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">If this is true, what if we trained a much wider neural net on the inputs to this neuron? Since this NN is wide, each neuron should end up representing a considerably fewer number of features. And since they are fewer in number, they should be easily understandable as well.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Well that’s what &nbsp;they try to do in this paper. They use a sparse autoencoder who’s width varies from 1X to 256X of the neural net. The input to this autoencoder is the activation of the neural net layer. The output is also the activations (thus the goal is to reconstruct them). They use MSE loss for it plus an L1 regularization penalty to force sparseness. Without sparseness you would have simply too many neurons activating in this autoencoder making feature distinction difficult.<br> </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 676.00px; height: 738.44px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(22).png" style="width: 676.00px; height: 757.00px; margin-left: 0.00px; margin-top: -18.56px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They haven’t done a half-assed job as well. They do a lot of analysis and publish it to make sure that the features that are detected are in fact representative of the text in the context plus they are simply not neurons weights slapped here. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 680.49px; height: 317.38px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-09 at 11.11.42 PM.png" style="width: 680.49px; height: 317.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span><br></span><span class="c29">Validating Specificity</span><span class="c3"><br>To validate the features, they use log likelihood. E.g. take an Arabic character.. You can find out the &nbsp;probability of that character occurring when the feature corresponding to it is activated vs prob. of &nbsp;that character occurring in the overall dataset. If the ratio is high, that means your feature does specifically point to that character. Now using a single character is tricky so for approximation they use a proxy here. E.g. just a word containing Arabic characters.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Validating sensitivity</span></p><p class="c9"><span class="c3">Here they just measure the correlation b/w the times our feature got activates vs the time we detected the proxy (i.e. any arabic character) in the context. High correlation means the feature is indeed sensitive to the proxy.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Downstream Effects</span></p><p class="c9"><span>If your feature does point to an arabic character, it should also make the future predictions gravitate more towards arabic characters (cause it makes natural sense). They try to measure this for each feature and find out that this is indeed true. They also do a study by disabling the feature and see that the future outputs skews towards normal english or something else. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 673.50px; height: 394.91px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-09 at 11.31.37 PM.png" style="width: 673.50px; height: 394.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br><br></span></p><h1 class="c1" id="h.qgun4aqx6v09"><span class="c5">SAM </span></h1><p class="c9"><span>Paper- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2304.02643&amp;sa=D&amp;source=editors&amp;ust=1715645745399914&amp;usg=AOvVaw3g6cTasj86Zq08K4Kavpy2">[2304.02643] Segment Anything</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Thought about reading it after yacine’s </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://dingboard.com/&amp;sa=D&amp;source=editors&amp;ust=1715645745400261&amp;usg=AOvVaw2PqSj4CncA2WC5Qp9EzdRx">dingboard</a></span><span class="c3">&nbsp;success. The model basically outputs the correct segment masks after downloading the data. <br><br>The model architecture is not that complicated.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 681.50px; height: 246.88px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-10 at 9.09.24 PM.png" style="width: 681.50px; height: 246.88px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span><br><br>You first have an Image Encoder (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/docs/transformers/model_doc/vit_mae&amp;sa=D&amp;source=editors&amp;ust=1715645745400753&amp;usg=AOvVaw1qlPhKgYUEmKGD-sXTUP24">ViTMAE</a></span><span>) to generate image embeddings. &nbsp;<br><br>Then you have </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/openai/CLIP&amp;sa=D&amp;source=editors&amp;ust=1715645745400980&amp;usg=AOvVaw0OHpLI7eYawmHRuaTWe8g9">CLIP</a></span><span class="c3">&nbsp;(obviously since it’s used everywhere) to generate embeddings for the text prompts. </span></p><p class="c9"><span class="c3"><br>For bounding box or pixel prompts they use learned embeddings</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For Mask based prompts, they use convolution and then sum it up with image embeddings. <br><br>Once you have the final embeddings you feed it to a transformer based decoder that outputs a mask. They use two way cross attention (prompt with image and vice-versa) in this decoder.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>When you get the mask, your next job is to map it to the image. For that they use a simple MLP that computes the mask probability at each image location. This mask decoder part is inspired by the </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2107.06278.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745401524&amp;usg=AOvVaw1xB-alwzpN4IVcaSswJ90n">Maskformer</a></span><span class="c3">&nbsp;paper. <br></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 653.01px; height: 366.36px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-10 at 11.57.45 PM.png" style="width: 653.01px; height: 366.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I did spend some time understand it’s code (and by spending time I mean going on an evening walk talking to chatGPT to explain the piece of code I copy pasted into it before going on the walk). So basically for each pixel you predict the probabilities of the mask class it belongs to. And then you can simply group up all the pixels with same mask class to form a segment.</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.u6dvoeauvv44"><span class="c5">Qwen-VL</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2308.12966&amp;sa=D&amp;source=editors&amp;ust=1715645745402302&amp;usg=AOvVaw179lJZGQA6IJWHi43quyZo">[2308.12966] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is llava but for Qwen model by Alibaba. Some have told me it is better than llava. <br>I checked its architecture and that might be possible simply cause it uses cross-attention instead of a simple projection layer / MLP to connect CLIP to Qwen.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">However weird part is they don’t do cross attention of image embeddings and text embeddings like flamingo. <br><br>They use some learned embeddings and do cross attention with that. <br><br>I think the major reason for such architectural choices is that no one wants to modify the base LLM. </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.u5yvo3l6618g"><span class="c5">SigLIP</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2303.15343&amp;sa=D&amp;source=editors&amp;ust=1715645745403098&amp;usg=AOvVaw0P2AoXETjyhpUU_NPnM69t">[2303.15343] Sigmoid Loss for Language Image Pre-Training</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Lucas has been sharing a lot of SigLIP hype on my TL so it made sense to actually see what it is. </span></p><p class="c9"><span class="c3">As yall already know, CLIP's image encoder is used in almost every Multimodal LLM out there.So how is CLIP trained? </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Well you take embeddings from an image encoder and a text encoder and project them to the same dimensions using weight matrices. Once you have the same dimension embeddings, you simply take a dot product along with a temperature param and get a 2d matrix signifying how close each image and text pair really are.<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 663.00px; height: 501.54px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-29 at 9.10.57 AM.png" style="width: 663.00px; height: 507.00px; margin-left: 0.00px; margin-top: -5.46px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">To train CLIP, you apply Cross entropy loss on the following 2d matrix </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.06px; height: 241.14px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-29 at 9.26.41 AM.png" style="width: 671.06px; height: 241.14px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>SigLip simplifies this by changing the loss function. They use sigmoid instead of softmax and change the function so that it’s just dependent on each image-text pair rather than combination of pairs.<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 675.86px; height: 459.92px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-10-29 at 10.08.05 AM.png" style="width: 675.86px; height: 459.92px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This doesn’t seem like a big changes but what it allows you to do is remove the need for all-gather communication during the training run </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 692.50px; height: 619.75px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-11-04 041133.png" style="width: 692.50px; height: 619.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 690.95px; height: 407.88px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(23).png" style="width: 690.95px; height: 407.88px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.q4mwkz57ks8f"><span class="c5">One peace</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2305.11172.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745404948&amp;usg=AOvVaw31lKkJqiNa3d-YwinAvSoq">https://arxiv.org/pdf/2305.11172.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Mostly interested cause it’s the only multimodal one that doesn’t use CLIP lmao</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.tel1g723lkx2"><span class="c5">Make LLM do Maths</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://polymathic-ai.org/blog/xval/&amp;sa=D&amp;source=editors&amp;ust=1715645745405765&amp;usg=AOvVaw2H5Tdc2_N9jMJ6zO2RfW8w">https://polymathic-ai.org/blog/xval/</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.1xzknt4rjy32"><span class="c5">Distil-Whisper</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2311.00430&amp;sa=D&amp;source=editors&amp;ust=1715645745406261&amp;usg=AOvVaw3KWTAxWTF0ixylHMgncXlz">[2311.00430] Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Distillation is a widely popular set of techniques to reduce the size of the model. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It involves training a smaller model to predict output probs same as the larger model (not the same as ground truth). We use KL Divergence loss to ensure this. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">In Distil Whisper, they are using the same approach with some modifications </span></p><p class="c0"><span class="c3"></span></p><ol class="c4 lst-kix_aqaqvdtizx75-0 start" start="1"><li class="c9 c14 li-bullet-0"><span class="c3">They also use the normal CE loss but considering the output predicted by the teacher as ground truth</span></li><li class="c9 c14 li-bullet-0"><span class="c3">They don’t consider every output from the teach for the above loss but only the ones with WER (Word error rate) below a certain threshold. This is done to avoid hallucinated transcriptions.</span></li><li class="c9 c14 li-bullet-0"><span class="c3">The small model is decoder only (as opposed to enc-dec of whisper). Even in decoder they use only the first and last decoder layer of larger model. </span></li></ol><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.ny720xp8wd4y"><span class="c5">It’s not AGI (it’s just your data)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2311.00871&amp;sa=D&amp;source=editors&amp;ust=1715645745408511&amp;usg=AOvVaw1t3i4UAmxBZghnozzrJCWt">[2311.00871] Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.edzoxhcrwk3z"><span class="c5">Insane ML Notes on Twitter with Q&amp;A </span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>&nbsp;</span><span class="c7 c33"><a class="c10" href="https://www.google.com/url?q=https://x.com/xariusrke/status/1727254622442791400?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745409366&amp;usg=AOvVaw1FDxy-t9JglnfWq83QmUlM">https://x.com/xariusrke/status/1727254622442791400?s=20</a></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.96px; height: 325.40px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-11-29 at 11.05.16 PM.png" style="width: 604.96px; height: 325.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c31"></span></p><p class="c0"><span class="c31"></span></p><p class="c0"><span class="c31 c59"></span></p><h1 class="c1" id="h.70eni9mb9xfr"><span class="c5">Stable Video Diffusion (SVD)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets&amp;sa=D&amp;source=editors&amp;ust=1715645745410620&amp;usg=AOvVaw3GBARRYNwuW-lm_2i80diT">https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Just realized we have cracked good Video gen after watching Pika announcements. <br><br>So it makes sense that I learn how does it work especially the temporal gen and interpolation (instead of doing my day job)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">One fascinating thing I found is the amount of effort spent in data curation for training. This still remains the most important part of all the models out there. &nbsp;They filter out the videos with small motions and text. <br>They also use video BLIP, CoCo and other models to generate descriptions of the videos. Then use the same information to filter out the data</span></p><p class="c9"><span class="c3"><br>Still need to read the rest &nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 511.00px; height: 408.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(24).png" style="width: 511.00px; height: 408.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>You can simply read this paper for model architecture - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2304.08818.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745411918&amp;usg=AOvVaw2fKbIdBea1B9IhW3yvGRh0">https://arxiv.org/pdf/2304.08818.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">One more thing - why do people use this word ‘latent’ a lot. Please, stop. Use something which plebs can understand.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So here’s what the basic model is </span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_xumdcbvsxp2a-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Take your regular SD T2I or I2I model</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Flatten the video frames into a Batch so that each frame is a separate row</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Add some temporal layers in between <br>What are temporal layers? Well they can be either Convolutions or Attentions which take all the rows for a video (i.e. the frames) and can operate on them at once, then they spit out a flattened version again where each frame is a row again </span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Now doing this for all the frames of the videos will be pretty expensive especially if input has high FPS. <br><br>So what you do is extract out only key frames in the videos which represent a high semantic shift (can be done with cosine distance or like text based descriptions). Then you only use those key frames to generate more keyframes. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Once you have the results, you use another model to interpolate b/w those frames.The interpolation model is also similar to the above one but just has different layers (what exactly tho?) instead of temporal ones b/w spatial layers</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Finally they finetune the model to produce sequence of frames rather than just the next frame </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 582.00px; height: 765.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot from 2023-11-30 09-11-50.png" style="width: 582.00px; height: 765.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 619.00px; height: 583.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot from 2023-11-30 09-11-59.png" style="width: 619.00px; height: 583.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.78v1y6r9a5om"><span class="c5">Stable Diffusion Turbo (or How to distill a diffusion model 101)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://static1.squarespace.com/static/6213c340453c3f502425776e/t/65663480a92fba51d0e1023f/1701197769659/adversarial_diffusion_distillation.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745414430&amp;usg=AOvVaw3gR-rHhalvtz6AetdnknVG">Adversarial Diffusion Distillation</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 553.00px; height: 644.00px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-03 125738.png" style="width: 553.00px; height: 644.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">At first I thought it’ll be the usual Student Teacher model with a different loss function. But turns out it has one more component - a discriminator model</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Another interesting aspect is the input to the teacher model is not the original image but the diffused latents from the student model</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The Discriminator model just tries to ascertain how close the provided image is to the original image. &nbsp;</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 628.50px; height: 851.12px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(25).png" style="width: 628.50px; height: 851.12px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c13" id="h.3a6qs3r7wns7"><span class="c22 c27">I can’t hear the MUSIC* &nbsp;!!!!!!! NEEEED TO GET BETTTTTTTER!!!</span></h2><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>* &nbsp;Music here implies diffusion operations in latent space</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 699.72px; height: 393.59px;"><img alt="" src="./Keeping up with AGI_files/cover12.jpg" style="width: 699.72px; height: 393.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.x03cdvmo4owf"><span class="c5">Images are Sentences</span></h1><h1 class="c1" id="h.ixfdm4b354d8"><span class="c5">Videos are sentences</span></h1><h1 class="c1" id="h.rshzl18he64i"><span class="c5">Sentences are predictable</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2312.00785.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745417323&amp;usg=AOvVaw3Pgqx_CpDt4dAyXmJvuNWr">https://arxiv.org/pdf/2312.00785.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.r8lisb201wer"><span class="c5">Mamba - faster architecture (Reading cause Tri Dao is author)</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.00752&amp;sa=D&amp;source=editors&amp;ust=1715645745418177&amp;usg=AOvVaw1qmjvO2lPt2q14KmFKuluw">[2312.00752] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Have just started reading it but I already know this one is going to be a banger. Mostly talking about taking an existing State space model architecture and how to make the parameters time variant which according to them is the major blocker for these models not being good enough in real world.<br><br>Why hasn’t someone done this already? Well, because it increases the computational overhead and makes the models slower and if you have slow models then why not just use transformers with quadratic attention. <br><br>To solve this they have developed a hardware aware algo (mostly that decides what data should be in HBM vs SRAM) and who’s better to do this than Tri dao who wrote Flash Attention. <br><br><br>I forgive this paper for using too many mathnerdsnipes simply cause it is tbh required to understand the motivation. &nbsp;Thanks to chatGPT, here's a simple explainer for most of the terms you’ll encounter in this paper.<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 636.30px; height: 620.60px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-06 at 12.19.21 AM.png" style="width: 636.30px; height: 620.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 638.08px; height: 499.20px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-06 at 12.19.39 AM.png" style="width: 638.08px; height: 499.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Most important part of the paper is this. The takeaway for me is that memory bandwidth is so low that recomputation is faster than storing and fetching. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 640.27px; height: 439.56px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-06 at 1.45.44 AM.png" style="width: 640.27px; height: 439.56px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.qk8avpij1hrh"><span class="c5">Gemini</span></h1><p class="c9"><span class="c3">They actually released it. And it’s good?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Sort of <br><br>But tbh, benchmarks are not what interest me. You can go ahead and view them. Most evals are trash anyways and suffer from data leakage in subtle forms.<br><br>I am more interested in tips and tricks scattered across the fine prints in the paper. Tbh, they didn’t reveal much of those. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Importance of dataset quality. Also, they finally realise that users leave the app if you deny them the response. Just mention something helpful instead.<br></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 680.50px; height: 192.39px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-07 at 4.29.56 PM.png" style="width: 680.50px; height: 192.39px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>I am truly excited about Nano and what usecases will be possible on the device after its release. I consulted the </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&amp;sa=D&amp;source=editors&amp;ust=1715645745420980&amp;usg=AOvVaw0hKQw1pVAClWJHXkO6RMHp">huggingface dashboard </a></span><span>and the stats (atleast for MMLU) look great for its size.<br><br>It is also multimodal which was a surprise to me for such a small model. But then again that’s the advantage of </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://www.adept.ai/blog/fuyu-8b&amp;sa=D&amp;source=editors&amp;ust=1715645745421240&amp;usg=AOvVaw0_v3ub7KkH6_FUPFoPFn3B">Fuyu like architectures</a></span><span>&nbsp;instead of using a dedicated image encoder which makes the overall size bloated. <br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.50px; height: 462.31px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-07 at 4.12.34 PM.png" style="width: 671.50px; height: 462.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 689.50px; height: 337.09px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-07 at 4.12.46 PM.png" style="width: 689.50px; height: 337.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 691.28px; height: 406.06px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-07 at 4.19.48 PM.png" style="width: 691.28px; height: 406.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is one of the most difficult parts about training, handling hardware failures. TBH I should read all the 3 linked papers and understand what can happen (I do have some idea from OPT-175B logbook)</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 693.50px; height: 189.29px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-07 at 4.09.21 PM.png" style="width: 693.50px; height: 189.29px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Another interesting thing is that they do not checkpoint the weights to distributed store since it’ll be simply too slow (rugged by Network IO). &nbsp;Instead what they do is either keep some robust copy of weights in memory or maybe transfer the weights from one of the other nodes in the cluster (due to Model + Data parallelism)r. <br><br></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 691.50px; height: 155.49px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-07 at 5.56.44 PM.png" style="width: 691.50px; height: 155.49px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br></span></p><p class="c9"><span class="c3"><br>Also, please shield your clusters from cosmic rays. Yet another win for basement dungeon AGI enthusiasts.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 712.34px; height: 173.63px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-07 at 6.01.10 PM.png" style="width: 712.34px; height: 173.63px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.in9cs1ahq46j"><span class="c5">Mitigating LLM Hallucinations</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Karpathy tweeted about it<br><br>TBH I am aware about the RAG Based approach to ground the output in facts.<br><br>Not sure about Reflection, Verification chains, Decoding uncertainty<br><br>If I had to guess what they means</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Reflection - Simply ask LLM if this is the correct answer or not and then ask it to modify it </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Verification Chain - Maybe just a fact checker and similar tools post the LLM output before sending it to user</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Decoding uncertainty - Basically if the logprobs are lower (or you can consult from some hidden layer), the output might not be true<br></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 649.00px; height: 261.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(26).png" style="width: 649.00px; height: 261.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.je2s215h0vxj"><span class="c5">LCMs</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2310.04378&amp;sa=D&amp;source=editors&amp;ust=1715645745423865&amp;usg=AOvVaw3-rFpjV7O5e2QM1qMFUOgN">[2310.04378] Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">SD Turbo is the talk of the town but before that we had LCMs. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">OK, there is way too much maths in this paper. Time to upload it into chatGPT</span></p><p class="c9"><span><br>Update: Math too hard, went over notebook shared by the kind (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/felix_red_panda&amp;sa=D&amp;source=editors&amp;ust=1715645745424548&amp;usg=AOvVaw1r_-1PK_mI64Vo5PdOmilz">https://twitter.com/felix_red_panda</a></span><span>), now math is easy<br><br>Also one thing that this paper finally is peaking my interest to dive into various aspects of diffusion models<br>E.g. Schedulers - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://blog.segmind.com/what-are-schedulers-in-stable-diffusion/&amp;sa=D&amp;source=editors&amp;ust=1715645745424827&amp;usg=AOvVaw0mjo8CarILjdrgRoaONAt0">The ML developers guide to Schedulers in Stable Diffusion</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The basic idea is really simple - you want to train a network to predict an image from a noisy latent without going through the whole iterative sampling process.<br><br><br>Why use EMA (exponential moving average) decay rate?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/ucflc2/comment/i6a52yc/?utm_source%3Dshare%26utm_medium%3Dweb2x%26context%3D3&amp;sa=D&amp;source=editors&amp;ust=1715645745425567&amp;usg=AOvVaw2dM1AeDq2OcYHRapkgbhPi">https://www.reddit.com/r/MachineLearning/comments/ucflc2/comment/i6a52yc/?utm_source=share&amp;utm_medium=web2x&amp;context=3</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Tbh you should just read this amazing blog instead - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://naklecha.notion.site/explained-latent-consistency-models-13a9290c0fd3427d8d1a1e0bed97bde2&amp;sa=D&amp;source=editors&amp;ust=1715645745426243&amp;usg=AOvVaw34Uk0_gcwp9p-8S5zRA4ti">https://naklecha.notion.site/explained-latent-consistency-models-13a9290c0fd3427d8d1a1e0bed97bde2</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.eouqh6ecnjxs"><span class="c5">Use smol models to train large models faster</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.05328&amp;sa=D&amp;source=editors&amp;ust=1715645745427120&amp;usg=AOvVaw3-Jnpf_xPWVx_4TyNMco3T">[2312.05328] Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Although I read this paper and I get what they are trying to do, I am still not getting a sense that I actually know this. Maybe it’s because I haven’t read the linked papers about online learning. Should do that first.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">OK, after reading the next paper and gaining some intuition, I remembered I am just baka.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It’s actually quite simple. What they are trying to do is almost the same as doremi in that they use 2 smol models to select the data to be used to train large models. Their main contribution is how smol can we make these two models so that overall we save FLOPs rather than spending less in training but overall more in total. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>They simply keep on running this in an actor model (which is actually quite popular in distributed computing if you have worked with spark etc.). Your workers keep on running in parallel and compute scores for the samples from datasets. The score is nothing but the cross-entropy loss of the scorer model and reference model. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 657.33px; height: 290.97px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-20 at 10.01.10 AM.png" style="width: 657.33px; height: 290.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">You keep on updating the scores in a memory bank (can be any DB). &nbsp;</span></p><p class="c9"><span class="c3">You then use these scores to sample the data, thus prioritizing examples that would actually help large model to learn something new instead of simply repeating it.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 669.38px; height: 433.75px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-20 at 10.03.35 AM.png" style="width: 669.38px; height: 433.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">In the end you update the weights of both reference model and large model. <br>Why both? Cause what makes all this work is that the loss trend of smol reference models serves as good enough proxy for the loss trend of large models.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Finally they keep on &nbsp;reducing the scorer and reference model size until the training regime is compute positive (i.e. takes fewer FLOPs w.r.t. Learner). As you can notice the learn obviously takes longer to train as reference models get smaller but the hit is not large <br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 665.50px; height: 276.53px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-20 at 10.05.45 AM.png" style="width: 665.50px; height: 276.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.tt3rvz6eqay2"><span class="c5">DoReMI</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.10429&amp;sa=D&amp;source=editors&amp;ust=1715645745430060&amp;usg=AOvVaw3x1eX2RqU-bMRdldUnoyPe">[2305.10429] DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The idea is simple. You want optimal data distribution in pretraining. By optimal I mean the samples that lead to lowest loss as early as possible.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">How to achieve that? <br><br>Well the intuition (a good one) in most of these papers is that smol models, although not great at producing output, are still good enough proxy to dictate how large models are gonna behave. So e.g. if a small model trained on dataset X l has high loss on some dataset Y, then it might be the same for a large model as well trained on the same dataset X. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Here we use 2 small models - one is a reference model that is petrained using normal sampling weighted by token count. Second is the proxy model. The whole trick lies in how to train this proxy. <br><br>We do that by first starting with uniform sample distribution over domains, doing a normal forward pass for a batch, taking difference of loss b/w this and reference model for each domain. Now we select the worst domain loss among these (max) and try to adjust the alpha weights (for sampling) to minimize this. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">After T steps, we simply take the average of all the alpha weights per domain and use that as sampling from the large model. <br></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 673.78px; height: 423.74px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-19 at 9.17.48 AM.png" style="width: 673.78px; height: 423.74px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 681.15px; height: 362.53px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-19 at 9.25.06 AM.png" style="width: 681.15px; height: 362.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Does it work?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Yes, as you can see at just 80K steps the model is performing better than baseline at 160K<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 677.50px; height: 466.83px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-19 at 9.34.27 AM.png" style="width: 677.50px; height: 466.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>I KEEP ON FORGETTING THAT I ACTUALLY NEED TO LEARN ABOUT DPO AND PPO AND OTHER SHIT (although I did make </span><span class="c7"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.5avjijk7fbrh">an attempt</a></span><span class="c3">&nbsp;earlier in this doc for DPO)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 516.79px; height: 516.79px;"><img alt="" src="./Keeping up with AGI_files/1702890557184676-transformed.jpeg" style="width: 516.79px; height: 516.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Why did I suddenly have this thought? Cause they released </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/mhdang/dpo-sd1.5-text2image-v1&amp;sa=D&amp;source=editors&amp;ust=1715645745432896&amp;usg=AOvVaw1gSzXVVW54OmoGsRDRJhPo">DPO fine tuned models for SD</a></span><span class="c3">&nbsp;which are actually good at prompt following!!! Ya khuda, no more ((masterpiece)), ((high-res)), 5 fingers, bullshit.</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2311.12908&amp;sa=D&amp;source=editors&amp;ust=1715645745433209&amp;usg=AOvVaw07fytYum4NBezjhun9zih1">[2311.12908] Diffusion Model Alignment Using Direct Preference Optimization</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.t7jrdjgdu018"><span class="c5">LLM Paper from Apple?? : That’s a rare sight</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.11514&amp;sa=D&amp;source=editors&amp;ust=1715645745433715&amp;usg=AOvVaw3apOTID_IRVgcYHuFllPqt">[2312.11514] LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So apple is def serious about running them in device. What they are trying to achieve is how to run models larger than the available memory. TBH, we already run all programs with memory usage larger than RAM (using virtual memory and paging)<br><br>Why can’t we have a similar thing for LLMs? Cause it’ll make inference slower.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">But smol devices have a big advantage, they simply use flash storage which is quite fast if used in the right manner. The focus of this paper is on this part.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They are relying on two important properties here:<br></span></p><p class="c9"><span class="c3">1. The FFN (not attention) weights of most LLMs are sparse</span></p><p class="c9"><span class="c3">2. The time to first byte from disk &gt; time to read data</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Now to take leverage of these two props </span></p><p class="c0"><span class="c3"></span></p><ol class="c4 lst-kix_1zrk6ew5gg6c-0 start" start="1"><li class="c9 c14 li-bullet-0"><span class="c3">They only load weights which give non-zero activation to save memory</span></li><li class="c9 c14 li-bullet-0"><span class="c3">They read more data than necessary from flash and then discard the non-useful one. </span></li></ol><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">To read only non-zero weights, they are using a low rank predictor that simply tells beforehand which neurons will give positive activations. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>To read more data, they load both up_proj and down_proj matrices and keep them in a single row. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 724.31px; height: 527.31px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-22 at 8.02.16 AM.png" style="width: 724.31px; height: 527.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>When I read the benchmark setup of this paper tho, I get an ick. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 726.50px; height: 133.41px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-22 at 8.00.29 AM.png" style="width: 726.50px; height: 133.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The memory management def seems neat - they ensure continuous allocation in a pre-allocated memory region. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 725.50px; height: 408.41px;"><img alt="" src="./Keeping up with AGI_files/x12.png" style="width: 725.50px; height: 408.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 727.88px; height: 284.63px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-22 at 8.23.19 AM.png" style="width: 727.88px; height: 284.63px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.qy665v47dt8m"><span class="c5">Multimodal paper from Apple???</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2310.07704&amp;sa=D&amp;source=editors&amp;ust=1715645745435868&amp;usg=AOvVaw0_7BVsKg6VvokpCUilOwxN">[2310.07704] Ferret: Refer and Ground Anything Anywhere at Any Granularity</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Initially I thought this paper was mid cause it was using Vicuna which hasn’t been sota for like a year now. But now that I read it, the point is not about the base model, it’s about the technique being used to ground the model.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">If you’ve read GPT 4-V, Gemini or Llava whitepapers, you will know the achilles heel of all these models is the ability to not parse as well as create bounding box correctly. Tbf, GPT-4V still can parse but creating is not it’s strong suite.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Here, they are trying to tackle only the parsing the bounding box part. Most of the magic is in how they represent it in the first place. </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They are using a special MLP based sampler which instead of representing the box as a set of coordinates, represent it as using a set of modified features. &nbsp; </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 709.86px; height: 399.30px;"><img alt="" src="./Keeping up with AGI_files/GCNYzL1aIAAKce6.jpeg" style="width: 709.86px; height: 399.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.8juh78if2ead"><span class="c5">Amazing paper to Learn about Dingboard</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2312.14385&amp;sa=D&amp;source=editors&amp;ust=1715645745437103&amp;usg=AOvVaw1l4xJEBG48G-froPPnYrFv">Paper page - Generative AI Beyond LLMs: System Implications of Multi-Modal Generation</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It presents common architectures powering the Text to image space and what are the common bottlenecks. It’s by FAIR meta so actually useful content rather than it being a blog post in PDF form.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 704.43px; height: 316.79px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-26 at 11.51.29 AM.png" style="width: 704.43px; height: 316.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 704.50px; height: 597.14px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-26 at 11.51.35 AM.png" style="width: 704.50px; height: 597.14px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Deepseek, YAYI-30B, WaveCoder <br><br>Seems like I should invest some time to learn about dataset creation and filter pipelines. Lots of alpha in case I actually choose to earn money via ML consulting<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 716.50px; height: 537.38px;"><img alt="" src="./Keeping up with AGI_files/1703442268449309.jpg" style="width: 716.50px; height: 537.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.mjdcoi7mufq0"><span class="c5">TDM edge Multimodal arc (I blame Vik)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c41">LISTEN TO ME RN!!!<br>EDGE AI<br>THAT’S IT<br>VTUBER WAIFU IN YOUR GLASSES<br>JUST BET ON MOBILE COMPUTE GETTING BETTER AND BETTER!!!</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 544.00px; height: 426.00px;"><img alt="" src="./Keeping up with AGI_files/1703801205577520.png" style="width: 544.00px; height: 426.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2312.17172&amp;sa=D&amp;source=editors&amp;ust=1715645745438425&amp;usg=AOvVaw1KDzZiB5ASkRnHKxKvxm0a">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2312.16862&amp;sa=D&amp;source=editors&amp;ust=1715645745438769&amp;usg=AOvVaw1z-LGtNx6gVy5K1rG8nVPV">Paper page - TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2312.16886&amp;sa=D&amp;source=editors&amp;ust=1715645745439094&amp;usg=AOvVaw0_OKVehbDcHI0AavHoc3u6">Paper page - MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.m6x3qi8j3soy"><span class="c5">MobileVLM</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2312.16886&amp;sa=D&amp;source=editors&amp;ust=1715645745439533&amp;usg=AOvVaw3nvOVwWu9ju9gqkLsSYH8C">Paper page - MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Loved this cause they actually tried to optimise the model to be able to run on snapdragon devices. </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 921.00px; height: 468.23px;"><img alt="" src="./Keeping up with AGI_files/IMG_9278.PNG" style="width: 946.00px; height: 2048.56px; margin-left: 0.00px; margin-top: -1145.22px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 873.06px; height: 1822.37px;"><img alt="" src="./Keeping up with AGI_files/IMG_9275.PNG" style="width: 880.04px; height: 1903.76px; margin-left: -6.98px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Apologies for the phone screenshot I was reading this in the mall while waiting for someone</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.2j5bz5aufu9r"><span class="c5">MathPile</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2312.17120&amp;sa=D&amp;source=editors&amp;ust=1715645745440381&amp;usg=AOvVaw1c4knnaCCUT8HaEDnek6q1">Paper page - Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Only reason you should read dataset papers is to just find out what they use to clean up the set. More often than not I just skip to that section.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 346.50px; height: 614.01px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-31 at 1.26.22 AM.png" style="width: 346.50px; height: 614.01px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 308.50px; height: 315.95px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-31 at 1.39.31 AM.png" style="width: 308.50px; height: 315.95px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span><br></span><span class="c29">For contamination detection:</span><span class="c3"><br><br>We employed line-level exact match detection for both our corpus and test sets, as the questions in these benchmarks are generally brief and often contained within a single line. Specifically, we split documents into lines, hashed each line using MD5, and took the first 64 bits along with the corresponding line to form a set. This procedure was also applied to the constructed reference test set collection. If a line from the test set, along with its corresponding hash code, is found in the training set’s corresponding set, and the length of the line is over 50 characters,13 we classify it as a leaked sample with an exact match</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.muhdaz5l8bei"><span class="c5">Unified-IO 2</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2312.17172&amp;sa=D&amp;source=editors&amp;ust=1715645745441309&amp;usg=AOvVaw2Qtc-Lv-fUXAVPyBpO5qFe">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is quite close to what Gemini is doing for multimodal gen<br><br>Uses llama tokenizer for text</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Uses the concatenated output from 2nd and 2nd last layer of ViT for images<br></span></p><p class="c9"><span class="c3">For audio, it is a bit more complex but I am pretty sure it would be like 1-2 ffmpeg commands. The main funda is you need to just need to 1. Encode and 2. Project into 3. 1-d space<br></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 706.50px; height: 315.99px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2024-01-03 at 3.16.07 AM.png" style="width: 706.50px; height: 315.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.x0s9sfo234op"><span class="c5">DocLLM</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2401.00908&amp;sa=D&amp;source=editors&amp;ust=1715645745442495&amp;usg=AOvVaw11Mh7x_h1LqLKFbaso2NuM">Paper page - DocLLM: A layout-aware generative language model for multimodal document understanding</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Just like the top comment on this page I was expecting this paper to be a snoozefest (banks, pffttt)<br></span></p><p class="c9"><span class="c3">But they really tried something other than prompt engineering a finetuning llama (tbh it is llama2-7B but not vanilla arch)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">That being said, some details are def missing from this paper cause well, it’s a bank so needs to think everything they do is a secret</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Basically they first use some OCR code (plenty of libs in OSS), to get text and their corresponding bounding boxes from the docs.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Next - they need some way to encode these bounding boxes (not mentioned clearly in the paper), once encoded they change the attention block to use different Q and K matrices for bounding boxes. You can think of this part like a simplified cross &nbsp;attention.<br><br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 802.00px; height: 410.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(27).png" style="width: 802.00px; height: 410.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Seems like a follow up paper to this - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2401.02823&amp;sa=D&amp;source=editors&amp;ust=1715645745443667&amp;usg=AOvVaw2PIBAfa_QNZaGRhQAY48E6">Paper page - DocGraphLM: Documental Graph Language Model for Information Extraction</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.5z79snx8iy8d"><span class="c5">Microsoft broke MTEB</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.00368&amp;sa=D&amp;source=editors&amp;ust=1715645745444143&amp;usg=AOvVaw1aA0d-7ozunkX7ASF1tMYg">[2401.00368] Improving Text Embeddings with Large Language Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.puqljqtpn7jp"><span class="c5">Reading List from AHM<br></span></h1><p class="c9"><span class="c3">Dataset Best Papers</span></p><ul class="c4 lst-kix_66y50s42p1gp-0 start"><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2206.14486&amp;sa=D&amp;source=editors&amp;ust=1715645745444706&amp;usg=AOvVaw0EC5s9U4kctXx8O4zJRAjq">Beyond neural scaling laws: beating power law scaling via data pruning</a></span></li><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2303.09540&amp;sa=D&amp;source=editors&amp;ust=1715645745445011&amp;usg=AOvVaw3mNAvd-QWcRwrINL2QprYz">SemDeDup: Data-efficient learning at web-scale through semantic deduplication</a></span></li><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2308.12284&amp;sa=D&amp;source=editors&amp;ust=1715645745445268&amp;usg=AOvVaw39ZR19ieJGraiTNRWryX_K">D4: Improving LLM Pretraining via Document De-Duplication and Diversification</a></span></li><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2310.15393&amp;sa=D&amp;source=editors&amp;ust=1715645745445601&amp;usg=AOvVaw23L1ThHtxsssCu0kJhRpSE">DoGE: Domain Reweighting with Generalization Estimation</a></span></li><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.15685&amp;sa=D&amp;source=editors&amp;ust=1715645745445896&amp;usg=AOvVaw1AdUyuWsYBd5HnNeVhdYYg">What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning</a></span><span>&nbsp;→ associated github repo for DEITA (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/hkust-nlp/deita&amp;sa=D&amp;source=editors&amp;ust=1715645745446080&amp;usg=AOvVaw3QlwIps4EWgoHCx5xdYc1H">https://github.com/hkust-nlp/deita</a></span><span class="c3">)</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Hallucination minimisation and Refusal on not knowing the answer</span></p><ul class="c4 lst-kix_mbxt44yhtx4c-0 start"><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.18153&amp;sa=D&amp;source=editors&amp;ust=1715645745446441&amp;usg=AOvVaw2K83JrlokpiziL1VyQ2VFM">Do Large Language Models Know What They Don't Know?</a></span></li><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2311.09677&amp;sa=D&amp;source=editors&amp;ust=1715645745446691&amp;usg=AOvVaw3Nt23A9xVbd5y-Fs9vkpqT">R-Tuning: Teaching Large Language Models to Refuse Unknown Questions</a></span><span class="c3">&nbsp;</span></li><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://cookbook.openai.com/examples/using_logprobs&amp;sa=D&amp;source=editors&amp;ust=1715645745446955&amp;usg=AOvVaw1ZlmTcpO33Hz4_vuwxCVIH">https://cookbook.openai.com/examples/using_logprobs</a></span></li><li class="c9 c14 li-bullet-0"><span>John Schulman’s talk on teaching models to identify uncertainty, 16:52 - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://www.youtube.com/live/hhiLw5Q_UFg?si%3Dl5I4Wox-L97UYxDs%26t%3D1007&amp;sa=D&amp;source=editors&amp;ust=1715645745447275&amp;usg=AOvVaw0Wn890T8DPZinysz41q5c3">https://www.youtube.com/live/hhiLw5Q_UFg?si=l5I4Wox-L97UYxDs&amp;t=1007</a></span><span class="c3">&nbsp;</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Transformers from a Maths perspective (not including finbarr, eleuther ai maths)</span></p><ul class="c4 lst-kix_1lunem3k9g20-0 start"><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.10794&amp;sa=D&amp;source=editors&amp;ust=1715645745447680&amp;usg=AOvVaw396dBWY-U4-Y3Fy_IWKQHV">A mathematical perspective on Transformers</a></span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Vikp’s work with dataset prep and related stuff</span></p><ul class="c4 lst-kix_lcf5q6ulbeb6-0 start"><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://x.com/VikParuchuri/status/1730357379194400803?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745448110&amp;usg=AOvVaw2Jv5r3clQsobZTANvaOq6E">marker</a></span><span class="c3">&nbsp;</span></li><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://x.com/VikParuchuri/status/1742036923668132280?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745448381&amp;usg=AOvVaw2QpjDCfqUmdVqAuABno02y">His other major works</a></span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">&nbsp;Factual Grounding methods</span></p><ul class="c4 lst-kix_y1s77pjhafgq-0 start"><li class="c9 c14 li-bullet-0"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.01313&amp;sa=D&amp;source=editors&amp;ust=1715645745448769&amp;usg=AOvVaw3u1fNrA4iOg2hMTFPc9kjJ">A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models</a></span></li></ul><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.b8pgdoszx5d0"><span class="c5">Reading List from Yacine</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://dreamtuner-diffusion.github.io/&amp;sa=D&amp;source=editors&amp;ust=1715645745449399&amp;usg=AOvVaw1XtY3YFZVN-MJbC0pKbXlz">DreamTuner</a></span><span class="c3">&nbsp;- ipadapter but different</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2312.13789.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745449751&amp;usg=AOvVaw2QBunJO-JbX8dEC9dOoFAX">https://arxiv.org/pdf/2312.13789.pdf </a></span><span class="c3">- how i beat the big wigs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.09608&amp;sa=D&amp;source=editors&amp;ust=1715645745450168&amp;usg=AOvVaw13YDVN8YEfyzDJg19IHGQb">[2312.09608] Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</a></span><span class="c3">&nbsp;- faster stable diffusion by skipping unecessary bits</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/damo-vilab/AnyDoor&amp;sa=D&amp;source=editors&amp;ust=1715645745450481&amp;usg=AOvVaw2Mj1Asafjkq0LhXsGcY3fa">Official implementations for paper: Anydoor: zero-shot object-level image customization</a></span><span>&nbsp;- instruct edit + </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://old.reddit.com/r/StableDiffusion/comments/18kd0na/code_for_anydoor_zeroshot_objectlevel_image/&amp;sa=D&amp;source=editors&amp;ust=1715645745450734&amp;usg=AOvVaw24DXkYYIdhs5Q2C5P2tgL6">https://old.reddit.com/r/StableDiffusion/comments/18kd0na/code_for_anydoor_zeroshot_objectlevel_image/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>dont forget - diffusion slider demo </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/Kevin-thu/DiffMorpher?tab%3Dreadme-ov-file&amp;sa=D&amp;source=editors&amp;ust=1715645745451088&amp;usg=AOvVaw2s2YqPrgIZ38kJZYWv9J6M">https://github.com/Kevin-thu/DiffMorpher?tab=readme-ov-file</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2312.01943.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745451485&amp;usg=AOvVaw2YGhCXR0yDf_af_XmGefwi">https://arxiv.org/pdf/2312.01943.pdf </a></span><span class="c3">- i should use this for anime</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/spaces/JingyeChen22/TextDiffuser-2&amp;sa=D&amp;source=editors&amp;ust=1715645745451916&amp;usg=AOvVaw2j_TNGhRpZLP9Dzp_hjTQW">TextDiffuser 2 - a Hugging Face Space by JingyeChen22</a></span><span class="c3">&nbsp;- people have been asking for text.. right?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/open-mmlab/PIA&amp;sa=D&amp;source=editors&amp;ust=1715645745452242&amp;usg=AOvVaw2Z6XtWRVgs4wFhr8NH1B27">GitHub - open-mmlab/PIA: PIA, your Personalized Image Animator. Animate your images by text prompt, combing with Dreambooth, achieving stunning videos. PIA，你的个性化图像动画生成器，利用文本提示将图像变为奇妙的动画</a></span><span class="c3">&nbsp;- video generator</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/cumulo-autumn/StreamDiffusion&amp;sa=D&amp;source=editors&amp;ust=1715645745452565&amp;usg=AOvVaw2wJnxMRUYCZZyHZRALeVd9">GitHub - cumulo-autumn/StreamDiffusion: StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation</a></span><span class="c3">&nbsp;- turbo go fast</span></p><p class="c0"><span class="c3"></span></p><h1 class="c1 c24" id="h.hdss4fi1felh"><span class="c5"></span></h1><h1 class="c1" id="h.at9212mywj8h"><span class="c5">LASERRRRR (for reasoning)</span></h1><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.13558&amp;sa=D&amp;source=editors&amp;ust=1715645745453121&amp;usg=AOvVaw2gNn78h1yKzTJpMHoI_Cvi">[2312.13558] The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">wowowowow</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 721.95px; height: 338.55px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2024-01-08 at 11.27.18 PM.png" style="width: 721.95px; height: 338.55px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 729.50px; height: 295.78px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2024-01-08 at 9.17.03 AM.png" style="width: 729.50px; height: 295.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2401.02669&amp;sa=D&amp;source=editors&amp;ust=1715645745454001&amp;usg=AOvVaw3TQ9o_dlnhw2h59lDv-0e4">Paper page - Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache</a></span><span class="c3">&nbsp;- Mostly engineering hence I love it</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.01325&amp;sa=D&amp;source=editors&amp;ust=1715645745454358&amp;usg=AOvVaw028hzyQacxmh8p6bdLLN02">[2401.01325] LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</a></span><span class="c3">&nbsp;- cause surya implemented it and it worked, high signal</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.02954&amp;sa=D&amp;source=editors&amp;ust=1715645745454770&amp;usg=AOvVaw2999VPrUZbLwavbOnUARfo">[2401.02954] DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</a></span><span class="c3">&nbsp;- Mostly to see scaling laws and hyperparams choice</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.00588&amp;sa=D&amp;source=editors&amp;ust=1715645745455072&amp;usg=AOvVaw1-kvIuOIyJNOAZp5ZagqTd">[2401.00588] Fairness in Serving Large Language Models</a></span><span class="c3">&nbsp;- Scheduler by lmsys</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2401.00448&amp;sa=D&amp;source=editors&amp;ust=1715645745455371&amp;usg=AOvVaw1oxPIFea8NgFWEFlhdTKRl">Paper page - Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws</a></span><span class="c3">&nbsp;- hmmm</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.01335&amp;sa=D&amp;source=editors&amp;ust=1715645745455676&amp;usg=AOvVaw3-bJ0QvFy3PDbn55Z4Ho2L">[2401.01335] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</a></span><span class="c3">&nbsp;- someone was talking about in a GC, high signal</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">gonna spend more time writing LLM code now, this is a fundamental blocker now in my path forward to actually becoming a good assistant to ML bros </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 588.50px; height: 582.46px;"><img alt="" src="./Keeping up with AGI_files/GEVkidJXoAAYsbX.jpeg" style="width: 588.50px; height: 582.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.q9qx67yy3hzn"><span class="c5">Embarrassing myself publicly arc (PHOTOMAKER)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>I was playing around with Tencent Photomaker for a few days. I am blown away by how good it is with faces so I naturally went ahead and read &nbsp;their paper<br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.04461&amp;sa=D&amp;source=editors&amp;ust=1715645745456661&amp;usg=AOvVaw1KuclV7EyScROL2ijjLCvF">[2312.04461] PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">What I realised after reading is it, I really do not understand how it works. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">What they are doing is pretty simple, they have trigger words and class words</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The trigger words can be something like </span><span class="c26">img</span><span>&nbsp;and class words can be </span><span class="c26">man</span><span>, </span><span class="c26">woman</span><span>, </span><span class="c26">boy</span><span class="c3">&nbsp;etc.<br><br>The class word should always be followed by the trigger word.<br><br>Whenever a &nbsp;trigger word is encountered in the prompt, they remove it, take the features of the class word preceding it and then do the tensor jujitsu <br><br>What technique? They take the embeddings of the face images you uploaded, project them into the same dimensions as text embeddings and then fuse them with the embeddings of the class word. &nbsp;</span></p><p class="c9"><span class="c3"><br>This is the part that kept on worrying me, like how does this even work, why is face information simply not lost, I was honestly expecting a control net like thing. <br><br>Then I proceeded to read their code and that made me question my skills further. Reason being I was not familiar with a few torch methods they were using and also the code is shitty in general. </span></p><p class="c9"><span class="c3"><br>Then in the paper they mentioned that it actually works simply cause SD already has Cross-attention which takes care of mixing face id info with image.<br></span></p><p class="c9"><span>So rn I am going through the whole SDXL architecture in colab (it’s embarrassing that I haven’t done that at all) and trying to understand this flow. The fact that I don’t know this already is so baaaaaaaad.<br><br></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 626.20px; height: 561.84px;"><img alt="" src="./Keeping up with AGI_files/1706054041379969.jpg" style="width: 626.20px; height: 561.84px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Today I was trying to verify if they are lying to us in SDXL paper. Turns out I am just dumb and forgot they concatenate the embeddings from two CLIP models </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 645.48px; height: 149.16px;"><img alt="" src="./Keeping up with AGI_files/figure6.png" style="width: 645.48px; height: 149.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.q1hqsepas7wp"><span class="c5">Lumiere</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2401.12945.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745458530&amp;usg=AOvVaw3389ERMUKB-9L7u1xhAS5G">https://arxiv.org/pdf/2401.12945.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Video gen model by goog. They are not relying on the </span><span class="c7"><a class="c10" href="https://docs.google.com/document/d/e/2PACX-1vQD8IlBotGdBxp3BnXkSjk8bNZlPV_0EH9ZA6wHd5dNf-BLSiwXUinvgv8ZoBEnNyTCF-chWO30NRw0/pub#h.70eni9mb9xfr">SVD</a></span><span>&nbsp;(Stable Video Diffusion and not Singular Value Decomposition) way to generate only keyframes and interpolate b/w them. They instead generate all frames in a single pass.<br><br>I am worried tho that this will have huge mem requirements though. Will read the paper to understand more.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 2.67px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 2048.00px; height: 1086.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(28).png" style="width: 2048.00px; height: 1086.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ok, so the way they are avoiding huge compute requirements is basically via doing temporal convolutions on very small latents. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 703.00px; height: 274.23px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(29).png" style="width: 703.00px; height: 297.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Once you have video at course resolution it is upscaled using MultiDiffusion (whatever that is, I need to read)<br><br></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 710.50px; height: 566.72px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(30).png" style="width: 710.50px; height: 566.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 718.00px; height: 748.02px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(31).png" style="width: 718.00px; height: 767.00px; margin-left: 0.00px; margin-top: -18.98px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.27dzbrc148zx"><span class="c5">Deepseek Coder</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.14196&amp;sa=D&amp;source=editors&amp;ust=1715645745460167&amp;usg=AOvVaw2dSX2a80qkxEHL2JlPr-2P">[2401.14196] DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Lots of alpha in this paper, asian bros I kneel</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 751.50px; height: 183.18px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(32).png" style="width: 751.50px; height: 183.18px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Thanks for being inclusive for us Java programmers</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 758.50px; height: 205.89px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(33).png" style="width: 758.50px; height: 205.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">You would think it’s just topo sort right? But then us programmers are so shit we do cyclic dependencies such as passing factory instance to implementation so it can create new instances for some recursion hell.<br><br>Worry not, the asian bros know about this practice. Interesting thing that they kept the file path info in comments as well, not sure why that is important.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 745.50px; height: 257.07px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(34).png" style="width: 745.50px; height: 257.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>They then dedup at repo level (i.e. concatening all the files of the repo and applying some near-dedup algo). It should be similar to cosine similarity but I need to see the exact implementation for this. Expecting something similar to this - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/blog/dedup&amp;sa=D&amp;source=editors&amp;ust=1715645745461398&amp;usg=AOvVaw0olITc0y2TGbwtGg-IRWzz">Large-scale Near-deduplication Behind BigCode</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Using smol models to estimate loss curve trends / accuracy of larger models should be widely known. Don’t waste compute unnecessarily. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 736.49px; height: 75.26px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(35).png" style="width: 736.49px; height: 75.26px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I will never not be amazed that CoT actually works in the LLMs</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 748.39px; height: 174.83px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(36).png" style="width: 748.39px; height: 174.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.kianmoajn1vd"><span class="c5">IPAdapter</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2308.06721&amp;sa=D&amp;source=editors&amp;ust=1715645745462431&amp;usg=AOvVaw0j8nranVly8Cb_uyI0xZqy">[2308.06721] IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Been using this model and impressed by its performance in local setup. It does a really simple thing tbh, where it allows you to prompt a diffusion model with images (not to be confused with i2i as it simply influences the style rather than the exact outlines)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 777.44px; height: 367.71px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(37).png" style="width: 777.44px; height: 367.71px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Their approach is quite simple. Diffusion models already use Cross Attention to condition the latents using text. <br>You can just add another cross attention layer that instead of using text embeddings as key and value, uses image embeddings as key and value <br><br>Once done, &nbsp;you can add the text and image cross attention output and get the final result<br><br></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 781.50px; height: 583.56px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(38).png" style="width: 781.50px; height: 583.56px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.a56vgc4qjq11"><span class="c5">How to create AGI?</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2401.14953&amp;sa=D&amp;source=editors&amp;ust=1715645745463496&amp;usg=AOvVaw1B_Z-74ZXNaQKYTXvE9Oo6">[2401.14953] Learning Universal Predictors</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>This paper has too much math but it is expected because of what they are trying to prove. <br>I am bad at it so taking help of GPT to guide me through the explanations (embarrassing myself (complementary)) - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://chat.openai.com/share/fdaaa434-011b-4da6-a3a2-d69ddc8c3180&amp;sa=D&amp;source=editors&amp;ust=1715645745463992&amp;usg=AOvVaw1TpOQRjBWIWo02Uu1hEbZR">https://chat.openai.com/share/fdaaa434-011b-4da6-a3a2-d69ddc8c3180</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The primary idea that they are trying to put forward is LLMs in itself can do meta learning (which is fancy way of saying if you give them context, they can figure out a solution from that concept even if they haven’t seen it before). And the way to make meta learning efficient according to them is to have enough diverse set of problems already part of pretraining.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">To prove this, what they do is that using Brainfuck (yeah, really, brainfuck) as the primary language for its simplicity (as in it just reads from one place and writes to another with a small working memory) as opposed to langs like java which can have read from multiple channels and do random access.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>For problem diversity, the ensure multiple tasks from different complexity are part of the training dataset<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 721.18px; height: 461.70px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(39).png" style="width: 721.18px; height: 461.70px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 795.50px; height: 293.48px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(40).png" style="width: 795.50px; height: 293.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span><br><br>I still feel overwhelmed by the mathematical terms in this paper, gonna switch to reading code instead </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/google-deepmind/neural_networks_solomonoff_induction/tree/main&amp;sa=D&amp;source=editors&amp;ust=1715645745465071&amp;usg=AOvVaw02XSeQQxnM6VrNQsfaFCnZ">https://github.com/google-deepmind/neural_networks_solomonoff_induction/tree/main</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So the code is really simple tbh, they are just sampling tasks with Brainfuck lang code. The samples are ensured to be diverse enough via randomisation.<br><br>This is done per batch and then used to train the network<br><br>Then the nets are evaluated against other types of tasks (specifically chomsky hierarchy ones and CTW ones). We observe the accuracy does increase for each model as their sizes are grown even if they are not trained on this dataset (hence, AGI)</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 840.65px; height: 372.30px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(41).png" style="width: 840.65px; height: 372.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>As a followup I would also need to read </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/1905.03030&amp;sa=D&amp;source=editors&amp;ust=1715645745465835&amp;usg=AOvVaw3Pla9l5TLSiPy4TIw3Wu3o">[1905.03030] Meta-learning of Sequential Strategies</a></span><span class="c3">&nbsp;so i can understand the whole meta-learning thing a bit better</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.sd5qq3bqy2xo"><span class="c5">ILYA’s READING LIST (For getting up to speed on today’s architectures)</span></h1><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE&amp;sa=D&amp;source=editors&amp;ust=1715645745466499&amp;usg=AOvVaw2FHUFZcUieF4pilFnwRJan">https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.2hp09tg85euh"><span class="c5">Stream Diffusion - Brrrrr ImageGen at 100FPS</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.12491&amp;sa=D&amp;source=editors&amp;ust=1715645745467126&amp;usg=AOvVaw01ZCPpnnYVQlE1YhwAVjC1">[2312.12491] StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Allows you to either use SD + LCM lora OR SD-Turbo to do insanely fast generation</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Doesn’t support SDXL yet<br><br>How come it is so fast tho?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Turns out it's just simple old optimisations you would do for you Java app as well</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">- &nbsp;multiple threads to offload lightweight computations (such as encoding/decoding images)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">- pipelining with batches to allow process multiple images in a single pass</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">- cache to store precomputed text prompt embddings as well as KV for those embeddings in cross-attention layer</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>- a bit of magic (</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://developer.nvidia.com/tensorrt&amp;sa=D&amp;source=editors&amp;ust=1715645745468913&amp;usg=AOvVaw27-73KALoZsNzRwFpyIqum">tensorrt</a></span><span class="c3">)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 689.17px; height: 335.50px;"><img alt="" src="./Keeping up with AGI_files/GFcLOFkW8AAj_Pc.jpeg" style="width: 689.17px; height: 335.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 692.98px; height: 438.86px;"><img alt="" src="./Keeping up with AGI_files/GFcLSl4WAAAdF6y.jpeg" style="width: 692.98px; height: 438.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 697.50px; height: 462.78px;"><img alt="" src="./Keeping up with AGI_files/GFcLcHkWUAADFgT.png" style="width: 697.50px; height: 462.78px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.h0f2as8iyimw"><span class="c5">MLLM-Guided Image Editing (MGIE)</span></h1><p class="c9"><span class="c3">Paper from Apple (along with the code, goddamn). Imagine those cool demos at google io where you can magically remove things from photos. Well this is a superior version of that. <br>Not only you can add/remove/move anything you want, but you can also do it via simple natural language instructions instead of using your fingers.<br></span></p><p class="c9"><span class="c3">The basic architecture is quite simple - &nbsp;they are just using a VLM (or as they say MLLM) to get description of the image with the edits requests. E.g. What would the image look like if I add a pizza on the table.<br><br>Now the answers of VLMs can be quite long, so they have this magical summariser that shortens the answers to make them more precise. <br><br>Now when you get this answer, they also add some [IMG] tokens. These tokens are used as input to anothersequence model that transforms them into embeddings. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Now you can use I2I mode of diffusion models. But we condition this image not on the prompt but on the embeddings we got in the previous step (using inbuilt cross-attention of most diffusion models)<br></span></p><p class="c9"><span class="c3">Now they train this whole pipeline to minimise two losses - </span></p><ul class="c4 lst-kix_dy2rea82z3y1-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">L(ins) where you ensure the shorter summarised instructions matches the expectations</span></li><li class="c9 c14 li-bullet-0"><span class="c3">L(edit) where you ensure the edited image matches the expected output</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 702.09px; height: 391.84px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(42).png" style="width: 702.09px; height: 391.84px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br><br></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 719.53px; height: 159.81px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(43).png" style="width: 719.53px; height: 159.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.tqy4w8t8csw2"><span class="c5">Matryoshka Embeddings</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2205.13147&amp;sa=D&amp;source=editors&amp;ust=1715645745470878&amp;usg=AOvVaw2SJ7O1bvoTE2omsoPjLIrV">[2205.13147] Matryoshka Representation Learning</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Hearing this term too frequently in the contexts that are most definitely not related to dolls (if they are then it’s concerning)<br></span></p><p class="c9"><span class="c3">Basically in the existing embedding models, you get floating-point vectors of fixed lengths (1536 for openai, 768 for clip, etc.). However sometimes you wish you had smaller vectors (to tradeoff accuracy for latency). Changing models just for embedding length sounds tedious<br><br>Enters Matryoshka</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The concept is pretty simple, generally you train embedding models by training classifiers and then taking hidden layer representation. <br><br>Now what if instead of just using the full output of the last layer to compute logits and calculate loss, you instead did it for multiple length vectors e.g. 8,16,32,64…2048 <br><br>Then you simply added up the loss for each of these and tried to gradient descent on this cumulative loss.<br><br>Well, yes, it’s as simple as that and it works perfectly. &nbsp;Using this loss formulation, the network tries to compress any coarse info in the smaller length vectors and then proceed to finer ones later on.<br><br><br></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 645.41px; height: 547.76px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(44).png" style="width: 645.41px; height: 547.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br><br>Another interesting usecase for this mentioned in the paper is adaptive retrieval. So basically you use only the first N floats to perform the similarity search while you use the M bits to perform re-ranking on the retrieved results where M &gt;&gt; N. &nbsp;This allows you to significantly make your queries faster while not sacrificing accuracy. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 671.15px; height: 312.96px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(45).png" style="width: 671.15px; height: 312.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.d4t3timi8crq"><span class="c5">Generalising Length of Transformers</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2402.09371&amp;sa=D&amp;source=editors&amp;ust=1715645745471950&amp;usg=AOvVaw1EX2Zb26i1CEh0j-vslH12">[2402.09371] Transformers Can Achieve Length Generalization But Not Robustly</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Just me (an outsider) trying to understand what might have went into gemini 1.5 pre training so that it can generalize to 10M context length even with limited training.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Tbh this paper is not the sauce as it is too simple plus tries to verify results only on a small problem</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 747.00px; height: 293.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(46).png" style="width: 747.00px; height: 293.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">3rd is just doing 321 + 654 instead of 123 + 456</span></p><p class="c9"><span class="c3">4th is just doing 3c2b1a + 6c5b4a instead of 321 + 654</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">2nd is instead of using encoding position 1 as vector of 1, 2 as vector of 2 and so on<br>What if you took a random set of positions from length L (which will be much much more than maximum context tokens we will feed to this model)</span></p><p class="c9"><span class="c3">Then you sort those random positions in ascending order and assign it to each actual position.</span></p><p class="c9"><span class="c3">E.g. sampled position are 4, 7 so 1st and 2nd tokens get assigned the embeddings of 4th and 7th<br><br>We can then training this network only for length N but then using the same technique during prediction for length M &gt;&gt; N.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">1st is FIRE which iiuc uses an MLP to learn embeddings for each position instead of using some fixed function like linear or sinusoidal</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 739.50px; height: 460.41px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(47).png" style="width: 739.50px; height: 460.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 736.50px; height: 537.97px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(48).png" style="width: 736.50px; height: 537.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.o0rdzd7qcvw7"><span class="c5">World Model </span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2402.08268&amp;sa=D&amp;source=editors&amp;ust=1715645745473777&amp;usg=AOvVaw3uoGd2ZxUzH_VZAKM9y0Wz">[2402.08268] World Model on Million-Length Video And Language With RingAttention</a></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.be2tfr7p2dj"><span class="c5">Diffusion Transformers</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2212.09748&amp;sa=D&amp;source=editors&amp;ust=1715645745474228&amp;usg=AOvVaw2mKgoIMmT5NaAiJ8bD7jRB">[2212.09748] Scalable Diffusion Models with Transformers</a></span></p><p class="c9"><span>Code - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/facebookresearch/DiT&amp;sa=D&amp;source=editors&amp;ust=1715645745474544&amp;usg=AOvVaw2SrHPIyNylOVyoOXcuCa4w">GitHub - facebookresearch/DiT: Official PyTorch Implementation of "Scalable Diffusion Models with Transformers"</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Since Sora has dropped and hints at using similar architecture (with a lot of magic), good idea to go through this</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Architecture wise there’s not a lot going on here. Other than standard diffusion models, there are two changes</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_ub4bbw2io895-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Sending a stream of tokens instead of &nbsp;latent to the model</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Using Transformer blocks instead of U-Net with Conv</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 778.50px; height: 421.92px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(49).png" style="width: 778.50px; height: 421.92px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For patching, simply use MLP to convert it into embeddings. Once you get the embeddings, you also apply positional encoding using sinusoidal frequency based version</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 517.00px; height: 364.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(50).png" style="width: 517.00px; height: 364.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 803.26px; height: 448.26px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(51).png" style="width: 803.26px; height: 448.26px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Then you proceed to predicting the tokens using the transform block. They use N transform blocks (where N varies according to model size i.e. S, B, L etc.) They also try different types of DiT blocks primarily to introduce text based conditioning. <br>Ultimately after ablation they used the one the Adaptive layer norm with zero init for gammas (Note for me, I should read more on this)<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 472.00px; height: 339.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(52).png" style="width: 472.00px; height: 339.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">What is the Adaptive layer norm? </span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 741.00px; height: 346.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(53).png" style="width: 741.00px; height: 346.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 742.29px; height: 384.22px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(54).png" style="width: 742.29px; height: 384.22px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 744.31px; height: 90.58px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(55).png" style="width: 744.31px; height: 90.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"><br></span></p><h1 class="c1" id="h.g6upvqdpnqeb"><span class="c5">Stable Diffusion 3</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.03206&amp;sa=D&amp;source=editors&amp;ust=1715645745476502&amp;usg=AOvVaw1LrDvrY8MRNao5fKjdLH55">[2403.03206] Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Interesting part is using a third text encoder T5 and then using separate weights for it instead of simply piping it in cross-attention</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 750.17px; height: 555.06px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(56).png" style="width: 750.17px; height: 555.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Synthetic datasets are now being part of Visual models as well. Might be time to create some simple repo to make this seamless.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 750.50px; height: 269.05px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(57).png" style="width: 750.50px; height: 269.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Increasing channels improves the performance of the models</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 757.00px; height: 322.69px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(58).png" style="width: 757.00px; height: 330.00px; margin-left: 0.00px; margin-top: -7.31px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.pyj6hj96jujj"><span class="c5">Deepseek-VL</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.05525&amp;sa=D&amp;source=editors&amp;ust=1715645745477948&amp;usg=AOvVaw0rc-zjdlbaxKulLKmMS_wN">[2403.05525] DeepSeek-VL: Towards Real-World Vision-Language Understanding</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Strangely Teorotaxes shilling deepseek on my TL almost every hour has worked and I have developed an affection for their smol models that perform great in benchmarks as well as are useful practically</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I am also reading this paper so I can steal moondream’s alpha without selling my house to hire vikhyatk</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For datasets, they have a pretty diverse mix that covers web UI screenshots, OCR and chart parsing. This is good as simple image text pairs is not good enough for practical utility of these models</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Interesting choice for the architecture to use two image encoders and not one</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">First they use SigLIP for semantic information. However siglip uses lower dimensional latents so the details get lost<br><br>For preserving details they use VitDet based on SAM-B that can accept higher dimensional latents. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The output of both of these encoders is concatenated before being passed via MLP</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 695.50px; height: 669.23px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(59).png" style="width: 695.50px; height: 669.23px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.dyk82i1ibtwq"><span class="c5">Synth2 </span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.07750&amp;sa=D&amp;source=editors&amp;ust=1715645745479440&amp;usg=AOvVaw1Uu1lKRxmx_NgchmZiYDBj">https://arxiv.org/abs/2403.07750</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Everyone is now aware about the usefulness of synthetic datasets for training LLMs. Even the latest and greatest Claude-3 has some synthetic datasets in its training set</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The next natural step for this is synthetic datasets for multimodal LMs. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">And the easiest modality to start with is images i.e VLMs </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">A simpler version of this would be simply generating captions for existing images using some other VLM </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">A harder version would be generating both image - text pairs since generating images is costly as well as might not be great in quality and adhere to prompt really well</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This is how they are doing it </span></p><p class="c0"><span class="c3"></span></p><ol class="c4 lst-kix_5eivyzuskjhk-0 start" start="1"><li class="c9 c14 li-bullet-0"><span>Generate Caption using some LLM (Gemini Pro here)<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.00px; height: 403.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(60).png" style="width: 538.00px; height: 403.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c9 c14 li-bullet-0"><span class="c3">Generate images from these captions</span></li></ol><p class="c9 c53"><span class="c3">Here they do pre training of their image gen so that to eliminate the effect of human-annotated VLM dataset for experiments. This step is not necessary for practical purposes imo. They are using Muse as the primary image gen here since it’s transformer based architecture.</span></p><ol class="c4 lst-kix_5eivyzuskjhk-0" start="3"><li class="c9 c14 li-bullet-0"><span class="c3">Now the next step is to simply use this data fr VLM training. One small optimization they do here is that they use the same image encoder in both image gen as well as VLM. This allows them to directly use the embedding tokens output from image gen and feed it into the VLMs projection layer without the expensive decoding and encoding to pixel space. </span></li></ol><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.00px; height: 526.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(61).png" style="width: 538.00px; height: 526.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.qjgq4dgi8jki"><span class="c5">Fashion Diffusion (Make your waifu dress in Zara)</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.01779&amp;sa=D&amp;source=editors&amp;ust=1715645745482574&amp;usg=AOvVaw03aLaczDNPs9RUUB-A7pn9">[2403.01779] OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The outfit fitting problem seems simple at first glance - you just have to inpaint right?<br></span></p><p class="c9"><span class="c3">But the issue becomes more apparent as you dive deeper </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">E.g. How do you make sure the clothes and brands are represented exactly the same? How do you account for weird body poses? </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">There are multiple approaches that exist currently to solve this problem. The authors propose a new one that’s far more accurate</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 680.73px; height: 779.36px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(62).png" style="width: 680.73px; height: 779.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So basically they train one more u-net in parallel to SD. This unet only gets the garment as well as some text label as input. We however, do not use it to generate anything. What we do is simply take the inputs to the spatial attention layer in the unet and then concatenate them with the inputs to the spatial attention layer of Stable diffusion along the width. This is what they call as output fusion</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 699.61px; height: 350.35px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(63).png" style="width: 699.61px; height: 350.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span>This would be confusing so you can check the following lines in the official code<br><br></span><span class="c58">Getting query from attention layers of outfitting unt</span><span>&nbsp;- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/levihsu/OOTDiffusion/blob/344112ad1c03c2af1cf7a1f07d689b18af4c175a/ootd/pipelines_ootd/attention_garm.py%23L234&amp;sa=D&amp;source=editors&amp;ust=1715645745484913&amp;usg=AOvVaw1c3gOx5FwlPJqa1fsQSm_n">https://github.com/levihsu/OOTDiffusion/blob/344112ad1c03c2af1cf7a1f07d689b18af4c175a/ootd/pipelines_ootd/attention_garm.py#L234</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Concatenating query with denoising unet - </span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/levihsu/OOTDiffusion/blob/344112ad1c03c2af1cf7a1f07d689b18af4c175a/ootd/pipelines_ootd/attention_vton.py%23L236&amp;sa=D&amp;source=editors&amp;ust=1715645745485699&amp;usg=AOvVaw3-9NtOrcsr87oJSGrxBCPA">https://github.com/levihsu/OOTDiffusion/blob/344112ad1c03c2af1cf7a1f07d689b18af4c175a/ootd/pipelines_ootd/attention_vton.py#L236</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Parent that calls the former and then passed it to the latter &nbsp;- <br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/levihsu/OOTDiffusion/blob/344112ad1c03c2af1cf7a1f07d689b18af4c175a/ootd/pipelines_ootd/pipeline_ootd.py%23L373&amp;sa=D&amp;source=editors&amp;ust=1715645745486437&amp;usg=AOvVaw1FP8D3XI7pG_FAeElp-ebp">https://github.com/levihsu/OOTDiffusion/blob/344112ad1c03c2af1cf7a1f07d689b18af4c175a/ootd/pipelines_ootd/pipeline_ootd.py#L373</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.q8nfqnfo0hvx"><span class="c5">Another Apple LLM (this time it’s multimodal)</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.09611&amp;sa=D&amp;source=editors&amp;ust=1715645745487489&amp;usg=AOvVaw0mdnM-2m3UP7SzIAcfFG19">[2403.09611] MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">I have no hope from apple’s ML teams to make a sota model. Reading this paper only cause they do ablations </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.50px; height: 150.36px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(64).png" style="width: 674.50px; height: 150.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.38px; height: 405.98px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(65).png" style="width: 674.38px; height: 405.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">For data they found that keeping almost an equal mixture of interleaved records along with image-text pairs gives best capability. You also need some text only data to preserve language capability of llms.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.15px; height: 317.16px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(66).png" style="width: 674.15px; height: 317.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 685.88px; height: 76.52px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(67).png" style="width: 685.88px; height: 76.52px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Second approach is interesting to support higher resolution while not sacrificing speed<br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 689.43px; height: 425.70px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(68).png" style="width: 689.43px; height: 425.70px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.8lfrd26bkebm"><span class="c5">Quiet-Star (Is it really the fabled openai algo, nope)</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.09629&amp;sa=D&amp;source=editors&amp;ust=1715645745489603&amp;usg=AOvVaw0LtgFYZC_ruYLhOoueaB0R">[2403.09629] Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Afraid to say but they use too many buzzwords for me to understand EXACTLY what they are doing and they don’t even have code as well. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">That being said, this is what I gather</span></p><p class="c0"><span class="c3"></span></p><ul class="c4 lst-kix_scc7wd6335r8-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Instead of simply generating tokens auto-regressively they also use the probs of the other tokens that COULD have been generated</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Then, They also have an MLP after lm_head layer to use the probs of these ‘THOUGHT’ &nbsp;tokens to influence the probs of the future tokens</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Then they use REINFORCE algo (whatever that is) to train the model such that only those thoughts are produced that actually increase the probability of the future tokens (and also train the MLP to do appropriate mixin)</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span>Teacher forcing - Nothing but using data from training set as input for next token as opposed to output from previous steps </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&amp;sa=D&amp;source=editors&amp;ust=1715645745490767&amp;usg=AOvVaw1JZsZCq_FqLV7GGnCX-ADi">https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Non-myopic loss - Calculate loss based on all future tokens instead of just next token</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Star - simply ask model to generate a rational (using prompt) and then an answer instead of directly generating an answer </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2203.14465&amp;sa=D&amp;source=editors&amp;ust=1715645745491477&amp;usg=AOvVaw1q5Gadg1Ers5uEum-aVg-J">https://arxiv.org/abs/2203.14465</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Ok, no worries, simply had to upload paper PDF in opus to crack it </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 724.77px; height: 913.60px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(69).png" style="width: 724.77px; height: 913.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 741.40px; height: 1042.57px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(70).png" style="width: 741.40px; height: 1042.57px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 759.50px; height: 1030.08px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(71).png" style="width: 759.50px; height: 1030.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.j332r3uucd5x"><span class="c5">Transformers for time series (truly retarded)</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.07815&amp;sa=D&amp;source=editors&amp;ust=1715645745492969&amp;usg=AOvVaw0LCYThKNSRjNxtL5sLFmH8">https://arxiv.org/abs/2403.07815</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Why would amazon do this when you can use ARIMA or other lightweight shit? <br><br>No idea, but it’s fun</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Tbf, they are not doing something insanely genius. <br><br>What they are doing is following</span></p><ul class="c4 lst-kix_tuirwe3v41gd-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Take real number time series</span></li></ul><ul class="c4 lst-kix_wbdf5s4tmdvx-0 start"><li class="c9 c14 li-bullet-0"><span class="c3">Normalise it using mean</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Quantise it into uniform number of bins B</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Assign a token to each bin</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Train a transformer (T5 and GPT2 here) to predict the bins using cross entropy loss</span></li><li class="c9 c14 li-bullet-0"><span class="c3">Dequantise the bin to get correct prediction</span></li></ul><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 670.40px; height: 400.46px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(72).png" style="width: 670.40px; height: 400.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They also show improvements with using synthetic data by augmenting time series with noise instead of just training data</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 670.50px; height: 171.23px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(73).png" style="width: 670.50px; height: 171.23px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Synthetic data gen for time series</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 655.50px; height: 530.99px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(74).png" style="width: 655.50px; height: 530.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.qzan5v8imnfn"><span class="c5">GaLore</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.03507&amp;sa=D&amp;source=editors&amp;ust=1715645745495526&amp;usg=AOvVaw1MiIUR6MRSIFlsvJCR2a0s">[2403.03507] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So their idea is the following, you already have LORA that uses low-rank matrices A&amp;B to update the weights. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 647.82px; height: 374.35px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(75).png" style="width: 647.82px; height: 374.35px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br><br>Here they propose that instead of using separate A&amp;B matrices, you can simply keep on reducing the rank of gradients and still get the same level of accuracy while using even lower memory. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They prove this using mathematical voodoo that indeed the gradient matrices tend to have lower rank as training progresses and so you don’t need to store the whole and secondly, if you use low-rank matrices, the loss still converges to a minimum value. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Another interesting thing is that they keep on recomputing the low-rank projections but not at every timestep. Only when T steps have passed, the projections are updated. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.90px; height: 556.40px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(76).png" style="width: 604.90px; height: 556.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The code is actually quite simple. Generated it using claude (so that no one has to bother with weird maths symbol in the paper)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c11">class GaLoreAdam:</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, rank=1, scale_factor=0.25, freq=200):</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.params = list(params)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.lr = lr</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.betas = betas</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.eps = eps</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.rank = rank</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.scale_factor = scale_factor</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.freq = freq</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.step_count = 0</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.m = [torch.zeros_like(p, memory_format=torch.preserve_format) for p in self.params]</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.v = [torch.zeros_like(p, memory_format=torch.preserve_format) for p in self.params]</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.P = [None] * len(self.params)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; def step(self):</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; with torch.no_grad():</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for i, p in enumerate(self.params):</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; grad = p.grad</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if grad is None:</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; continue</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if self.step_count % self.freq == 0:</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; U, S, V = torch.svd(grad)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.P[i] = U[:, :self.rank]</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; m, v = self.m[i], self.v[i]</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; P_t = self.P[i]</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; R_t = P_t.T @ grad</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; m.mul_(self.betas[0]).add_(R_t, alpha=1 - self.betas[0])</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; v.mul_(self.betas[1]).addcmul_(R_t, R_t, value=1 - self.betas[1])</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; m_hat = m / (1 - self.betas[0] ** (self.step_count + 1))</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; v_hat = v / (1 - self.betas[1] ** (self.step_count + 1))</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; N_t = m_hat / (torch.sqrt(v_hat) + self.eps)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; G_t = self.scale_factor * P_t @ N_t</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; p.add_(G_t, alpha=-self.lr)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; </span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.step_count += 1</span></p><h1 class="c1" id="h.8qvoptlt4jsx"><span class="c5"><br>ORPO</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.07691&amp;sa=D&amp;source=editors&amp;ust=1715645745502909&amp;usg=AOvVaw1OWW1j4B-hQzQqIsehpebX">[2403.07691] ORPO: Monolithic Preference Optimization without Reference Model</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So what they are proposing here is a way to RLHF the model without using any reward model as well as the need for a separate preference phase. They tell us you can simply do it along with SFT </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 715.82px; height: 305.13px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(77).png" style="width: 715.82px; height: 305.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">The algorithm proposed is really simple. They use the log prob of the chosen and rejected samples from the model and then use it to calculate odd ratio<br><br>Once you have odd ratio, you can formulate a second loss term based on that, scale it down by a factor lambda and then add it to SFT loss<br><br>Now you simply train the model to using gradient from this combined loss</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 666.10px; height: 714.33px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(78).png" style="width: 666.10px; height: 714.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They show that the model is actually larning to adhere to preference and keeps on lowering reject samples log prob.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 674.25px; height: 397.30px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(79).png" style="width: 674.25px; height: 397.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They also show that the trained model produces responses that are quite same (i.e. closer to reward) given the same prompt (even tho temperature is kept as 1.0)</span></p><p class="c9"><span class="c3"><br><br>They also show that trained models do change their responses significantly if you change the prompt meaning they are not over-optimised towards a single reward and do adhere to instructions</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.00px; height: 941.32px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(80).png" style="width: 601.00px; height: 941.32px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Implementation from claude:<br><br></span><span class="c11">import torch</span></p><p class="c9"><span class="c11">import torch.nn as nn</span></p><p class="c9"><span class="c11">from torch.nn import functional as F</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">class ORPOLoss(nn.Module):</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; def __init__(self, lambda_or=1.0):</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; super(ORPOLoss, self).__init__()</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; self.lambda_or = lambda_or</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; def forward(self, logits_chosen, logits_rejected):</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; # Compute SFT loss (negative log-likelihood)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; sft_loss = F.cross_entropy(logits_chosen.view(-1, logits_chosen.size(-1)), labels_chosen.view(-1))</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; # Compute L_OR loss</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; probs_chosen = F.softmax(logits_chosen, dim=-1)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; probs_rejected = F.softmax(logits_rejected, dim=-1)</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; odds_chosen = probs_chosen / (1 - probs_chosen)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; odds_rejected = probs_rejected / (1 - probs_rejected)</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; log_odds_ratio = torch.log(odds_chosen / odds_rejected)</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; l_or = -torch.log(torch.sigmoid(log_odds_ratio))</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; # Combine SFT and L_OR losses</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; total_loss = sft_loss + self.lambda_or * l_or.mean()</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; return total_loss</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11"># Training loop</span></p><p class="c9"><span class="c11">model = ... &nbsp;# Initialize your model</span></p><p class="c9"><span class="c11">optimizer = ... &nbsp;# Initialize your optimizer</span></p><p class="c9"><span class="c11">orpo_loss = ORPOLoss(lambda_or=0.2) &nbsp;# Initialize ORPO loss with lambda_or value</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">for epoch in range(num_epochs):</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; for batch in dataloader:</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; input_ids, attention_mask, labels_chosen, labels_rejected = batch</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; # Forward pass</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; logits_chosen = model(input_ids, attention_mask=attention_mask).logits</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; logits_rejected = model(input_ids, attention_mask=attention_mask).logits</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; # Compute ORPO loss</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; loss = orpo_loss(logits_chosen, logits_rejected)</span></p><p class="c0"><span class="c11"></span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; # Backward pass and optimization</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; loss.backward()</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; optimizer.step()</span></p><p class="c9"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; optimizer.zero_grad()</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.2axbf5pkc531"><span class="c5">MyVLM (Shitty Name only Snapchat can think of)</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.14599&amp;sa=D&amp;source=editors&amp;ust=1715645745512703&amp;usg=AOvVaw31BgdcULicNekd_7wOBAQg">[2403.14599] MyVLM: Personalizing VLMs for User-Specific Queries</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Aim is to add personalisation to any existing visual language model or train one from scratch. Mostly they want to do this to capture snap userbase’s image and other objects accurately.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Most of such ideas revolve around playing with embeddings which is the case here as well. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They run a face detection model, figure out whose face it is &nbsp;using cosine similarity from existing database and then pass those embeddings and the metadata to the VLM</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They also have object detection models but here they use classifiers instead of embeddings. The classifier they train are pretty fine grained e.g. recognise which type of dog it is then just say it’s a dog</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 578.37px; height: 429.68px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(81).png" style="width: 578.37px; height: 429.68px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 577.93px; height: 473.51px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(82).png" style="width: 577.93px; height: 473.51px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">This paper contains a lot of hyperparameters in the appendix so better to go through them before jumping to implementation.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The QFormer here is only used in case of BLIP-2, when used with llava, they simply remove it and append the embeddings to the ones after the projection MLP</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 701.58px; height: 337.43px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(83).png" style="width: 701.58px; height: 337.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">One thing I liked about this paper is that they have actually done a thorough analysis if this approach is even sound mathematically or not. Plenty of nuggets in the paper where they scaled the embeddings up/down to make sure their effects are not over/under emphasized</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 657.88px; height: 449.84px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(84).png" style="width: 657.88px; height: 449.84px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 659.32px; height: 529.81px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(85).png" style="width: 659.32px; height: 529.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3">&nbsp; </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 658.92px; height: 541.77px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(86).png" style="width: 658.92px; height: 541.77px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.d70okn1gvhmg"><span class="c5">Factuality in LLMs</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2403.18802&amp;sa=D&amp;source=editors&amp;ust=1715645745517165&amp;usg=AOvVaw3m0K5ZfNc7SJKJNA-AqQTQ">[2403.18802] Long-form factuality in large language models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Nothing groundbreaking here imo<br><br>They use LLMs to split out facts from a given generated passage and then use google search to verify if those facts are correct or not</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">It is simply working now cause the models are simply bigger and much more smarter.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">All the alpha is in Appendix as well as one section where they discuss precision recall of LLMs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 715.84px; height: 497.09px;"><img alt="" src="./Keeping up with AGI_files/GJxP-2eWcAA15pB.png" style="width: 715.84px; height: 497.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 719.96px; height: 567.12px;"><img alt="" src="./Keeping up with AGI_files/GJxP3AmXQAAqHin.png" style="width: 719.96px; height: 567.12px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.50px; height: 622.64px;"><img alt="" src="./Keeping up with AGI_files/GJxPwVjWEAAN4Eb.png" style="width: 720.50px; height: 622.64px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.8nh1t46pwwcm"><span class="c5">Layer Skip </span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2404.16710&amp;sa=D&amp;source=editors&amp;ust=1715645745518860&amp;usg=AOvVaw1olIx_-OfupGBQ0GDfmS7i">[2404.16710] Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding</a></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 756.77px; height: 351.41px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(87).png" style="width: 756.77px; height: 351.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c3">Quite an important paper imo (cause of my bias when it comes to matryoshka embeddings)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The premise is that not all tokens require you to go through all layers of the models (this has been explored in other papers as well). Some tokens are much easier to predict than other</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">So what you can do instead is that directly take embeddings from an intermediate model layer and pass it through lm_head to get predictions. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">But what if we predict something wrong? Well for that, instead of using normal auto-regressive decoding you can use speculative decoding. You simply keep on predicting using earlier layers and then keep on correcting it using the output from all the layers. The correction is fast since there’s no need to do that auto-regressively. <br><br>Also if we use the same model to correct, we should already have KV cache from initial few layers, so effectively we only need to compute the final layer. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">The only issue with this idea is that it require training the model in a way so that earlier embeddings are still good enough (basically by incorporating their loss scaled down by a factor in the final loss) plus the training itself is more costly</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 757.50px; height: 186.05px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(88).png" style="width: 757.50px; height: 186.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 764.50px; height: 456.53px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(89).png" style="width: 764.50px; height: 456.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.2tivp680y45b"><span class="c5">Training a Judge (model)</span></h1><p class="c9"><span>Paper - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2405.01535&amp;sa=D&amp;source=editors&amp;ust=1715645745520930&amp;usg=AOvVaw1PovX37fOyYJWndMLAljdg">Paper page - Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">They are training two models here and then merging them<br><br>The first model is pretty obvious, given an answer by an LLM, rate how good it is from 1 - 5. However to improve the response they do 2-3 prompt engineering tricks which can be used without this model as well. Instead of just providing initial instruction and answer to generate a score, they also provide a reference answer in the prompt along with explicit evaluation criteria rubrik. Also, instead of only generating the score, they also ask the model to first generate a verbal explanation first.</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>The second model they train is a pairwise ranking one where along with instruction you give two answers and ask LLM to tell which is better. Again they do all the prompt engineering tricks mentioned before but with small variations e.g. evaluation criteria doesn’t contain scores, generate two verbal feedback that compare them with criteria<br></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 594.71px; height: 779.20px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(90).png" style="width: 594.71px; height: 779.20px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>I will never intuitively understand why model merging works but I have made my peace with it. <br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 590.50px; height: 489.40px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(91).png" style="width: 590.50px; height: 489.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Merge cheatsheet hidden in the paper, woohooo, till now I only pretended to know what they were. All seem simple<br><br></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 641.53px; height: 1082.16px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(92).png" style="width: 641.53px; height: 1082.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h1 class="c1" id="h.s8xy0lg5btwp"><span class="c5">How to create a FAQ dataset from your company docs?</span></h1><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 656.50px; height: 276.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(93).png" style="width: 656.50px; height: 276.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 664.50px; height: 167.42px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(94).png" style="width: 664.50px; height: 167.42px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c1" id="h.sul6xbl9y84l"><span class="c5">APPENDIX</span></h1><h2 class="c13" id="h.f8teq1wui52j"><span class="c22 c27">Tips and Tricks</span></h2><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 668.50px; height: 47.00px;"><img alt="" src="./Keeping up with AGI_files/6i6DCX52.gif" style="width: 668.50px; height: 47.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><a id="t.f40eb6dd8059a38bc880d8d9bf827ce671162adb"></a><a id="t.1"></a><table class="c21"><tbody><tr class="c54"><td class="c62" colspan="1" rowspan="1"><p class="c9"><span class="c29">Don’t know anything about LLMs?<br></span><span>Start here &nbsp;- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2401.02038&amp;sa=D&amp;source=editors&amp;ust=1715645745525541&amp;usg=AOvVaw0qQPJn1zgzEQpgnC-J5qAk">Paper page - Understanding LLMs: A Comprehensive Overview from Training to Inference</a></span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span class="c12">Not getting good results for your vector DB based search, Use these</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c52">HyDE </span><span>- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://python.langchain.com/en/latest/modules/chains/index_examples/hyde.html&amp;sa=D&amp;source=editors&amp;ust=1715645745526763&amp;usg=AOvVaw1pUlHov32-84BP6NkqKu4y">Hypothetical Document Embeddings — 🦜🔗 LangChain 0.0.155</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Maybe this as well</span></p><p class="c9"><span>Cohere launched re-rank - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://txt.cohere.com/rerank/&amp;sa=D&amp;source=editors&amp;ust=1715645745527579&amp;usg=AOvVaw00HyfZlsSfKIwJ0fctCx9E">Say Goodbye to Irrelevant Search Results: Cohere Rerank Is Here</a></span><span class="c3">&nbsp;</span></p><p class="c9"><span class="c3">This should be better than using HyDE.</span></p><p class="c9"><span>Also - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2302.00093&amp;sa=D&amp;source=editors&amp;ust=1715645745528159&amp;usg=AOvVaw383GuBSpvybIZaqSpI_ea5">[2302.00093] Large Language Models Can Be Easily Distracted by Irrelevant Context</a></span><span class="c3">&nbsp;(why feeding correct docs in LLM is important)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Do you want to build a scalable vector DB?</span></p><p class="c9"><span>Use this - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/google-research/google-research/tree/master/scann&amp;sa=D&amp;source=editors&amp;ust=1715645745529012&amp;usg=AOvVaw0qqItKYihDiJg9msx6Ivk7">ScaNN</a></span></p><p class="c9"><span class="c3">OR</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/nmslib/hnswlib&amp;sa=D&amp;source=editors&amp;ust=1715645745529608&amp;usg=AOvVaw33_qTWCpjTsrxJanDzR6rM">GitHub - nmslib/hnswlib: Header-only C++/python library for fast approximate nearest neighbors</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Do you want local embeddings which rival openAI ones?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/hkunlp/instructor-xl&amp;sa=D&amp;source=editors&amp;ust=1715645745530457&amp;usg=AOvVaw0Fx82V2g_U4lp3DvAPQSRz">https://huggingface.co/hkunlp/instructor-xl</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">USE THIS LIB IF YOU WANT PERFECT JSON OUTPUT FROM LLM</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/1rgs/jsonformer/&amp;sa=D&amp;source=editors&amp;ust=1715645745531223&amp;usg=AOvVaw0eNy0bDAUvDgYSPr-nGDV4">https://github.com/1rgs/jsonformer/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Are you Lazy? Do you still want to finetune LLaMa?</span></p><p class="c9"><span>Worry not, use this to generate dataset -</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/yizhongw/self-instruct&amp;sa=D&amp;source=editors&amp;ust=1715645745532526&amp;usg=AOvVaw1Wxmull2gsUuRuKnC0EkTP">&nbsp;https://github.com/yizhongw/self-instruct</a></span></p><p class="c9"><span>OR this </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/togethercomputer/RedPajama-Data&amp;sa=D&amp;source=editors&amp;ust=1715645745532990&amp;usg=AOvVaw0ci4eHD0tbMQc_IZZD-GWS">https://github.com/togethercomputer/RedPajama-Data</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Check this model out for good conversational capabilities (AI Waifu ready) </span><span><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/PygmalionAI/pygmalion-7b&amp;sa=D&amp;source=editors&amp;ust=1715645745533690&amp;usg=AOvVaw0kMBw7nZnCX8JXQPxmz24k">https://huggingface.co/PygmalionAI/pygmalion-7b</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Limited Memory? Still want to run Machine god? No worries, all state secrets here </span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/oobabooga/text-generation-webui/blob/main/docs/Spell-book.md&amp;sa=D&amp;source=editors&amp;ust=1715645745534420&amp;usg=AOvVaw39glpQiaBBVzNRZiAefcmv">https://github.com/oobabooga/text-generation-webui/blob/main/docs/Spell-book.md</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Smol model List with trusted reviews (imageboard replies)<br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://rentry.org/lmg_models&amp;sa=D&amp;source=editors&amp;ust=1715645745535287&amp;usg=AOvVaw1NnNJFzQtJtHFFUlmTI_Uk">https://rentry.org/lmg_models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Finetuning LLMs (Most asked on twitter recently)</span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://rentry.org/llm-training&amp;sa=D&amp;source=editors&amp;ust=1715645745536766&amp;usg=AOvVaw2i23r22i0MGWbI5dv0yqEx">The Novice's LLM Training Guide</a></span></p><p class="c0"><span class="c12"></span></p><p class="c9"><span>​​</span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/13gbbv8/d_have_you_tried_finetuning_an_open_source_llm/&amp;sa=D&amp;source=editors&amp;ust=1715645745537487&amp;usg=AOvVaw2CTfM2_OnJQZ1Fjf3tFkpn">[D] Have you tried fine-tuning an open source LLM? : r/MachineLearning</a></span><span class="c3">&nbsp; (endorsed by Hardmaru)</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Github issues are a great source for learning comp sci</span><span><br><br>Learn NuMA - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ggerganov/llama.cpp/issues/1437&amp;sa=D&amp;source=editors&amp;ust=1715645745538321&amp;usg=AOvVaw3fYOGsWge5KSjmuATy0xUO">https://github.com/ggerganov/llama.cpp/issues/1437</a></span><span><br><br>Learn Mmap - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ggerganov/llama.cpp/issues/91&amp;sa=D&amp;source=editors&amp;ust=1715645745538575&amp;usg=AOvVaw0w6qRa9OmuibHSr6BPnQuC">https://github.com/ggerganov/llama.cpp/issues/91</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Learn Loop Unrolling - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/ggerganov/llama.cpp/pull/1530&amp;sa=D&amp;source=editors&amp;ust=1715645745539385&amp;usg=AOvVaw16ALbYsy_qQYYAhpFwZOP5">https://github.com/ggerganov/llama.cpp/pull/1530</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Adding code samples to your training data is good way to increase LLMs slight reasoning capabilities</span><span><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2210.07128.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745540556&amp;usg=AOvVaw17FMZtDG9eGCJVskOwSiMP">https://arxiv.org/pdf/2210.07128.pdf</a></span><span><br><br>Confirmed by Teknium as well - </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/Teknium1/status/1658123329763409921?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745540872&amp;usg=AOvVaw29RPQ_YBHImdCvu-gkbW4Z">https://twitter.com/Teknium1/status/1658123329763409921?s=20</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">(yes I trust anons on twitter more than a paper)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Incase you want to run AGI in your iphone</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/mlc-ai/mlc-llm/blob/main/ios/README.md&amp;sa=D&amp;source=editors&amp;ust=1715645745542064&amp;usg=AOvVaw1WveRpxkZ9lRaD2XN2fEpe">https://github.com/mlc-ai/mlc-llm/blob/main/ios/README.md</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">In case you want to generate instruction data from LLM itself<br></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/tatsu-lab/stanford_alpaca/blob/main/prompt.txt&amp;sa=D&amp;source=editors&amp;ust=1715645745542904&amp;usg=AOvVaw1jFn6apPclOPgcplU0JYW6">https://github.com/tatsu-lab/stanford_alpaca/blob/main/prompt.txt</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Estimate memory required for training </span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://blog.eleuther.ai/transformer-math/&amp;sa=D&amp;source=editors&amp;ust=1715645745543729&amp;usg=AOvVaw3wkN8omHup3o4x8j_mCHVD">https://blog.eleuther.ai/transformer-math/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">SD QR codes</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://learn.thinkdiffusion.com/creating-qr-codes-with-controlnet/&amp;sa=D&amp;source=editors&amp;ust=1715645745544437&amp;usg=AOvVaw1D3clpgReS9VZyxJfqYWEv">https://learn.thinkdiffusion.com/creating-qr-codes-with-controlnet/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">GPU inference</span><span><br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://medium.com/@ngoodger_7766/fast-gpu-based-pytorch-model-serving-in-100-lines-of-python-9ad3ebd0a1d9&amp;sa=D&amp;source=editors&amp;ust=1715645745545160&amp;usg=AOvVaw2n0ZSyRctecVJKebXHkJQr">Fast GPU based PyTorch model serving in 100 lines of Python | by Nikolaj Goodger | Medium</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/notes/cuda.html%23asynchronous-execution&amp;sa=D&amp;source=editors&amp;ust=1715645745545655&amp;usg=AOvVaw1LjGhxbOGMTq3zt2DgRooa">https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/replicate/cog&amp;sa=D&amp;source=editors&amp;ust=1715645745546125&amp;usg=AOvVaw1_gn_oaV_bYdN-HFTEdPly">GitHub - replicate/cog: Containers for machine learning</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>Want to understand how transformer inference works?<br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://kipp.ly/blog/transformer-inference-arithmetic/&amp;sa=D&amp;source=editors&amp;ust=1715645745546740&amp;usg=AOvVaw3TEEjMxOCGfflTSaTG5bMg">Transformer Inference Arithmetic | kipply's blog</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Read this and get close to cloud’s alpha:</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://en.algorithmica.org/hpc/&amp;sa=D&amp;source=editors&amp;ust=1715645745547462&amp;usg=AOvVaw3il9g62F9boZJWcxw4_RvJ">https://en.algorithmica.org/hpc/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>You don’t know who cloud is? He’s an eastern euro dev who might solve OpenAI’s inference cost issues </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://twitter.com/cloud11665&amp;sa=D&amp;source=editors&amp;ust=1715645745548092&amp;usg=AOvVaw1mDKEVpchJymRO6v0_5h_F">https://twitter.com/cloud11665</a></span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Relation-extraction near Sota</span><span><br><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/Babelscape/rebel-large&amp;sa=D&amp;source=editors&amp;ust=1715645745548743&amp;usg=AOvVaw2wrzhtLWeDoeeT-GdV-QG4">Babelscape/rebel-large · Hugging Face</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">OSS LLM pricing estimation</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://together.ai/pricing&amp;sa=D&amp;source=editors&amp;ust=1715645745549642&amp;usg=AOvVaw3rX1lbNqRY82bXTuqmsS-l">https://together.ai/pricing</a></span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Fast Inference for LLMs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c52">TensortRT LLM</span><span>&nbsp;- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://x.com/abacaj/status/1719486649610940615?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745550435&amp;usg=AOvVaw27XJ6w9F7fZtr6B_HWXsrm">https://x.com/abacaj/status/1719486649610940615?s=20</a></span><span>&nbsp;(complicated setup)<br><br></span><span class="c52">vLLM + AWQ</span><span>&nbsp;- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://x.com/HamelHusain/status/1719882599261413840?s%3D20&amp;sa=D&amp;source=editors&amp;ust=1715645745550704&amp;usg=AOvVaw2byw2keVYJxGYo4tEzoj5U">https://x.com/HamelHusain/status/1719882599261413840?s=20</a></span><span class="c3">&nbsp;(easy setup, python native)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c52">Deepspeed-fastgen</span><span>&nbsp;- </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen&amp;sa=D&amp;source=editors&amp;ust=1715645745551233&amp;usg=AOvVaw1R7r_wybaJ9l1mcHwTxQ6G">https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Python Profiling</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/plasma-umass/scalene&amp;sa=D&amp;source=editors&amp;ust=1715645745552005&amp;usg=AOvVaw3bNWv-cGNq-BXK94ggp2Xy">https://github.com/plasma-umass/scalene</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Kipply’s blog (for when you just need to find something to read)</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://kipp.ly/sept-oct-2023/&amp;sa=D&amp;source=editors&amp;ust=1715645745552575&amp;usg=AOvVaw20bII9Hg9pGhOknoGdGom1">https://kipp.ly/sept-oct-2023/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">For when I actually decide to learn some maths behind ML</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://databookuw.com/databookV2.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745553289&amp;usg=AOvVaw3M-DZLzhVf6Z4Y3Q54jNdf">https://databookuw.com/databookV2.pdf</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Training Recipe (Overfit then Regularize)</span><span><br></span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://karpathy.github.io/2019/04/25/recipe/&amp;sa=D&amp;source=editors&amp;ust=1715645745553960&amp;usg=AOvVaw2VOGzK6LHxaflLOENFZ9Ag">https://karpathy.github.io/2019/04/25/recipe/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">A good summary of all the techniques for faster inference</span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://vgel.me/posts/faster-inference/&amp;sa=D&amp;source=editors&amp;ust=1715645745554614&amp;usg=AOvVaw0eVq9xIKFAvjXFhYeONpZY">https://vgel.me/posts/faster-inference/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">Dataset curation </span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/lilacai/lilac&amp;sa=D&amp;source=editors&amp;ust=1715645745555348&amp;usg=AOvVaw1p0Z0XJsX2xzFD5jPX6h51">https://github.com/lilacai/lilac</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">CUDA tutorial<br></span></p><p class="c9"><span class="c23"><a class="c10" href="https://www.google.com/url?q=https://youtu.be/nOxKexn3iBo?si%3DB0Eo-r7VDxeXyQr2&amp;sa=D&amp;source=editors&amp;ust=1715645745556181&amp;usg=AOvVaw2Com6xnBCBSOHMAmo6Jwrq">Getting Started With CUDA for Python Programmers</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">I should implement this </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2402.09171&amp;sa=D&amp;source=editors&amp;ust=1715645745557019&amp;usg=AOvVaw2Xn-dTTaGvjMgcguE37Y_q">[2402.09171] Automated Unit Test Improvement using Large Language Models at Meta</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p></td></tr></tbody></table><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c3">Pro-tip:</span></p><p class="c9"><span class="c3">If you are not a shape-rotator, just add shape dimensions in the code comments. Rotation skills should not stop you from taming the machine god. Example - </span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 677.50px; height: 309.58px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-07-08 at 6.44.07 PM.png" style="width: 677.50px; height: 309.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c13" id="h.hbwudrcbj6u6"><span class="c22 c27">Karpathy’s Presentation in MS Build 2023 </span></h2><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 680.00px; height: 378.00px;"><img alt="" src="./Keeping up with AGI_files/Fw3mCgKaIAEO7Cn.jfif" style="width: 680.00px; height: 378.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 680.00px; height: 368.00px;"><img alt="" src="./Keeping up with AGI_files/Fw3mXQFagAAyYwc.jfif" style="width: 680.00px; height: 368.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 682.75px; height: 388.66px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(95).png" style="width: 723.82px; height: 405.46px; margin-left: -19.80px; margin-top: 5.79px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c12">Hidden alpha - </span></p><p class="c9"><span class="c3">You need to use large batches for a good model in case of contrastive learning. Reason being it needs to see a lot of diverse samples to form a good intuition.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.sp8pgb4nordc"><span class="c22 c27">GEMINI PRO VS GPT-4V comparison<br></span></h2><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2312.12436.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745559419&amp;usg=AOvVaw2GWqBUghFUdFN1wWClKQZ8">https://arxiv.org/pdf/2312.12436.pdf</a></span></p><p class="c9"><span class="c3"><br>Obviously GPT-4V is better but not by a lot (except when it comes to coding). I would simply quickly go over this to figure out some new ideas for side projects. Nothing major tbh. </span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 612.01px; height: 637.33px;"><img alt="" src="./Keeping up with AGI_files/Screenshot 2023-12-20 at 10.21.38 AM.png" style="width: 612.01px; height: 637.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.m4duwpe1fehc"><span>How to protect PII in data <br></span><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 2048.00px; height: 647.00px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(96).png" style="width: 2048.00px; height: 647.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><h2 class="c13" id="h.jmdmf79vdtmj"><span class="c22 c27">Answers to Stupid questions (mostly for me)</span></h2><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://datascience.stackexchange.com/questions/64278/what-is-a-channel-in-a-cnn&amp;sa=D&amp;source=editors&amp;ust=1715645745560606&amp;usg=AOvVaw1R2FEM8Tc5VB3tzEOYJsJV">machine learning - What is a channel in a CNN? - Data Science Stack Exchange</a></span></p><p class="c0"><span class="c3"></span></p><h2 class="c13" id="h.7wyq1bmblo6l"><span class="c22 c27">Reading List</span></h2><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2304.13712&amp;sa=D&amp;source=editors&amp;ust=1715645745561253&amp;usg=AOvVaw0oniWu-STFk-XVsk7eQNFb">[2304.13712] Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</a></span><span class="c3">&nbsp;( survey of Language models, extremely good)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.00050&amp;sa=D&amp;source=editors&amp;ust=1715645745561672&amp;usg=AOvVaw3KWOUeBx6K35EvdEhOQ9p3">[2305.00050] Causal Reasoning and Large Language Models: Opening a New Frontier for Causality</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.00833&amp;sa=D&amp;source=editors&amp;ust=1715645745562081&amp;usg=AOvVaw16VH1nwUEdjjxy07do7PGq">[2305.00833] Learning to Reason and Memorize with Self-Notes</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.01210&amp;sa=D&amp;source=editors&amp;ust=1715645745562504&amp;usg=AOvVaw2gpuuF9sJsbIee8YaY15pF">[2305.01210] Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2304.13835&amp;sa=D&amp;source=editors&amp;ust=1715645745562937&amp;usg=AOvVaw306bwgzsIbvqilJgECy_Vd">[2304.13835] Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2303.14177&amp;sa=D&amp;source=editors&amp;ust=1715645745563315&amp;usg=AOvVaw0DqEnZ_wAZvbBg0SsX9Upo">[2303.14177] Scaling Expert Language Models with Unsupervised Domain Discovery</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.01625/&amp;sa=D&amp;source=editors&amp;ust=1715645745563625&amp;usg=AOvVaw1FbFJkJva2shulAsQQ5Tkg">https://arxiv.org/abs/2305.01625/</a></span><span class="c3">&nbsp;[Unlimiformer - kNN inside transformers]</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2212.14034.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745563932&amp;usg=AOvVaw3AgiwXSd6ldaIEM4S2f5ZW">https://arxiv.org/pdf/2212.14034.pdf</a></span><span class="c3">&nbsp;[How far can you train a model on a single consume GPU]</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2304.01982&amp;sa=D&amp;source=editors&amp;ust=1715645745564242&amp;usg=AOvVaw1DTozQFqQLPa778Q6Y7H8g">[2304.01982] Rethinking the Role of Token Retrieval in Multi-Vector Retrieval</a></span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.10425&amp;sa=D&amp;source=editors&amp;ust=1715645745564693&amp;usg=AOvVaw38B1alniZjtLBexAE3tgxA">[2305.10425] SLiC-HF: Sequence Likelihood Calibration with Human Feedback</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.16291&amp;sa=D&amp;source=editors&amp;ust=1715645745565146&amp;usg=AOvVaw1L2bw9d6Oy8iL3RKgY82hb">[2305.16291] Voyager: An Open-Ended Embodied Agent with Large Language Models</a></span><span class="c3">&nbsp;[AI Minecraft player upgraded, much better than previous attempts]</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2306.03341&amp;sa=D&amp;source=editors&amp;ust=1715645745565483&amp;usg=AOvVaw3UXSfW8F4napPhVeICUUQl">[2306.03341] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://www.dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/&amp;sa=D&amp;source=editors&amp;ust=1715645745565968&amp;usg=AOvVaw3ySEB6WDKiAnRZKZkJNhOD">Speeding up the GPT - KV cache | Becoming The Unbeatable</a></span><span class="c3">&nbsp;(old but gold, almost default in every lib)</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2306.13253&amp;sa=D&amp;source=editors&amp;ust=1715645745566349&amp;usg=AOvVaw0mFITUSPoF93yTbKbk5TMJ">Predicting Grokking in LLMs</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2306.10209&amp;sa=D&amp;source=editors&amp;ust=1715645745566707&amp;usg=AOvVaw1XqhhnhT-FpT8BBDg5yfeN">[2306.10209] ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c29">RAGs (Retrieval augmented generation)</span><span class="c3">:</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2112.04426&amp;sa=D&amp;source=editors&amp;ust=1715645745567205&amp;usg=AOvVaw20dl7iS3_7HwW1_BxL9rSv">[2112.04426] Improving language models by retrieving from trillions of tokens</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745567588&amp;usg=AOvVaw1KR_lZgy65R1RjBt6X37Xy">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2208.03299&amp;sa=D&amp;source=editors&amp;ust=1715645745567917&amp;usg=AOvVaw0WZYtS48U4vM-UwWQETbgD">[2208.03299] Atlas: Few-shot Learning with Retrieval Augmented Language Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://knowledge-nlp.github.io/aaai2023/papers/003-PQA-oral.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745568255&amp;usg=AOvVaw350Fa-IHNQcvtCM5G6NZvD">Knowledge Retrieval Over Public and Private Data</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2211.09260&amp;sa=D&amp;source=editors&amp;ust=1715645745568553&amp;usg=AOvVaw13w8pkQKy09D4HmRUtM560">[2211.09260] Task-aware Retrieval with Instructions</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2307.07164/&amp;sa=D&amp;source=editors&amp;ust=1715645745568879&amp;usg=AOvVaw2uMO9lAsz_6nvx0wSSEGam">https://arxiv.org/abs/2307.07164/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2302.00083&amp;sa=D&amp;source=editors&amp;ust=1715645745569234&amp;usg=AOvVaw1F-_JXJlKIyY80EMVyjCVG">[2302.00083] In-Context Retrieval-Augmented Language Models</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2209.14290&amp;sa=D&amp;source=editors&amp;ust=1715645745569546&amp;usg=AOvVaw1ZJlyO9vdYlvonkdIPlqel">[2209.14290] FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2303.08518&amp;sa=D&amp;source=editors&amp;ust=1715645745569875&amp;usg=AOvVaw0bv66dkXAD_J2YVhTlApEc">[2303.08518] UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2212.10496&amp;sa=D&amp;source=editors&amp;ust=1715645745570184&amp;usg=AOvVaw19ZJm-eoiS4MKMCPkSXqKY">[2212.10496] Precise Zero-Shot Dense Retrieval without Relevance Labels</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://gpus.llm-utils.org/nvidia-h100-gpus-supply-and-demand/&amp;sa=D&amp;source=editors&amp;ust=1715645745570523&amp;usg=AOvVaw1vvsrBjMruSrbMq0EyucYN">Nvidia H100 GPUs: Supply and Demand · GPU Utils ⚡️</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://blog.fireworks.ai/speed-python-pick-two-how-cuda-graphs-enable-fast-python-code-for-deep-learning-353bf6241248&amp;sa=D&amp;source=editors&amp;ust=1715645745570889&amp;usg=AOvVaw3uXrRQb8cGNEBafc8ib3YO">https://blog.fireworks.ai/speed-python-pick-two-how-cuda-graphs-enable-fast-python-code-for-deep-learning-353bf6241248</a></span><span class="c3">&nbsp;- Cuda Graphs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2306.02519&amp;sa=D&amp;source=editors&amp;ust=1715645745571202&amp;usg=AOvVaw2r94ZEyJrKNISzO_FBHkLe">[2306.02519] Transformative AGI by 2043 is &lt;1% likely</a></span><span class="c3">&nbsp;- Interesting section on compute limitation pointed out by jeremy howard</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/papers/2309.03409&amp;sa=D&amp;source=editors&amp;ust=1715645745571523&amp;usg=AOvVaw3zMfCvoa6ECP0kTzyUzc_I">Paper page - Large Language Models as Optimizers</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2309.05444&amp;sa=D&amp;source=editors&amp;ust=1715645745571838&amp;usg=AOvVaw0Nh_xEWqQ26cwuZWVc7uIy">Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning</a></span><span class="c3">&nbsp;- Must read since approved by pony anon</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.05065&amp;sa=D&amp;source=editors&amp;ust=1715645745572145&amp;usg=AOvVaw0wq61IuIHpKFJmVi61naoL">[2305.05065] Recommender Systems with Generative Retrieval</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2309.06180&amp;sa=D&amp;source=editors&amp;ust=1715645745572434&amp;usg=AOvVaw3g-6xxWByTcnxiTVIOqzHN">https://arxiv.org/abs/2309.06180</a></span><span class="c3">&nbsp;- vLLM paper, paged attention, how to scale up LLM inference basically</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://transformer-circuits.pub/2021/framework/index.html&amp;sa=D&amp;source=editors&amp;ust=1715645745572780&amp;usg=AOvVaw1Fm-92g1DNtg_VobaGP0x5">https://transformer-circuits.pub/2021/framework/index.html</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Moravec%2527s_paradox%23:~:text%3DMoravec%27s%2520paradox%2520is%2520the%2520observation,skills%2520require%2520enormous%2520computational%2520resources&amp;sa=D&amp;source=editors&amp;ust=1715645745573150&amp;usg=AOvVaw0rAEW6fJG6a_0qE7tUEicE">https://en.wikipedia.org/wiki/Moravec%27s_paradox#:~:text=Moravec's%20paradox%20is%20the%20observation,skills%20require%20enormous%20computational%20resources</a></span><span class="c3">.</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2204.03084&amp;sa=D&amp;source=editors&amp;ust=1715645745573543&amp;usg=AOvVaw3cUYwsRfimOXA6EEoMuOeh">[2204.03084] Knowledge Infused Decoding</a></span><span class="c3">&nbsp;</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2210.06316&amp;sa=D&amp;source=editors&amp;ust=1715645745573853&amp;usg=AOvVaw1Q52qy4LOH56tfKu050SHZ">[2210.06316] Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2303.11366&amp;sa=D&amp;source=editors&amp;ust=1715645745574166&amp;usg=AOvVaw0FhsgyqV-ntHMdxZ8Ako5G">[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning</a></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 750.99px; height: 554.53px;"><img alt="" src="./Keeping up with AGI_files/pasted image 0(97).png" style="width: 750.99px; height: 554.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c34 c29 c57"></span></p><p class="c9"><span class="c29 c57">Legacy reading list (borrowed from </span><span class="c7 c29 c57"><a class="c10" href="https://www.google.com/url?q=https://yacine.ca/&amp;sa=D&amp;source=editors&amp;ust=1715645745574760&amp;usg=AOvVaw0FWSQC3z7Qd2A65lc3p0pf">yacine.ca</a></span><span class="c34 c29 c57">)</span></p><ul class="c4 lst-kix_bjy5owhtjxgd-0 start"><li class="c6 li-bullet-0"><span class="c22 c30">https://sites.research.google/usm/</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.14177 (d)</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.14189 (n)</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.15274</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.14420</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.15233</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.14177</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.13648</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.13455</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec/</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.13375</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.13496</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://medhini.github.io/task_structure/</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2303.12208&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://blog.eladgil.com/p/video-and-transcript-fireside-chat?utm_source=twitter&amp;sd=pf</span></li><li class="c6 li-bullet-0"><span class="c7 c30"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2303.15389&amp;sa=D&amp;source=editors&amp;ust=1715645745576170&amp;usg=AOvVaw0698A0GQrrp7uoJCX9FdMf">https://arxiv.org/abs/2303.15389</a></span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/pdf/2106.09685.pdf </span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/abs/2203.08913</span></li><li class="c6 li-bullet-0"><span class="c22 c30">compare it with retro https://arxiv.org/pdf/2112.04426.pdf</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/pdf/2303.10512.pdf</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://twitter.com/WeizhuChen/status/1637670118334545920 </span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/pdf/2303.12712.pdf </span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/pdf/2303.13506.pdf</span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/pdf/2303.11366.pdf </span></li><li class="c6 li-bullet-0"><span class="c22 c30">https://arxiv.org/pdf/2301.12597.pdf</span></li><li class="c6 li-bullet-0"><span class="c7 c30"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2301.12597.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745577061&amp;usg=AOvVaw05-Wf0vYn0Y91z5GoSe9AV">https://arxiv.org/pdf/2301.12597.pdf</a></span></li><li class="c6 li-bullet-0"><span class="c7 c30"><a class="c10" href="https://www.google.com/url?q=https://horace.io/brrr_intro.html&amp;sa=D&amp;source=editors&amp;ust=1715645745577327&amp;usg=AOvVaw18FbVLMbqLHpD_wn7KpAiO">https://horace.io/brrr_intro.html</a></span></li><li class="c6 li-bullet-0"><span class="c7 c30"><a class="c10" href="https://www.google.com/url?q=https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&amp;sa=D&amp;source=editors&amp;ust=1715645745577629&amp;usg=AOvVaw07UkayeNxBswd7yQ-R4Oxv">https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</a></span></li></ul><p class="c0"><span class="c3"></span></p><p class="c9"><span>New Reading List + Alpha (Borrowed from </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://yacine.ca/%23:~:text%3Da%2520new%2520image-,useful%2520links,-https%253A//dreamtuner%252Ddiffusion&amp;sa=D&amp;source=editors&amp;ust=1715645745577989&amp;usg=AOvVaw3JejrcTH5ORA7pEtQ1fuAk">yacine</a></span><span class="c3">)</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://dreamtuner-diffusion.github.io/&amp;sa=D&amp;source=editors&amp;ust=1715645745578303&amp;usg=AOvVaw0D678LJiJFV43DaaIEU7hN">DreamTuner</a></span><span class="c3">&nbsp;- ipadapter but different</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2312.13789.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745578652&amp;usg=AOvVaw0109ClorZseJruIGxfKaYK">https://arxiv.org/pdf/2312.13789.pdf </a></span><span class="c3">- how i beat the big wigs</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/2312.09608&amp;sa=D&amp;source=editors&amp;ust=1715645745578974&amp;usg=AOvVaw2zc6EgKU0ip98GCmYfMD9u">[2312.09608] Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</a></span><span class="c3">&nbsp;- faster stable diffusion by skipping unecessary bits</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/damo-vilab/AnyDoor&amp;sa=D&amp;source=editors&amp;ust=1715645745579310&amp;usg=AOvVaw10ywus_tkaEx6KVSh5VcdO">Official implementations for paper: Anydoor: zero-shot object-level image customization</a></span><span>&nbsp;- instruct edit + </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://old.reddit.com/r/StableDiffusion/comments/18kd0na/code_for_anydoor_zeroshot_objectlevel_image/&amp;sa=D&amp;source=editors&amp;ust=1715645745579677&amp;usg=AOvVaw2sbArgDhf7j0yVSVP-xhWZ">https://old.reddit.com/r/StableDiffusion/comments/18kd0na/code_for_anydoor_zeroshot_objectlevel_image/</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span>dont forget - diffusion slider demo </span><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/Kevin-thu/DiffMorpher?tab%3Dreadme-ov-file&amp;sa=D&amp;source=editors&amp;ust=1715645745580304&amp;usg=AOvVaw3v4Zy3ARTpWODzozTKNTGO">https://github.com/Kevin-thu/DiffMorpher?tab=readme-ov-file</a></span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/pdf/2312.01943.pdf&amp;sa=D&amp;source=editors&amp;ust=1715645745580861&amp;usg=AOvVaw0xkc8DaDJAzCeYULbmkthI">https://arxiv.org/pdf/2312.01943.pdf </a></span><span class="c3">- i should use this for anime</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://huggingface.co/spaces/JingyeChen22/TextDiffuser-2&amp;sa=D&amp;source=editors&amp;ust=1715645745581420&amp;usg=AOvVaw2GjLcJ5Dd7u94kj36ZgNuB">TextDiffuser 2 - a Hugging Face Space by JingyeChen22</a></span><span class="c3">&nbsp;- people have been asking for text.. right?</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/open-mmlab/PIA&amp;sa=D&amp;source=editors&amp;ust=1715645745581985&amp;usg=AOvVaw11CF3olC7v5z3Tsw1efWu7">GitHub - open-mmlab/PIA: PIA, your Personalized Image Animator. Animate your images by text prompt, combing with Dreambooth, achieving stunning videos. PIA，你的个性化图像动画生成器，利用文本提示将图像变为奇妙的动画</a></span><span class="c3">&nbsp;- video generator</span></p><p class="c0"><span class="c3"></span></p><p class="c9"><span class="c7"><a class="c10" href="https://www.google.com/url?q=https://github.com/cumulo-autumn/StreamDiffusion&amp;sa=D&amp;source=editors&amp;ust=1715645745582507&amp;usg=AOvVaw17amMAc34FZyKawMLUshHv">GitHub - cumulo-autumn/StreamDiffusion: StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation</a></span><span class="c3">&nbsp;- turbo go fast</span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p><p class="c0"><span class="c3"></span></p></div></div><script nonce="">function _F_toggles_initialize(a){("undefined"!==typeof globalThis?globalThis:"undefined"!==typeof self?self:this)._F_toggles=a||[]}_F_toggles_initialize([]);
function k(){return function(){}}var aa="function"==typeof Object.defineProperties?Object.defineProperty:function(a,b,c){if(a==Array.prototype||a==Object.prototype)return a;a[b]=c.value;return a};function ba(a){a=["object"==typeof globalThis&&globalThis,a,"object"==typeof window&&window,"object"==typeof self&&self,"object"==typeof global&&global];for(var b=0;b<a.length;++b){var c=a[b];if(c&&c.Math==Math)return c}throw Error("a");}var ca=ba(this);
function l(a,b){if(b)a:{var c=ca;a=a.split(".");for(var d=0;d<a.length-1;d++){var e=a[d];if(!(e in c))break a;c=c[e]}a=a[a.length-1];d=c[a];b=b(d);b!=d&&null!=b&&aa(c,a,{configurable:!0,writable:!0,value:b})}}var da="function"==typeof Object.create?Object.create:function(a){function b(){}b.prototype=a;return new b},m;
if("function"==typeof Object.setPrototypeOf)m=Object.setPrototypeOf;else{var n;a:{var ea={a:!0},fa={};try{fa.__proto__=ea;n=fa.a;break a}catch(a){}n=!1}m=n?function(a,b){a.__proto__=b;if(a.__proto__!==b)throw new TypeError("b`"+a);return a}:null}var ha=m;
function p(a,b){a.prototype=da(b.prototype);a.prototype.constructor=a;if(ha)ha(a,b);else for(var c in b)if("prototype"!=c)if(Object.defineProperties){var d=Object.getOwnPropertyDescriptor(b,c);d&&Object.defineProperty(a,c,d)}else a[c]=b[c];a.o=b.prototype}l("Object.is",function(a){return a?a:function(b,c){return b===c?0!==b||1/b===1/c:b!==b&&c!==c}});
l("String.fromCodePoint",function(a){return a?a:function(b){for(var c="",d=0;d<arguments.length;d++){var e=Number(arguments[d]);if(0>e||1114111<e||e!==Math.floor(e))throw new RangeError("c`"+e);65535>=e?c+=String.fromCharCode(e):(e-=65536,c+=String.fromCharCode(e>>>10&1023|55296),c+=String.fromCharCode(e&1023|56320))}return c}});
l("Array.prototype.find",function(a){return a?a:function(b,c){a:{var d=this;d instanceof String&&(d=String(d));for(var e=d.length,f=0;f<e;f++){var g=d[f];if(b.call(c,g,f,d)){b=g;break a}}b=void 0}return b}});/*

 Copyright The Closure Library Authors.
 SPDX-License-Identifier: Apache-2.0
*/
var q=this||self,ia=q._F_toggles||[];function ja(a){var b=typeof a;return"object"==b&&null!=a||"function"==b}function ka(a,b){var c=Array.prototype.slice.call(arguments,1);return function(){var d=c.slice();d.push.apply(d,arguments);return a.apply(this,d)}}
function la(a,b){function c(){}c.prototype=b.prototype;a.o=b.prototype;a.prototype=new c;a.prototype.constructor=a;a.H=function(d,e,f){for(var g=Array(arguments.length-2),h=2;h<arguments.length;h++)g[h-2]=arguments[h];return b.prototype[e].apply(d,g)}}function ma(a){return a};function na(a){na[" "](a);return a}na[" "]=k();function oa(a,b,c){return Object.prototype.hasOwnProperty.call(a,b)?a[b]:a[b]=c(b)};function r(){}r.prototype.l=function(){return this.j||(Object.defineProperties(this,{j:{value:pa=pa+1|0,enumerable:!1}}),this.j)};r.prototype.toString=function(){return t(u(v(this.constructor)))+"@"+t((this.l()>>>0).toString(16))};r.prototype.i=["java.lang.Object",0];function w(){}p(w,r);function x(a,b){a.g=b;qa(b,a)}function y(a){ra(a.g)&&(Error.captureStackTrace?Error.captureStackTrace(z(a.g,ra,sa)):z(a.g,ra,sa).stack=Error().stack)}w.prototype.toString=function(){var a=u(v(this.constructor)),b=this.h;return null==b?a:t(a)+": "+t(b)};function ta(a){if(null!=a){var b=a.v;if(b)return b}a instanceof TypeError?b=ua():(b=new A,y(b),x(b,Error(b)));b.h=null==a?"null":a.toString();x(b,a);return b}w.prototype.i=["java.lang.Throwable",0];function B(){}p(B,w);B.prototype.i=["java.lang.Exception",0];function C(){}p(C,B);C.prototype.i=["java.lang.RuntimeException",0];function va(){}p(va,C);va.prototype.i=["java.lang.IndexOutOfBoundsException",0];function z(a,b,c){if(null!=a&&!b(a))throw a=t(u(wa(a)))+" cannot be cast to "+t(u(v(c))),b=new xa,b.h=a,y(b),x(b,Error(b)),b.g;return a};function ya(){}p(ya,r);ya.prototype.i=["java.lang.Boolean",0];function za(){}p(za,r);za.prototype.i=["java.lang.Number",0];function Ba(){}p(Ba,za);Ba.prototype.i=["java.lang.Double",0];function Ca(a,b){D(a)?(Da(b,a.length),a=a.charCodeAt(b)):a=a.g(b);return a};function A(){}p(A,C);A.prototype.i=["java.lang.JsException",0];function Ea(){}p(Ea,A);function ua(){var a=new Ea;y(a);x(a,new TypeError(a));return a}Ea.prototype.i=["java.lang.NullPointerException",0];function xa(){}p(xa,C);xa.prototype.i=["java.lang.ClassCastException",0];var pa=0;function wa(a){switch(Fa(typeof a)){case "number":return v(Ba);case "boolean":return v(ya);case "string":return v(E);case "function":return v(Ga)}if(a instanceof r)a=v(a.constructor);else if(Array.isArray(a))a=(a=a.G)?v(a.J,a.I):v(r,1);else if(null!=a)a=v(Ha);else throw new TypeError("d");return a};function Ga(){}Ga.prototype.i=["<native function>",1];function Ha(){}p(Ha,r);Ha.prototype.i=["<native object>",0];function Ia(){}p(Ia,va);function Ja(a){var b=new Ia;b.h=a;y(b);x(b,Error(b));return b}Ia.prototype.i=["java.lang.StringIndexOutOfBoundsException",0];function Fa(a){if(null==a)throw ua().g;return a}function Da(a,b){if(0>a||a>=b)throw Ja("e`"+a+"`"+b).g;};function sa(){}function ra(a){return a instanceof Error}sa.prototype.i=["Error",0];function qa(a,b){if(a instanceof Object)try{a.v=b,Object.defineProperties(a,{cause:{get:function(){return b.m&&b.m.g}}})}catch(c){}};function E(){}p(E,r);function t(a){return null==a?"null":a.toString()}function Ka(a,b){var c=b,d=a.length,e;b=Ca(a,(e=c,c=c+1|0,e));var f;if(e=55296<=b&&56319>=b&&c<d)a=f=Ca(a,c),e=56320<=a&&57343>=a;var g;e?g=65536+((b&1023)<<10)+(f&1023)|0:g=b;return g}function D(a){return"string"===typeof a}E.prototype.i=["java.lang.String",0];function F(){}var La,Ma;p(F,r);function Na(){Na=k();Ma=new G;La=new H}F.prototype.i=["java.util.Locale",0];function G(){}p(G,F);G.prototype.toString=function(){return""};G.prototype.i=["java.util.Locale$1",0];function H(){}p(H,F);H.prototype.toString=function(){return"unknown"};H.prototype.i=["java.util.Locale$4",0];function I(a,b){this.g=a;this.h=b}p(I,r);function u(a){if(0!=a.h){var b=3==a.g.prototype.i[1]?a.g.prototype.i[2]:"L"+t(a.g.prototype.i[0])+";";a=a.h;for(var c="",d=0;d<a;d=d+1|0)c=t(c)+"[";return t(c)+t(b)}return a.g.prototype.i[0]}I.prototype.toString=function(){return String(0==this.h&&1==this.g.prototype.i[1]?"interface ":0==this.h&&3==this.g.prototype.i[1]?"":"class ")+t(u(this))};I.prototype.i=["java.lang.Class",0];
function v(a,b){var c=b||0;return oa(a.prototype,"$$class/"+c,function(){return new I(a,c)})};function Oa(){}function Pa(a){return a instanceof Array}Oa.prototype.i=["Array",0];function Qa(a){var b=a;try{var c;if(c="www.google.com"===Ra(a)){var d=Sa(a);var e=z(d[5],D,E);c="/url"===(null==e?null:decodeURI(e))}if(c){var f=Ta(a,"q");b=f?f:Ta(a,"url")}}catch(h){var g=ta(h);if(!(g instanceof B))throw g.g;}return null!=b?b:""};var Ua;function Sa(a){Va();a=Ua.exec(a);for(var b=z([],Pa,Oa),c=0;7>=c;c=c+1|0)if(!a||a.length<=c||null==z(a[c],D,E))b.push(null);else{var d=b,e=z(a[c],D,E);d.push(e)}return b}function Ra(a){a=Sa(a);a=z(a[3],D,E);return null==a?null:decodeURI(a)}function Va(){Va=k();Ua=RegExp("^(?:([^:/?#.]+):)?(?://(?:([^/?#]*)@)?([^/#?]*?)(?::([0-9]+))?(?=[/#?]|$))?([^?#]+)?(?:\\?([^#]*))?(?:#([\\S\\s]*))?$","")};function Ta(a,b){var c=String.fromCodePoint(35);c=a.indexOf(c);c=0>c?a.length:c;a:{var d=0;for(var e=b.length;0<(d=a.indexOf(b,d))&&d<c;){var f=Ka(a,d-1|0);if(38==f||63==f){if((d+e|0)>=a.length)break a;f=Ka(a,d+e|0);if(61==f||38==f||35==f)break a}d=d+(e+1)|0}d=-1}if(0>d)return null;e=d;f=String.fromCodePoint(38);e=a.indexOf(f,e);if(0>e||e>c)e=c;d=d+(b.length+1)|0;b=decodeURIComponent;c=Math.min(a.length,d);d=Math.min(a.length,Math.max(d,e));e=a.length;if(0>c||d>e||d<c)throw Ja("f`"+c+"`"+d+"`"+e).g;
a=a.substr(c,d-c|0);c=(43).toString(16);d=c.length;Da(d,5);a=a.replace(new RegExp("\\u"+t("0000".substr(d))+t(c),"g"),String.fromCharCode(32));return b(a)};function Wa(){Wa=k();Xa||Ya()};var Za=!!(ia[0]&512);var $a;if(ia[0]&256)$a=Za;else{var J;a:{for(var ab=["WIZ_global_data","oxN3nb"],K=q,bb=0;bb<ab.length;bb++)if(K=K[ab[bb]],null==K){J=null;break a}J=K}var cb=J&&J[610401301];$a=null!=cb?cb:!1}var db=$a;var eb=/&/g,fb=/</g,gb=/>/g,hb=/"/g,ib=/'/g,jb=/\x00/g,kb=/[\x00&<>"']/;var lb=null,L,mb=q.navigator;L=mb?mb.userAgentData||null:null;function M(){var a;if(null==lb)a:{if(a=q.navigator)if(a=a.userAgent)break a;a=""}else a=lb;return a}function N(a){return-1!=M().indexOf(a)};function nb(){if(!db)return!1;var a=L;return!!a&&0<a.brands.length}function ob(a){a.forEach(k())}function pb(){for(var a=M(),b=RegExp("([A-Z][\\w ]+)/([^\\s]+)\\s*(?:\\((.*?)\\))?","g"),c=[],d;d=b.exec(a);)c.push([d[1],d[2],d[3]||void 0]);ob(c)}function Ya(){nb()?L.brands.find(function(a){return"Firefox"===a.brand}):pb()};var qb=Array.prototype.indexOf?function(a,b){return Array.prototype.indexOf.call(a,b,void 0)}:function(a,b){if("string"===typeof a)return"string"!==typeof b||1!=b.length?-1:a.indexOf(b,0);for(var c=0;c<a.length;c++)if(c in a&&a[c]===b)return c;return-1};var rb=N("Gecko")&&!(-1!=M().toLowerCase().indexOf("webkit")&&!N("Edge"))&&!(N("Trident")||N("MSIE"))&&!N("Edge"),Xa=-1!=M().toLowerCase().indexOf("webkit")&&!N("Edge"),sb;if(db){var tb=L;sb=!!tb&&!!tb.platform}else sb=!1;var ub=sb?"macOS"===L.platform:N("Macintosh");function vb(a){a&&"function"==typeof a.u&&a.u()};function O(){this.h=this.h;this.g=this.g}O.prototype.h=!1;O.prototype.u=function(){this.h||(this.h=!0,this.m())};O.prototype.m=function(){if(this.g)for(;this.g.length;)this.g.shift()()};function wb(a,b,c){for(var d in a)b.call(c,a[d],d,a)};function P(a,b){this.g=a===xb&&b||"";this.h=yb}P.prototype.toString=function(){return this.g};var yb={},xb={};var Q;/*

 SPDX-License-Identifier: Apache-2.0
*/
function R(a){this.g=a}R.prototype.toString=function(){return this.g};var zb=new R("about:invalid#zClosurez");function Ab(a){if(a instanceof R)return a.g;throw Error("h");};function Bb(a){this.A=a}function S(a){return new Bb(function(b){return b.substr(0,a.length+1).toLowerCase()===a+":"})}var Cb=[S("data"),S("http"),S("https"),S("mailto"),S("ftp"),new Bb(function(a){return/^[^:]*([/?#]|$)/.test(a)})],Db=/^\s*(?!javascript:)(?:[\w+.-]+:|[^:/?#]*(?:[/?#]|$))/i;var Eb={};function T(a){this.g=a}T.prototype.toString=function(){return this.g.toString()};function Fb(a,b,c,d){a=Gb(a);b=b||q;c=c instanceof P?c instanceof P&&c.constructor===P&&c.h===yb?c.g:"type_error:Const":c||"";return void 0!==d?b.open(a,c,d):b.open(a,c)}function Gb(a){a instanceof R?a=Ab(a):a=Db.test(a)?a:void 0;return void 0!==a?a:zb.toString()};function Hb(a,b){this.type=a;this.target=b;this.j=!1}Hb.prototype.g=function(){this.j=!0};var Ib=function(){if(!q.addEventListener||!Object.defineProperty)return!1;var a=!1,b=Object.defineProperty({},"passive",{get:function(){a=!0}});try{var c=k();q.addEventListener("test",c,b);q.removeEventListener("test",c,b)}catch(d){}return a}();function U(a){Hb.call(this,a?a.type:"");this.relatedTarget=this.target=null;this.button=this.screenY=this.screenX=this.clientY=this.clientX=0;this.key="";this.metaKey=this.shiftKey=this.altKey=this.ctrlKey=!1;this.state=null;this.pointerId=0;this.pointerType="";this.h=null;if(a){var b=this.type=a.type,c=a.changedTouches&&a.changedTouches.length?a.changedTouches[0]:null;this.target=a.target||a.srcElement;var d=a.relatedTarget;if(d){if(rb){a:{try{na(d.nodeName);var e=!0;break a}catch(f){}e=!1}e||(d=
null)}}else"mouseover"==b?d=a.fromElement:"mouseout"==b&&(d=a.toElement);this.relatedTarget=d;c?(this.clientX=void 0!==c.clientX?c.clientX:c.pageX,this.clientY=void 0!==c.clientY?c.clientY:c.pageY,this.screenX=c.screenX||0,this.screenY=c.screenY||0):(this.clientX=void 0!==a.clientX?a.clientX:a.pageX,this.clientY=void 0!==a.clientY?a.clientY:a.pageY,this.screenX=a.screenX||0,this.screenY=a.screenY||0);this.button=a.button;this.key=a.key||"";this.ctrlKey=a.ctrlKey;this.altKey=a.altKey;this.shiftKey=
a.shiftKey;this.metaKey=a.metaKey;this.pointerId=a.pointerId||0;this.pointerType="string"===typeof a.pointerType?a.pointerType:Jb[a.pointerType]||"";this.state=a.state;this.h=a;a.defaultPrevented&&U.o.g.call(this)}}la(U,Hb);var Jb={2:"touch",3:"pen",4:"mouse"};U.prototype.g=function(){U.o.g.call(this);var a=this.h;a.preventDefault?a.preventDefault():a.returnValue=!1};var Kb="closure_listenable_"+(1E6*Math.random()|0);var Lb=0;function Mb(a,b,c,d,e){this.listener=a;this.proxy=null;this.src=b;this.type=c;this.capture=!!d;this.h=e;this.key=++Lb;this.g=this.s=!1}function Nb(a){a.g=!0;a.listener=null;a.proxy=null;a.src=null;a.h=null};function Ob(a){this.src=a;this.g={};this.h=0}Ob.prototype.add=function(a,b,c,d,e){var f=a.toString();a=this.g[f];a||(a=this.g[f]=[],this.h++);var g;a:{for(g=0;g<a.length;++g){var h=a[g];if(!h.g&&h.listener==b&&h.capture==!!d&&h.h==e)break a}g=-1}-1<g?(b=a[g],c||(b.s=!1)):(b=new Mb(b,this.src,f,!!d,e),b.s=c,a.push(b));return b};var Pb="closure_lm_"+(1E6*Math.random()|0),Qb={},Rb=0;function Sb(a,b,c,d,e){if(d&&d.once)return Tb(a,b,c,d,e);if(Array.isArray(b)){for(var f=0;f<b.length;f++)Sb(a,b[f],c,d,e);return null}c=Ub(c);return a&&a[Kb]?a.g(b,c,ja(d)?!!d.capture:!!d,e):Vb(a,b,c,!1,d,e)}
function Vb(a,b,c,d,e,f){if(!b)throw Error("j");var g=ja(e)?!!e.capture:!!e,h=Wb(a);h||(a[Pb]=h=new Ob(a));c=h.add(b,c,d,g,f);if(c.proxy)return c;d=Xb();c.proxy=d;d.src=a;d.listener=c;if(a.addEventListener)Ib||(e=g),void 0===e&&(e=!1),a.addEventListener(b.toString(),d,e);else if(a.attachEvent)a.attachEvent(Yb(b.toString()),d);else if(a.addListener&&a.removeListener)a.addListener(d);else throw Error("k");Rb++;return c}
function Xb(){function a(c){return b.call(a.src,a.listener,c)}var b=Zb;return a}function Tb(a,b,c,d,e){if(Array.isArray(b)){for(var f=0;f<b.length;f++)Tb(a,b[f],c,d,e);return null}c=Ub(c);return a&&a[Kb]?a.h(b,c,ja(d)?!!d.capture:!!d,e):Vb(a,b,c,!0,d,e)}
function $b(a){if("number"!==typeof a&&a&&!a.g){var b=a.src;if(b&&b[Kb])b.j(a);else{var c=a.type,d=a.proxy;b.removeEventListener?b.removeEventListener(c,d,a.capture):b.detachEvent?b.detachEvent(Yb(c),d):b.addListener&&b.removeListener&&b.removeListener(d);Rb--;if(c=Wb(b)){d=a.type;if(d in c.g){var e=c.g[d],f=qb(e,a),g;(g=0<=f)&&Array.prototype.splice.call(e,f,1);g&&(Nb(a),0==c.g[d].length&&(delete c.g[d],c.h--))}0==c.h&&(c.src=null,b[Pb]=null)}else Nb(a)}}}
function Yb(a){return a in Qb?Qb[a]:Qb[a]="on"+a}function Zb(a,b){if(a.g)a=!0;else{b=new U(b,this);var c=a.listener,d=a.h||a.src;a.s&&$b(a);a=c.call(d,b)}return a}function Wb(a){a=a[Pb];return a instanceof Ob?a:null}var ac="__closure_events_fn_"+(1E9*Math.random()>>>0);function Ub(a){if("function"===typeof a)return a;a[ac]||(a[ac]=function(b){return a.handleEvent(b)});return a[ac]};function V(a){O.call(this);this.l=a;this.j={}}la(V,O);var bc=[];function cc(a){wb(a.j,function(b,c){this.j.hasOwnProperty(c)&&$b(b)},a);a.j={}}V.prototype.m=function(){V.o.m.call(this);cc(this)};V.prototype.handleEvent=function(){throw Error("l");};function dc(a,b,c,d){b=void 0===b?!1:b;c=void 0===c?!1:c;d=void 0===d?"editors":d;O.call(this);this.j=a||document.body;this.B=!!b;this.C=!!c;this.D=d;this.l=new V(this);a=ka(vb,this.l);this.h?a():(this.g||(this.g=[]),this.g.push(a));a=this.l;b=this.j;c=this.F;d="click";Array.isArray(d)||(d&&(bc[0]=d.toString()),d=bc);for(var e=0;e<d.length;e++){var f=Sb(b,d[e],c||a.handleEvent,!1,a.l||a);if(!f)break;a.j[f.key]=f}}p(dc,O);
dc.prototype.F=function(a){if(!(0!=a.h.button||ub&&a.ctrlKey||a.j))for(var b=a.target;b&&b!=this.j;){if(b.tagName&&"a"==b.tagName.toLowerCase()){var c=b.getAttribute("href")||b.getAttributeNS("http://www.w3.org/1999/xlink","href"),d=Qa(c);if(c!=d){a:{b=void 0;Wa();try{b=Ra(d)}catch(Aa){b=ta(Aa);if(b instanceof C){b=!1;break a}throw b.g;}var e;if(e=null!=b){e=(Na(),Ma);var f=La;b=Object.is(e,f)||null==e&&null==f?b.toLocaleLowerCase():b.toLowerCase();e=".google.com"===Fa(b.substr(b.length-11|0,11))}b=
e}if(!b)if(this.C){c=Qa(d);c=Fa(c);if(Object.is(c,d)||null==c&&null==d)d="//www.google.com/url?q="+t(encodeURIComponent(d)),d=t(d)+"&sa=D";d+="&source="+this.D}else d=this.B?c:d;e=d;b=void 0;c={target:"_blank",noreferrer:!0};d=window;if(e instanceof R)f=e;else{var g=void 0;g=void 0===g?Cb:g;a:if(f="undefined"!=typeof e.href?e.href:String(e),g=void 0===g?Cb:g,!(f instanceof R)){for(var h=0;h<g.length;++h){var W=g[h];if(W instanceof Bb&&W.A(f)){f=new R(f);break a}}f=void 0}f=f||zb}h=void 0!==self.crossOriginIsolated;
g="strict-origin-when-cross-origin";window.Request&&(g=(new Request("/")).referrerPolicy);W="unsafe-url"===g;g=c.noreferrer;if(h&&g){if(W)throw Error("m");g=!1}e=c.target||e.target;h=[];for(b in c)switch(b){case "width":case "height":case "top":case "left":h.push(b+"="+c[b]);break;case "target":case "noopener":case "noreferrer":break;default:h.push(b+"="+(c[b]?1:0))}b=h.join(",");if((N("iPhone")&&!N("iPod")&&!N("iPad")||N("iPad")||N("iPod"))&&d.navigator&&d.navigator.standalone&&e&&"_self"!=e)c="A",
b=document,c=String(c),"application/xhtml+xml"===b.contentType&&(c=c.toLowerCase()),c=b.createElement(c),c.href=Gb(f),c.target=e,g&&(c.rel="noreferrer"),b=document.createEvent("MouseEvent"),b.initMouseEvent("click",!0,!0,d,1),c.dispatchEvent(b);else if(g){if(d=Fb("",d,e,b),c=Ab(f),d){d.opener=null;""===c&&(c="javascript:''");kb.test(c)&&(-1!=c.indexOf("&")&&(c=c.replace(eb,"&amp;")),-1!=c.indexOf("<")&&(c=c.replace(fb,"&lt;")),-1!=c.indexOf(">")&&(c=c.replace(gb,"&gt;")),-1!=c.indexOf('"')&&(c=c.replace(hb,
"&quot;")),-1!=c.indexOf("'")&&(c=c.replace(ib,"&#39;")),-1!=c.indexOf("\x00")&&(c=c.replace(jb,"&#0;")));c='<meta name="referrer" content="no-referrer"><meta http-equiv="refresh" content="0; url='+c+'">';if(void 0===Q)if(b=null,(e=q.trustedTypes)&&e.createPolicy){try{b=e.createPolicy("goog#html",{createHTML:ma,createScript:ma,createScriptURL:ma})}catch(Aa){q.console&&q.console.error(Aa.message)}Q=b}else Q=b;c=(b=Q)?b.createHTML(c):c;c=new T(c,Eb);(d=d.document)&&d.write&&(d.write(c instanceof T&&
c.constructor===T?c.g:"type_error:SafeHtml"),d.close())}}else(d=Fb(f,d,e,b))&&c.noopener&&(d.opener=null),d&&c.noreferrer&&(d.opener=null);a.g();break}}b=b.parentNode}};function ec(a,b,c,d){new dc(a,void 0===b?!1:b,void 0===c?!1:c,d)}var X=["DOCS_installLinkReferrerSanitizer"],Y=q;X[0]in Y||"undefined"==typeof Y.execScript||Y.execScript("var "+X[0]);for(var Z;X.length&&(Z=X.shift());)X.length||void 0===ec?Y[Z]&&Y[Z]!==Object.prototype[Z]?Y=Y[Z]:Y=Y[Z]={}:Y[Z]=ec;
// Google Inc.

//# sourceMappingURL=linkreferrer_binary_linkreferrer_binary_chunk.sourcemap
</script><script nonce="">DOCS_installLinkReferrerSanitizer( undefined,  true,  true, 'docs');</script><script nonce="">_docs_flag_initialData={"ilcm":{"eui":"ADFN-ct4C5qX7AeFaO3ks2H2u0BgTpXZLdk6lUqEI2c1y3P2_GtnOAVoPtTWJZwIJMaoIkGaF-4f","je":1,"sstu":1715642145808602,"si":"CNvBpoXhi4YDFc_jcwQdl60Ddg","gsc":3,"ei":[5700032,5700057,5700103,5700114,5700333,5700611,5703307,5703757,5703839,5704567,5704572,5704621,5704883,5705793,5705891,5706210,5706270,5706786,5707047,5707075,5707204,5707361,5707711,5707820,5707939,5708365,5708566,5708886,5709085,5709476,5709673,5710587,5710692,5711389,5711550,5712189,5712635,5712909,5712913,5713064,5713195,5713554,5714310,5715270,5715349,5715791,5716196,5716239,5717567,5723473,5724215,5724896,5725855,5726695,5727257,5727701,5729092,5730225,5731835,5734632,5734840,5735134,5735252,5735670,5735806,5737153,5737254,5737800,5739778,5740186,5740341,5740814,5741714,5742482,5742652,5743787,5746138,5746724,5747216,5748169,5750253,5752369,5752461,5752674,5752921,5753681,5753942,5754025,5754289,5754938,5756675,5757342,5757621,5757641,5758674,5759278,5760167,5760287,5760450,5760470,5761271,5762401,5763597,5763617,5763698,5763830,5770385,5771216,5771292,5773287,5773914,5775512,5780512,5781109,5781195,5781990,5787726,5788566,5791121,5792081,5796948,5798892,5798975,5799569,5799631,5799662,5799694,5799746,5799778,5799841,5799883,5799904,13702623,48966242,49372308,49375202,49381142,49398669,49421372,49451719,49472111,49491745,49499309,49501784,49512333,49518630,49622711,49629222,49642845,49643556,49644063,49658622,49661348,49769425,49822849,49823072,49833570,49842883,49894868,49904447,49924634,49926161,49943107,49972001,49974162,49979546,50109039,50167019,50179450,50221708,50224290,50266110,50273476,50293596,50297354,50335837,50360108,50361004,50387143,50389218,50439248,50498855,50503887,50515275,50520275,50529231,50549443,50550119,50562752,50578551,50587130,50602189,70971124,70973210,70979530,71008221,71035348,71035545,71036561,71038202,71061469,71079966,71085289,71113327,71117260,71123632,71185278,71197894,71239134,71254162,71274325,71289094,71291313,71316515,71330561,71346880,71357761,71376164,71386973,71387398,71387817,71401636,71429343,71451150,71473329,71477439,71478228,71489228,71505708,71520498,71528645,71530231,71531325,71531777,71544802,71554500,71560505,71569387,71573838,71574277,71582296,71608308,71612765,71614443,71625628,71628335,71628345,71628375,71628455,71628465,71631403,71632234,71638491,71640899,71652642,71657948,71659941,71661128,71667986,71671746,71675235,71679628,71679804,71684681,71689908,71704773,71710068,71710709,71722680,71732206,71733051,71748761,71798703,71810993,71829358,71832993,71833048,71835006,71835026,71853970,71868130,71868389,71897955,71898329,71902351,71924239,71927994,71958929,71960428,71966105,94327569,94339215,94353356,94354581,94356250,94359523,94363061,94364549,94374207,94375719,94399352,94409697,94429540,94434325,94434948,94437799,94442511,94444977,94461479,94478632,94482495,94490398,94492977,94493238,94507832,94514669,94516154,94516262,94537004,94542868,94543149,94554562,94568225,94573640,94586067,94592552,94605039,94617274,94628866,94661730,94692096,94716848,94733275,94776430,94802823,94819006,94819054,94819398,94819458,94839913],"crc":0,"cvi":[]},"docs-cclt":56}; _docs_flag_cek='M9xP1B2szkhl-_pNKDLeHQ\x3d\x3d'; if (window['DOCS_timing']) {DOCS_timing['ifdld']=new Date().getTime();}</script><script nonce="">DOCS_timing['sjl'] = performance.now();this._pubi=this._pubi||{};(function(_){var window=this;
_._F_toggles_initialize=function(a){("undefined"!==typeof globalThis?globalThis:"undefined"!==typeof self?self:this)._F_toggles=a||[]};(0,_._F_toggles_initialize)([]);
/*

 Copyright The Closure Library Authors.
 SPDX-License-Identifier: Apache-2.0
*/
var q=function(){return function(){}};var aa=function(a){return function(b){this[a]=b}};var ba=function(a){return function(){return a}};
var r,ca=function(a){var b=0;return function(){return b<a.length?{done:!1,value:a[b++]}:{done:!0}}},da=function(a){a=["object"==typeof globalThis&&globalThis,a,"object"==typeof window&&window,"object"==typeof self&&self,"object"==typeof global&&global];for(var b=0;b<a.length;++b){var c=a[b];if(c&&c.Math==Math)return c}throw Error("a");},u=function(a,b){if(b)a:{var c=ea;a=a.split(".");for(var d=0;d<a.length-1;d++){var e=a[d];if(!(e in c))break a;c=c[e]}a=a[a.length-1];d=c[a];b=b(d);b!=d&&null!=b&&
fa(c,a,{configurable:!0,writable:!0,value:b})}},ha=function(a){a={next:a};a[Symbol.iterator]=function(){return this};return a},v=function(a){var b="undefined"!=typeof Symbol&&Symbol.iterator&&a[Symbol.iterator];if(b)return b.call(a);if("number"==typeof a.length)return{next:ca(a)};throw Error("d`"+String(a));},ia=function(a){for(var b,c=[];!(b=a.next()).done;)c.push(b.value);return c},ja=function(a){return a instanceof Array?a:ia(v(a))},la=function(a,b){return Object.prototype.hasOwnProperty.call(a,
b)},w=function(a,b){a.prototype=ma(b.prototype);a.prototype.constructor=a;if(na)na(a,b);else for(var c in b)if("prototype"!=c)if(Object.defineProperties){var d=Object.getOwnPropertyDescriptor(b,c);d&&Object.defineProperty(a,c,d)}else a[c]=b[c];a.M=b.prototype},oa=function(){this.o=!1;this.i=null;this.u=void 0;this.h=1;this.j=this.m=0;this.v=this.g=null},pa=function(a){if(a.o)throw new TypeError("g");a.o=!0},qa=function(a,b){a.g={ua:b,xa:!0};a.h=a.m||a.j},ra=function(a,b,c){a.h=c;return{value:b}},
sa=function(a){a.m=0;var b=a.g.ua;a.g=null;return b},ta=function(a){var b=a.v.splice(0)[0];(b=a.g=a.g||b)?b.xa?a.h=a.m||a.j:void 0!=b.O&&a.j<b.O?(a.h=b.O,a.g=null):a.h=a.j:a.h=0},ua=function(a){this.g=new oa;this.h=a},xa=function(a,b){pa(a.g);var c=a.g.i;if(c)return va(a,"return"in c?c["return"]:function(d){return{value:d,done:!0}},b,a.g.return);a.g.return(b);return wa(a)},va=function(a,b,c,d){try{var e=b.call(a.g.i,c);if(!(e instanceof Object))throw new TypeError("f`"+e);if(!e.done)return a.g.o=
!1,e;var g=e.value}catch(f){return a.g.i=null,qa(a.g,f),wa(a)}a.g.i=null;d.call(a.g,g);return wa(a)},wa=function(a){for(;a.g.h;)try{var b=a.h(a.g);if(b)return a.g.o=!1,{value:b.value,done:!1}}catch(c){a.g.u=void 0,qa(a.g,c)}a.g.o=!1;if(a.g.g){b=a.g.g;a.g.g=null;if(b.xa)throw b.ua;return{value:b.return,done:!0}}return{value:void 0,done:!0}},ya=function(a){this.next=function(b){pa(a.g);a.g.i?b=va(a,a.g.i.next,b,a.g.s):(a.g.s(b),b=wa(a));return b};this.throw=function(b){pa(a.g);a.g.i?b=va(a,a.g.i["throw"],
b,a.g.s):(qa(a.g,b),b=wa(a));return b};this.return=function(b){return xa(a,b)};this[Symbol.iterator]=function(){return this}},za=function(a){function b(d){return a.next(d)}function c(d){return a.throw(d)}return new Promise(function(d,e){function g(f){f.done?d(f.value):Promise.resolve(f.value).then(b,c).then(g,e)}g(a.next())})},Ba=function(){for(var a=Number(this),b=[],c=a;c<arguments.length;c++)b[c-a]=arguments[c];return b},Ca=function(a,b,c){if(null==a)throw new TypeError("k`"+c);if(b instanceof
RegExp)throw new TypeError("l`"+c);return a+""},Da=function(a,b){a instanceof String&&(a+="");var c=0,d=!1,e={next:function(){if(!d&&c<a.length){var g=c++;return{value:b(g,a[g]),done:!1}}d=!0;return{done:!0,value:void 0}}};e[Symbol.iterator]=function(){return e};return e},Ea=function(a){a:{var b=["WIZ_global_data","oxN3nb"];for(var c=x,d=0;d<b.length;d++)if(c=c[b[d]],null==c){b=null;break a}b=c}a=b&&b[a];return null!=a?a:!1},Fa=function(a){var b=typeof a;return"object"==b&&null!=a||"function"==b},
Ia=function(a){return Object.prototype.hasOwnProperty.call(a,Ga)&&a[Ga]||(a[Ga]=++Ha)},Ja=function(a,b,c){return a.call.apply(a.bind,arguments)},Ka=function(a,b,c){if(!a)throw Error();if(2<arguments.length){var d=Array.prototype.slice.call(arguments,2);return function(){var e=Array.prototype.slice.call(arguments);Array.prototype.unshift.apply(e,d);return a.apply(b,e)}}return function(){return a.apply(b,arguments)}},La=function(a,b,c){La=Function.prototype.bind&&-1!=Function.prototype.bind.toString().indexOf("native code")?
Ja:Ka;return La.apply(null,arguments)},Ma=function(a,b){var c=Array.prototype.slice.call(arguments,1);return function(){var d=c.slice();d.push.apply(d,arguments);return a.apply(this,d)}},Na=function(a,b){x[a]=b},Oa=function(a,b){function c(){}c.prototype=b.prototype;a.M=b.prototype;a.prototype=new c;a.prototype.constructor=a;a.eb=function(d,e,g){for(var f=Array(arguments.length-2),k=2;k<arguments.length;k++)f[k-2]=arguments[k];return b.prototype[e].apply(d,f)}},Pa=function(a,b){if(Error.captureStackTrace)Error.captureStackTrace(this,
Pa);else{var c=Error().stack;c&&(this.stack=c)}a&&(this.message=String(a));void 0!==b&&(this.cause=b)},Qa=function(a){x.setTimeout(function(){throw a;},0)},y=function(){var a=x.navigator;return a&&(a=a.userAgent)?a:""},Sa=function(a){return Ra?A?A.brands.some(function(b){return(b=b.brand)&&-1!=b.indexOf(a)}):!1:!1},B=function(a){return-1!=y().indexOf(a)},Ta=function(){return Ra?!!A&&0<A.brands.length:!1},Ua=function(){return Ta()?!1:B("Opera")},Va=function(){return Ta()?Sa("Microsoft Edge"):B("Edg/")},
Wa=function(){return B("Firefox")||B("FxiOS")},Xa=function(){return Ta()?Sa("Chromium"):(B("Chrome")||B("CriOS"))&&!(Ta()?0:B("Edge"))||B("Silk")},Ya=function(){return Ra?!!A&&!!A.platform:!1},Za=function(){return B("iPhone")&&!B("iPod")&&!B("iPad")},$a=function(){return Za()||B("iPad")||B("iPod")},ab=function(){return Ya()?"macOS"===A.platform:B("Macintosh")},bb=function(){return Ya()?"Chrome OS"===A.platform:B("CrOS")},cb=function(a){cb[" "](a);return a},db=function(a,b,c,d){d=d?d(b):b;return Object.prototype.hasOwnProperty.call(a,
d)?a[d]:a[d]=c(b)},eb=function(){return"function"===typeof BigInt},hb=function(a){var b=0>a;a=Math.abs(a);var c=a>>>0;a=Math.floor((a-c)/4294967296);b&&(c=v(fb(c,a)),b=c.next().value,a=c.next().value,c=b);D=c>>>0;gb=a>>>0},jb=function(a,b){b>>>=0;a>>>=0;if(2097151>=b)var c=""+(4294967296*b+a);else eb()?c=""+(BigInt(b)<<BigInt(32)|BigInt(a)):(c=(a>>>24|b<<8)&16777215,b=b>>16&65535,a=(a&16777215)+6777216*c+6710656*b,c+=8147497*b,b*=2,1E7<=a&&(c+=Math.floor(a/1E7),a%=1E7),1E7<=c&&(b+=Math.floor(c/1E7),
c%=1E7),c=b+ib(c)+ib(a));return c},ib=function(a){a=String(a);return"0000000".slice(a.length)+a},kb=function(){var a=D,b=gb;b&2147483648?eb()?a=""+(BigInt(b|0)<<BigInt(32)|BigInt(a>>>0)):(b=v(fb(a,b)),a=b.next().value,b=b.next().value,a="-"+jb(a,b)):a=jb(a,b);return a},fb=function(a,b){b=~b;a?a=~a+1:b+=1;return[a,b]},lb=function(a){return Array.prototype.slice.call(a)},mb=function(a){return"function"===typeof Symbol&&"symbol"===typeof Symbol()?Symbol():a},E=function(a,b,c){return c?a|b:a&~b},nb=function(a,
b){G(b,(a|0)&-14591)},ob=function(a,b){G(b,(a|34)&-14557)},pb=function(a){a=a>>14&1023;return 0===a?536870912:a},rb=function(a){return!(!a||"object"!==typeof a||a.g!==qb)},sb=function(a){return null!==a&&"object"===typeof a&&!Array.isArray(a)&&a.constructor===Object},tb=function(a,b,c){if(!Array.isArray(a)||a.length)return!1;var d=H(a);if(d&1)return!0;if(!(b&&(Array.isArray(b)?b.includes(c):b.has(c))))return!1;G(a,d|1);return!0},ub=function(a){if(a&2)throw Error();},vb=function(a,b){a.__closure__error__context__984382||
(a.__closure__error__context__984382={});a.__closure__error__context__984382.severity=b},wb=function(a){a=Error(a);vb(a,"warning");return a},xb=function(a){return a.displayName||a.name||"unknown type name"},zb=function(a){var b=typeof a;return"number"===b?Number.isFinite(a):"string"!==b?!1:yb.test(a)},Ab=function(a){if("number"!==typeof a)throw wb("int32");if(!Number.isFinite(a))throw wb("int32");return a|0},Bb=function(a){if(null==a)return a;if("string"===typeof a){if(!a)return;a=+a}if("number"===
typeof a)return Number.isFinite(a)?a|0:void 0},Fb=function(a){if(null!=a){var b=!!b;if(!zb(a))throw wb("int64");a="string"===typeof a?Cb(a):b?Db(a):Eb(a)}return a},Gb=function(a){return"-"===a[0]?20>a.length?!0:20===a.length&&-922337<Number(a.substring(0,7)):19>a.length?!0:19===a.length&&922337>Number(a.substring(0,6))},Eb=function(a){zb(a);a=Math.trunc(a);if(!Number.isSafeInteger(a)){hb(a);var b=D,c=gb;if(a=c&2147483648)b=~b+1>>>0,c=~c>>>0,0==b&&(c=c+1>>>0);b=4294967296*c+(b>>>0);a=a?-b:b}return a},
Db=function(a){zb(a);a=Math.trunc(a);if(Number.isSafeInteger(a))a=String(a);else{var b=String(a);Gb(b)?a=b:(hb(a),a=kb())}return a},Cb=function(a){zb(a);var b=Math.trunc(Number(a));if(Number.isSafeInteger(b))return String(b);b=a.indexOf(".");-1!==b&&(a=a.substring(0,b));if(!Gb(a)){if(16>a.length)hb(Number(a));else if(eb())a=BigInt(a),D=Number(a&BigInt(4294967295))>>>0,gb=Number(a>>BigInt(32)&BigInt(4294967295));else{b=+("-"===a[0]);gb=D=0;for(var c=a.length,d=b,e=(c-b)%6+b;e<=c;d=e,e+=6)d=Number(a.slice(d,
e)),gb*=1E6,D=1E6*D+d,4294967296<=D&&(gb+=Math.trunc(D/4294967296),gb>>>=0,D>>>=0);b&&(b=v(fb(D,gb)),a=b.next().value,b=b.next().value,D=a,gb=b)}a=kb()}return a},Hb=function(a,b){if(!(a instanceof b))throw Error("s`"+xb(b)+"`"+(a&&xb(a.constructor)));},Jb=function(a,b,c){if(null!=a&&"object"===typeof a&&a.oa===Ib)return a;if(Array.isArray(a)){var d=H(a),e=d;0===e&&(e|=c&32);e|=c&2;e!==d&&G(a,e);return new b(a)}},Lb=function(a,b){Kb=b;a=new a(b);Kb=void 0;return a},I=function(a,b,c){null==a&&(a=Kb);
Kb=void 0;if(null==a){var d=96;c?(a=[c],d|=512):a=[];b&&(d=d&-16760833|(b&1023)<<14)}else{if(!Array.isArray(a))throw Error("t");d=H(a);if(d&2048)throw Error("u");if(d&64)return a;d|=64;if(c&&(d|=512,c!==a[0]))throw Error("v");a:{c=a;var e=c.length;if(e){var g=e-1;if(sb(c[g])){d|=256;b=g-(+!!(d&512)-1);if(1024<=b)throw Error("w");d=d&-16760833|(b&1023)<<14;break a}}if(b){b=Math.max(b,e-(+!!(d&512)-1));if(1024<b)throw Error("x");d=d&-16760833|(b&1023)<<14}}}G(a,d);return a},Nb=function(a,b){return Mb(b)},
Mb=function(a){switch(typeof a){case "number":return isFinite(a)?a:String(a);case "boolean":return a?1:0;case "object":if(a)if(Array.isArray(a)){if(tb(a,void 0,0))return}else if(Ob&&null!=a&&a instanceof Uint8Array){if(Qb){for(var b="",c=0,d=a.length-10240;c<d;)b+=String.fromCharCode.apply(null,a.subarray(c,c+=10240));b+=String.fromCharCode.apply(null,c?a.subarray(c):a);a=btoa(b)}else{void 0===b&&(b=0);if(!Rb){Rb={};c="ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789".split("");d=["+/=",
"+/","-_=","-_.","-_"];for(var e=0;5>e;e++){var g=c.concat(d[e].split(""));Sb[e]=g;for(var f=0;f<g.length;f++){var k=g[f];void 0===Rb[k]&&(Rb[k]=f)}}}b=Sb[b];c=Array(Math.floor(a.length/3));d=b[64]||"";for(e=g=0;g<a.length-2;g+=3){var h=a[g],l=a[g+1];k=a[g+2];f=b[h>>2];h=b[(h&3)<<4|l>>4];l=b[(l&15)<<2|k>>6];k=b[k&63];c[e++]=f+h+l+k}f=0;k=d;switch(a.length-g){case 2:f=a[g+1],k=b[(f&15)<<2]||d;case 1:a=a[g],c[e]=b[a>>2]+b[(a&3)<<4|f>>4]+k+d}a=c.join("")}return a}}return a},Tb=function(a,b,c){a=lb(a);
var d=a.length,e=b&256?a[d-1]:void 0;d+=e?-1:0;for(b=b&512?1:0;b<d;b++)a[b]=c(a[b]);if(e){b=a[b]={};for(var g in e)b[g]=c(e[g])}return a},Vb=function(a,b,c,d,e){if(null!=a){if(Array.isArray(a))a=tb(a,void 0,0)?void 0:e&&H(a)&2?a:Ub(a,b,c,void 0!==d,e);else if(sb(a)){var g={},f;for(f in a)g[f]=Vb(a[f],b,c,d,e);a=g}else a=b(a,d);return a}},Ub=function(a,b,c,d,e){var g=d||c?H(a):0;d=d?!!(g&32):void 0;a=lb(a);for(var f=0;f<a.length;f++)a[f]=Vb(a[f],b,c,d,e);c&&c(g,a);return a},Wb=function(a){return a.oa===
Ib?a.toJSON():Mb(a)},Xb=function(a,b,c){c=void 0===c?ob:c;if(null!=a){if(Ob&&a instanceof Uint8Array)return b?a:new Uint8Array(a);if(Array.isArray(a)){var d=H(a);if(d&2)return a;b&&(b=0===d||!!(d&32)&&!(d&64||!(d&16)));return b?G(a,(d|34)&-12293):Ub(a,Xb,d&4?ob:c,!0,!0)}a.oa===Ib&&(c=a.l,d=J(c),a=d&2?a:Lb(a.constructor,Yb(c,d,!0)));return a}},Yb=function(a,b,c){var d=c||b&2?ob:nb,e=!!(b&32);a=Tb(a,b,function(g){return Xb(g,e,d)});Zb(a,32|(c?2:0));return a},$b=function(a){var b=a.l,c=J(b);return c&
2?Lb(a.constructor,Yb(b,c,!1)):a},bc=function(a,b){a=a.l;return ac(a,J(a),b)},cc=function(a,b,c,d){b=d+(+!!(b&512)-1);if(!(0>b||b>=a.length||b>=c))return a[b]},ac=function(a,b,c,d){if(-1===c)return null;var e=pb(b);if(c>=e){if(b&256)return a[a.length-1][c]}else{var g=a.length;if(d&&b&256&&(d=a[g-1][c],null!=d)){if(cc(a,b,e,c)&&null!=dc){var f;a=null!=(f=ec)?f:ec={};f=a[dc]||0;4<=f||(a[dc]=f+1,f=Error(),vb(f,"incident"),Qa(f))}return d}return cc(a,b,e,c)}},gc=function(a,b,c){var d=a.l,e=J(d);ub(e);
fc(d,e,b,c);return a},fc=function(a,b,c,d,e){var g=pb(b);if(c>=g||e){var f=b;if(b&256)e=a[a.length-1];else{if(null==d)return f;e=a[g+(+!!(b&512)-1)]={};f|=256}e[c]=d;c<g&&(a[c+(+!!(b&512)-1)]=void 0);f!==b&&G(a,f);return f}a[c+(+!!(b&512)-1)]=d;b&256&&(a=a[a.length-1],c in a&&delete a[c]);return b},K=function(a,b,c){return void 0!==hc(a,b,c,!1)},ic=function(a,b){a=bc(a,b);return null==a||"boolean"===typeof a?a:"number"===typeof a?!!a:void 0},jc=function(a){return!!(2&a)&&!!(4&a)||!!(2048&a)},mc=function(a,
b,c){var d=a.l,e=J(d);ub(e);if(null==c)return fc(d,e,b),a;if(!Array.isArray(c))throw wb();var g=H(c),f=g,k=!!(2&g)||Object.isFrozen(c),h=!k&&!1;if(!(4&g))for(g=21,k&&(c=lb(c),f=0,g=kc(g,e),g=lc(g,e,!0)),k=0;k<c.length;k++)c[k]=Ab(c[k]);h&&(c=lb(c),f=0,g=kc(g,e),g=lc(g,e,!0));g!==f&&G(c,g);fc(d,e,b,c);return a},hc=function(a,b,c,d){a=a.l;var e=J(a),g=ac(a,e,c,d);b=Jb(g,b,e);b!==g&&null!=b&&fc(a,e,c,b,d);return b},L=function(a,b,c,d){d=void 0===d?!1:d;b=hc(a,b,c,d);if(null==b)return b;a=a.l;var e=J(a);
if(!(e&2)){var g=$b(b);g!==b&&(b=g,fc(a,e,c,b,d))}return b},oc=function(a,b,c){a=a.l;var d=J(a),e=d,g=!(2&d),f=!!(2&e),k=f?1:2;d=1===k;k=2===k;g&&(g=!f);f=ac(a,e,c);f=Array.isArray(f)?f:nc;var h=H(f),l=!!(4&h);if(!l){var m=h;0===m&&(m=kc(m,e));m=E(m,1,!0);h=f;var n=e,t=!!(2&m);t&&(n=E(n,2,!0));for(var p=!t,z=!0,F=0,C=0;F<h.length;F++){var Aa=Jb(h[F],b,n);if(Aa instanceof b){if(!t){var ka=!!(H(Aa.l)&2);p&&(p=!ka);z&&(z=ka)}h[C++]=Aa}}C<F&&(h.length=C);m=E(m,4,!0);m=E(m,16,z);m=E(m,8,p);G(h,m);t&&Object.freeze(h);
h=m}b=!!(8&h)||d&&!f.length;if(g&&!b){jc(h)&&(f=lb(f),h=kc(h,e),e=fc(a,e,c,f));b=f;g=h;for(h=0;h<b.length;h++)m=b[h],n=$b(m),m!==n&&(b[h]=n);g=E(g,8,!0);g=E(g,16,!b.length);G(b,g);h=g}jc(h)||(b=h,d?(g=!!(32&h),g||(f=lb(f),b=0,e=fc(a,e,c,f)),h=E(h,!f.length||16&h&&(!l||g)?2:2048,!0)):h=lc(h,e,!1),h!==b&&G(f,h),d&&Object.freeze(f));k&&jc(h)&&(f=lb(f),h=kc(h,e),h=lc(h,e,!1),G(f,h),fc(a,e,c,f));return f},M=function(a,b,c,d){null!=d?Hb(d,b):d=void 0;return gc(a,c,d)},pc=function(a,b,c,d){var e=a.l,g=J(e);
ub(g);if(null==d)return fc(e,g,c),a;if(!Array.isArray(d))throw wb();for(var f=H(d),k=f,h=!!(2&f)||!!(2048&f),l=h||Object.isFrozen(d),m=!l&&!1,n=!0,t=!0,p=0;p<d.length;p++){var z=d[p];Hb(z,b);h||(z=!!(H(z.l)&2),n&&(n=!z),t&&(t=z))}h||(f=E(f,5,!0),f=E(f,8,n),f=E(f,16,t));if(m||l&&f!==k)d=lb(d),k=0,f=kc(f,g),f=lc(f,g,!0);f!==k&&G(d,f);fc(e,g,c,d);return a},kc=function(a,b){a=E(a,2,!!(2&b));a=E(a,32,!0);return a=E(a,2048,!1)},lc=function(a,b,c){32&b&&c||(a=E(a,32,!1));return a},N=function(a,b){a=bc(a,
b);var c;null==a?c=a:zb(a)?"number"===typeof a?c=Eb(a):c=Cb(a):c=void 0;return c},qc=function(a){a=bc(a,1);var b=void 0===b?!1:b;b=null==a?a:zb(a)?"string"===typeof a?Cb(a):b?Db(a):Eb(a):void 0;return b},rc=function(a,b){a=bc(a,b);return null==a||"string"===typeof a?a:void 0},sc=function(a,b){a=bc(a,b);return null==a?a:Number.isFinite(a)?a|0:void 0},tc=function(a,b){return null!=a?a:b},O=function(a,b,c){if(null!=c&&"boolean"!==typeof c)throw a=typeof c,Error("r`"+("object"!=a?a:c?Array.isArray(c)?
"array":a:"null")+"`"+c);return gc(a,b,c)},P=function(a,b,c){return gc(a,b,null==c?c:Ab(c))},Q=function(a,b,c){return gc(a,b,Fb(c))},R=function(a,b,c){if(null!=c&&"string"!==typeof c)throw Error();return gc(a,b,c)},S=function(a,b,c){if(null!=c){if(!Number.isFinite(c))throw wb("enum");c|=0}return gc(a,b,c)},T=function(a,b,c){this.l=I(a,b,c)},vc=function(a){uc=!0;try{return JSON.stringify(a.toJSON(),Nb)}finally{uc=!1}},wc=function(a){var b=a.l;return Lb(a.constructor,Yb(b,J(b),!1))},yc=function(a,b,
c){var d=xc?void 0:a.constructor.D;var e=J(c?a.l:b);a=b.length;if(!a)return b;var g;if(sb(c=b[a-1])){a:{var f=c;var k={},h=!1,l;for(l in f){var m=f[l];if(Array.isArray(m)){var n=m;if(tb(m,d,+l)||rb(m)&&0===m.size)m=null;m!=n&&(h=!0)}null!=m?k[l]=m:h=!0}if(h){for(var t in k){f=k;break a}f=null}}f!=c&&(g=!0);a--}for(l=+!!(e&512)-1;0<a;a--){t=a-1;c=b[t];t-=l;if(!(null==c||tb(c,d,t)||rb(c)&&0===c.size))break;var p=!0}if(!g&&!p)return b;b=Array.prototype.slice.call(b,0,a);f&&b.push(f);return b},zc=function(a){if(!a)return"";
if(/^about:(?:blank|srcdoc)$/.test(a))return window.origin||"";0===a.indexOf("blob:")&&(a=a.substring(5));a=a.split("#")[0].split("?")[0];a=a.toLowerCase();0==a.indexOf("//")&&(a=window.location.protocol+a);/^[\w\-]*:\/\//.test(a)||(a=window.location.href);var b=a.substring(a.indexOf("://")+3),c=b.indexOf("/");-1!=c&&(b=b.substring(0,c));c=a.substring(0,a.indexOf("://"));if(!c)throw Error("A`"+a);if("http"!==c&&"https"!==c&&"chrome-extension"!==c&&"moz-extension"!==c&&"file"!==c&&"android-app"!==
c&&"chrome-search"!==c&&"chrome-untrusted"!==c&&"chrome"!==c&&"app"!==c&&"devtools"!==c)throw Error("B`"+c);a="";var d=b.indexOf(":");if(-1!=d){var e=b.substring(d+1);b=b.substring(0,d);if("http"===c&&"80"!==e||"https"===c&&"443"!==e)a=":"+e}return c+"://"+b+a},Bc=function(a){return db(a.prototype,"$$generatedClassName",function(){return"Class$obf_"+{valueOf:function(){return++Ac}}})},Cc=q(),Dc=q(),Ec=function(a){a.g instanceof Error&&(Error.captureStackTrace?Error.captureStackTrace(a.g):a.g.stack=
Error().stack)},Fc=q(),Gc=q(),Hc=function(a,b){this.h=a|0;this.g=b|0},Ic=function(a){return 4294967296*a.g+(a.h>>>0)},Jc=function(a){return 0==a.h&&0==a.g},Kc=function(a,b){return a.g==b.g?a.h==b.h?0:a.h>>>0>b.h>>>0?1:-1:a.g>b.g?1:-1},Lc=function(a){var b=~a.h+1|0;return U(b,~a.g+!b|0)},Mc=function(a,b){if(Jc(a))return a;if(Jc(b))return b;var c=a.g>>>16,d=a.g&65535,e=a.h>>>16;a=a.h&65535;var g=b.g>>>16,f=b.g&65535,k=b.h>>>16;b=b.h&65535;var h=a*b;var l=(h>>>16)+e*b;var m=l>>>16;l=(l&65535)+a*k;m+=
l>>>16;m+=d*b;var n=m>>>16;m=(m&65535)+e*k;n+=m>>>16;m=(m&65535)+a*f;n=n+(m>>>16)+(c*b+d*k+e*f+a*g)&65535;return U((l&65535)<<16|h&65535,n<<16|m&65535)},Qc=function(a,b){if(Jc(b))throw Error("D");if(0>a.g){if(a.I(Nc)){if(b.I(Oc)||b.I(Pc))return Nc;if(b.I(Nc))return Oc;var c=a.g;c=U(a.h>>>1|c<<31,c>>1);c=Qc(c,b);var d=c.h;c=U(d<<1,c.g<<1|d>>>31);if(c.I(Rc))return 0>b.g?Oc:Pc;a=a.add(Lc(Mc(b,c)));return c.add(Qc(a,b))}return 0>b.g?Qc(Lc(a),Lc(b)):Lc(Qc(Lc(a),b))}if(Jc(a))return Rc;if(0>b.g)return b.I(Nc)?
Rc:Lc(Qc(a,Lc(b)));for(d=Rc;0<=Kc(a,b);){c=Math.max(1,Math.floor(Ic(a)/Ic(b)));var e=Math.ceil(Math.log(c)/Math.LN2);e=48>=e?1:Math.pow(2,e-48);for(var g=Sc(c),f=Mc(g,b);0>f.g||0<Kc(f,a);)c-=e,g=Sc(c),f=Mc(g,b);Jc(g)&&(g=Oc);d=d.add(g);a=a.add(Lc(f))}return d},Sc=function(a){return 0<a?0x7fffffffffffffff<=a?Tc:new Hc(a,a/4294967296):0>a?-0x7fffffffffffffff>=a?Nc:Lc(new Hc(-a,-a/4294967296)):Rc},U=function(a,b){return new Hc(a,b)},Uc=q(),Vc=q(),Wc=q(),Xc=q(),V=function(a){return null==a?"null":a.toString()},
Yc=function(a,b){this.g=a;this.i=b},Zc=function(a){if(0!=a.i){var b="L"+V(Bc(a.g))+";";a=a.i;for(var c="",d=0;d<a;d=d+1|0)c=V(c)+"[";return V(c)+V(b)}return Bc(a.g)},$c=function(a){return db(a.prototype,"$$class/0",function(){return new Yc(a,0)})},ad=function(a,b){if(a instanceof Object)try{a.za=b,Object.defineProperties(a,{cause:{get:function(){return b.j&&b.j.g}}})}catch(c){}},cd=function(){var a=a?a:function(c){return Math.max(Math.min(Math.floor(Math.random()*c),2147483647),-2147483648)|0};var b=
(a(2147483647)>>>0).toString(16);b=V(bd("0",Math.max(0,8-b.length|0)))+V(b);a=(a(2147483647)>>>0).toString(16);return V(a)+V(b)},ed=function(){this.g=new dd},fd=function(a){this.l=I(a)},dd=function(a){this.l=I(a)},gd=function(a){this.l=I(a)},hd=function(a){this.l=I(a)},id=function(a){this.l=I(a)},jd=function(a){this.l=I(a)},md=function(){var a;if(!kd){var b=new ld(null);kd=function(){return b}}return a=kd,a()},nd=q(),ld=function(a){this.g=new nd;if(a)for(var b in a){var c=b,d=a[b],e=this.g.get();
e[c]=null!=d?d:null}},od=function(a,b){a=a.g.get();return b in a},pd=function(a,b){a=a.get(b);return"string"==typeof a?"true"==a||"1"==a:!!a},rd=function(a,b){if(!od(a,b)||null==a.get(b))return NaN;try{var c=V(a.get(b));qd||(qd=RegExp("^\\s*[+-]?(NaN|Infinity|((\\d+\\.?\\d*)|(\\.\\d+))([eE][+-]?\\d+)?[dDfF]?)\\s*$"));if(!qd.test(c)){var d=new Xc;d.i="E`"+V(c);Ec(d);var e=Error(d);d.g=e;ad(e,d);throw d.g;}return parseFloat(c)}catch(f){a:{a=f;if(null!=a&&(b=a.za)){var g=b;break a}a instanceof TypeError?
(b=new Vc,Ec(b),c=new TypeError(b),b.g=c,ad(c,b)):(b=new Uc,Ec(b),c=Error(b),b.g=c,ad(c,b));b.i=null==a?"null":a.toString();b.g=a;ad(a,b);g=b}if(g instanceof Xc)return NaN;throw g.g;}},sd=function(a,b){if(!od(a,b))return"";a=a.get(b);if(null==a)var c="";else{if(b="number"===typeof a)b=Sc(a).I(Sc(a));b?c=""+Sc(a):c=V(a)}return c},td=function(){function a(){e[0]=1732584193;e[1]=4023233417;e[2]=2562383102;e[3]=271733878;e[4]=3285377520;m=l=0}function b(n){for(var t=f,p=0;64>p;p+=4)t[p/4]=n[p]<<24|n[p+
1]<<16|n[p+2]<<8|n[p+3];for(p=16;80>p;p++)n=t[p-3]^t[p-8]^t[p-14]^t[p-16],t[p]=(n<<1|n>>>31)&4294967295;n=e[0];var z=e[1],F=e[2],C=e[3],Aa=e[4];for(p=0;80>p;p++){if(40>p)if(20>p){var ka=C^z&(F^C);var Pb=1518500249}else ka=z^F^C,Pb=1859775393;else 60>p?(ka=z&F|C&(z|F),Pb=2400959708):(ka=z^F^C,Pb=3395469782);ka=((n<<5|n>>>27)&4294967295)+ka+Aa+Pb+t[p]&4294967295;Aa=C;C=F;F=(z<<30|z>>>2)&4294967295;z=n;n=ka}e[0]=e[0]+n&4294967295;e[1]=e[1]+z&4294967295;e[2]=e[2]+F&4294967295;e[3]=e[3]+C&4294967295;e[4]=
e[4]+Aa&4294967295}function c(n,t){if("string"===typeof n){n=unescape(encodeURIComponent(n));for(var p=[],z=0,F=n.length;z<F;++z)p.push(n.charCodeAt(z));n=p}t||(t=n.length);p=0;if(0==l)for(;p+64<t;)b(n.slice(p,p+64)),p+=64,m+=64;for(;p<t;)if(g[l++]=n[p++],m++,64==l)for(l=0,b(g);p+64<t;)b(n.slice(p,p+64)),p+=64,m+=64}function d(){var n=[],t=8*m;56>l?c(k,56-l):c(k,64-(l-56));for(var p=63;56<=p;p--)g[p]=t&255,t>>>=8;b(g);for(p=t=0;5>p;p++)for(var z=24;0<=z;z-=8)n[t++]=e[p]>>z&255;return n}for(var e=
[],g=[],f=[],k=[128],h=1;64>h;++h)k[h]=0;var l,m;a();return{reset:a,update:c,digest:d,Ba:function(){for(var n=d(),t="",p=0;p<n.length;p++)t+="0123456789ABCDEF".charAt(Math.floor(n[p]/16))+"0123456789ABCDEF".charAt(n[p]%16);return t}}},vd=function(a,b,c){var d=String(x.location.href);return d&&a&&b?[b,ud(zc(d),a,c||null)].join(" "):null},ud=function(a,b,c){var d=[],e=[];if(1==(Array.isArray(c)?2:1))return e=[b,a],wd(d,function(k){e.push(k)}),xd(e.join(" "));var g=[],f=[];wd(c,function(k){f.push(k.key);
g.push(k.value)});c=Math.floor((new Date).getTime()/1E3);e=0==g.length?[c,b,a]:[g.join(":"),c,b,a];wd(d,function(k){e.push(k)});a=xd(e.join(" "));a=[c,a];0==f.length||a.push(f.join(""));return a.join("_")},xd=function(a){var b=td();b.update(a);return b.Ba().toLowerCase()},yd=function(a,b,c){for(var d in a)b.call(c,a[d],d,a)},Ad=function(a,b){for(var c,d,e=1;e<arguments.length;e++){d=arguments[e];for(c in d)a[c]=d[c];for(var g=0;g<zd.length;g++)c=zd[g],Object.prototype.hasOwnProperty.call(d,c)&&(a[c]=
d[c])}},Bd=function(a){return a.replace(RegExp("(^|[\\s]+)([a-z])","g"),function(b,c,d){return c+d.toUpperCase()})},Cd=function(){this.g=document||{cookie:""}},Ed=function(a){return!!Dd.FPA_SAMESITE_PHASE2_MOD||!(void 0===a||!a)},Fd=function(a,b,c,d){(a=x[a])||"undefined"===typeof document||(a=(new Cd).get(b));return a?vd(a,c,d):null},Gd=function(a,b){b=void 0===b?!1:b;var c=zc(String(x.location.href)),d=[];var e=b;e=void 0===e?!1:e;var g=x.__SAPISID||x.__APISID||x.__3PSAPISID||x.__OVERRIDE_SID;Ed(e)&&
(g=g||x.__1PSAPISID);if(g)e=!0;else{if("undefined"!==typeof document){var f=new Cd;g=f.get("SAPISID")||f.get("APISID")||f.get("__Secure-3PAPISID")||f.get("SID")||f.get("OSID");Ed(e)&&(g=g||f.get("__Secure-1PAPISID"))}e=!!g}e&&(e=(c=0==c.indexOf("https:")||0==c.indexOf("chrome-extension:")||0==c.indexOf("moz-extension:"))?x.__SAPISID:x.__APISID,e||"undefined"===typeof document||(e=new Cd,e=e.get(c?"SAPISID":"APISID")||e.get("__Secure-3PAPISID")),(e=e?vd(e,c?"SAPISIDHASH":"APISIDHASH",a):null)&&d.push(e),
c&&Ed(b)&&((b=Fd("__1PSAPISID","__Secure-1PAPISID","SAPISID1PHASH",a))&&d.push(b),(a=Fd("__3PSAPISID","__Secure-3PAPISID","SAPISID3PHASH",a))&&d.push(a)));return 0==d.length?null:d.join(" ")},Hd=function(a){a&&"function"==typeof a.ta&&a.ta()},W=function(){this.u=this.u;this.m=this.m},Id=function(a,b){b=Ma(Hd,b);a.u?b():(a.m||(a.m=[]),a.m.push(b))},Jd=function(a,b,c,d,e,g,f,k,h){this.o=a;this.g=b;this.s=c;this.v=d;this.h=e;this.i=g;this.j=f;this.m=k;this.u=h},Ld=function(){var a=md(),b=a.get("ilcm");
if(null==b)return null;var c=b.eui,d=b.je,e=b.sstu;if(Kd)var g=Kd;else{g=md();var f=g.get("ilcm");null==f?g=null:Kd=g=pd(g,"icso")?cd():f.si}f=b.ei;a=a.get("buildLabel");return new Jd(c,d,e,g,f,a,b.crc||0,b.cvi||[],b.gsc||null)},Md=function(a){this.l=I(a)},Nd=function(a){this.l=I(a)},Od=function(a){this.l=I(a)},Pd=function(a){this.l=I(a)},Qd=function(a,b,c){W.call(this);this.s=a;this.h="number"===typeof c?c:null;this.o=(a=Ld())?a.g:0;this.i=a?a.h:[];this.g=null;this.j=b},Rd=function(a){this.l=I(a)},
Sd=function(a){this.l=I(a)},Td=function(a){this.l=I(a,4)},Ud=function(a){this.l=I(a,35)},Vd=function(a,b){return R(a,8,b)},Wd=aa("h"),Xd=function(a){Pa.call(this);this.g=a},Zd=function(a){this.j=a;this.g=this.i=this.h=0;for(a=Yd;3E4>a;)a*=2;this.m=a},$d=function(a,b){if(pd(a.j,"docs-irbfes"))if(0!==a.g&&2!==b)if(1===b)b=4>a.h?Yd:3E4>a.g?2*a.g:a.m;else if(3===b)b=18E4>a.g?2*a.g:a.g;else throw Error("L");else b=Yd;else{var c=2!=b&&!(4>a.h);b=Yd;c&&0!=a.g&&(b=3E4>a.g?2*a.g:a.g)}a.g=b;return Math.max(0,
b-(Date.now()-a.i))},ae=function(a,b){this.i=a;this.j=b;this.h=0;this.g=null},be=function(a,b){a.j(b);100>a.h&&(a.h++,b.next=a.g,a.g=b)},ce=q(),de=function(){var a=document;var b="IFRAME";"application/xhtml+xml"===a.contentType&&(b=b.toLowerCase());return a.createElement(b)},ee=function(){this.g=x.document||document},fe=function(){var a=x.MessageChannel;"undefined"===typeof a&&"undefined"!==typeof window&&window.postMessage&&window.addEventListener&&!B("Presto")&&(a=function(){var e=de();e.style.display=
"none";document.documentElement.appendChild(e);var g=e.contentWindow;e=g.document;e.open();e.close();var f="callImmediate"+Math.random(),k="file:"==g.location.protocol?"*":g.location.protocol+"//"+g.location.host;e=La(function(h){if(("*"==k||h.origin==k)&&h.data==f)this.port1.onmessage()},this);g.addEventListener("message",e,!1);this.port1={};this.port2={postMessage:function(){g.postMessage(f,k)}}});if("undefined"!==typeof a){var b=new a,c={},d=c;b.port1.onmessage=function(){if(void 0!==c.next){c=
c.next;var e=c.ra;c.ra=null;e()}};return function(e){d.next={ra:e};d=d.next;b.port2.postMessage(0)}}return function(e){x.setTimeout(e,0)}},ge=function(){this.h=this.g=null},ie=function(){var a=he,b=null;a.g&&(b=a.g,a.g=a.g.next,a.g||(a.h=null),b.next=null);return b},je=function(){this.next=this.g=this.h=null},ne=function(a,b){ke||le();me||(ke(),me=!0);he.add(a,b)},le=function(){if(x.Promise&&x.Promise.resolve){var a=x.Promise.resolve(void 0);ke=function(){a.then(oe)}}else ke=function(){var b=oe;"function"!==
typeof x.setImmediate||x.Window&&x.Window.prototype&&x.Window.prototype.setImmediate==x.setImmediate?(pe||(pe=fe()),pe(b)):x.setImmediate(b)}},oe=function(){for(var a;a=ie();){try{a.h.call(a.g)}catch(b){Qa(b)}be(qe,a)}me=!1},X=function(a){this.g=0;this.u=void 0;this.j=this.h=this.i=null;this.m=this.o=!1;if(a!=ce)try{var b=this;a.call(void 0,function(c){re(b,2,c)},function(c){re(b,3,c)})}catch(c){re(this,3,c)}},se=function(){this.next=this.i=this.h=this.m=this.g=null;this.j=!1},ue=function(a,b,c){var d=
te.get();d.m=a;d.h=b;d.i=c;return d},we=function(a,b,c){ve(a,b,c,null)||ne(Ma(b,a))},xe=function(a){return new X(function(b){var c=a.length,d=[];if(c)for(var e=function(k,h,l){c--;d[k]=h?{Da:!0,value:l}:{Da:!1,reason:l};0==c&&b(d)},g=0,f;g<a.length;g++)f=a[g],we(f,Ma(e,g,!0),Ma(e,g,!1));else b(d)})},ze=function(a,b){b=ue(b,b);b.j=!0;ye(a,b)},Ae=function(a,b){if(0==a.g)if(a.i){var c=a.i;if(c.h){for(var d=0,e=null,g=null,f=c.h;f&&(f.j||(d++,f.g==a&&(e=f),!(e&&1<d)));f=f.next)e||(g=f);e&&(0==c.g&&1==
d?Ae(c,b):(g?(d=g,d.next==c.j&&(c.j=d),d.next=d.next.next):Be(c),Ce(c,e,3,b)))}a.i=null}else re(a,3,b)},ye=function(a,b){a.h||2!=a.g&&3!=a.g||De(a);a.j?a.j.next=b:a.h=b;a.j=b},Fe=function(a,b,c,d){var e=ue(null,null,null);e.g=new X(function(g,f){e.m=b?function(k){try{var h=b.call(d,k);g(h)}catch(l){f(l)}}:g;e.h=c?function(k){try{var h=c.call(d,k);void 0===h&&k instanceof Ee?f(k):g(h)}catch(l){f(l)}}:f});e.g.i=a;ye(a,e);return e.g},re=function(a,b,c){0==a.g&&(a===c&&(b=3,c=new TypeError("M")),a.g=
1,ve(c,a.Ma,a.Na,a)||(a.u=c,a.g=b,a.i=null,De(a),3!=b||c instanceof Ee||Ge(a,c)))},ve=function(a,b,c,d){if(a instanceof X)return ye(a,ue(b||ce,c||null,d)),!0;if(a)try{var e=!!a.$goog_Thenable}catch(f){e=!1}else e=!1;if(e)return a.then(b,c,d),!0;if(Fa(a))try{var g=a.then;if("function"===typeof g)return He(a,g,b,c,d),!0}catch(f){return c.call(d,f),!0}return!1},He=function(a,b,c,d,e){function g(h){k||(k=!0,d.call(e,h))}function f(h){k||(k=!0,c.call(e,h))}var k=!1;try{b.call(a,f,g)}catch(h){g(h)}},De=
function(a){a.o||(a.o=!0,ne(a.Ca,a))},Be=function(a){var b=null;a.h&&(b=a.h,a.h=b.next,b.next=null);a.h||(a.j=null);return b},Ce=function(a,b,c,d){if(3==c&&b.h&&!b.j)for(;a&&a.m;a=a.i)a.m=!1;if(b.g)b.g.i=null,Ie(b,c,d);else try{b.j?b.m.call(b.i):Ie(b,c,d)}catch(e){Je.call(null,e)}be(te,b)},Ie=function(a,b,c){2==b?a.m.call(a.i,c):a.h&&a.h.call(a.i,c)},Ge=function(a,b){a.m=!0;ne(function(){a.m&&Je.call(null,b)})},Ee=function(a){Pa.call(this,a)},Ke=function(a,b){this.type=a;this.g=this.target=b;this.defaultPrevented=
!1},Ne=function(a,b){Ke.call(this,a?a.type:"");this.relatedTarget=this.g=this.target=null;this.button=this.screenY=this.screenX=this.clientY=this.clientX=0;this.key="";this.metaKey=this.shiftKey=this.altKey=this.ctrlKey=!1;this.state=null;this.pointerId=0;this.pointerType="";this.i=null;if(a){var c=this.type=a.type,d=a.changedTouches&&a.changedTouches.length?a.changedTouches[0]:null;this.target=a.target||a.srcElement;this.g=b;if(b=a.relatedTarget){if(Le){a:{try{cb(b.nodeName);var e=!0;break a}catch(g){}e=
!1}e||(b=null)}}else"mouseover"==c?b=a.fromElement:"mouseout"==c&&(b=a.toElement);this.relatedTarget=b;d?(this.clientX=void 0!==d.clientX?d.clientX:d.pageX,this.clientY=void 0!==d.clientY?d.clientY:d.pageY,this.screenX=d.screenX||0,this.screenY=d.screenY||0):(this.clientX=void 0!==a.clientX?a.clientX:a.pageX,this.clientY=void 0!==a.clientY?a.clientY:a.pageY,this.screenX=a.screenX||0,this.screenY=a.screenY||0);this.button=a.button;this.key=a.key||"";this.ctrlKey=a.ctrlKey;this.altKey=a.altKey;this.shiftKey=
a.shiftKey;this.metaKey=a.metaKey;this.pointerId=a.pointerId||0;this.pointerType="string"===typeof a.pointerType?a.pointerType:Me[a.pointerType]||"";this.state=a.state;this.i=a;a.defaultPrevented&&Ne.M.h.call(this)}},Pe=function(a,b,c,d,e){this.listener=a;this.proxy=null;this.src=b;this.type=c;this.capture=!!d;this.ga=e;this.key=++Oe;this.W=this.ea=!1},Qe=function(a){a.W=!0;a.listener=null;a.proxy=null;a.src=null;a.ga=null},Re=function(a){this.src=a;this.g={};this.h=0},Te=function(a,b){var c=b.type;
if(c in a.g){var d=a.g[c],e=Se(d,b),g;(g=0<=e)&&Array.prototype.splice.call(d,e,1);g&&(Qe(b),0==a.g[c].length&&(delete a.g[c],a.h--))}},Ue=function(a,b,c,d){for(var e=0;e<a.length;++e){var g=a[e];if(!g.W&&g.listener==b&&g.capture==!!c&&g.ga==d)return e}return-1},We=function(a,b,c,d,e){if(d&&d.once)return Ve(a,b,c,d,e);if(Array.isArray(b)){for(var g=0;g<b.length;g++)We(a,b[g],c,d,e);return null}c=Xe(c);return a&&a[Ye]?a.g.add(String(b),c,!1,Fa(d)?!!d.capture:!!d,e):Ze(a,b,c,!1,d,e)},Ze=function(a,
b,c,d,e,g){if(!b)throw Error("N");var f=Fa(e)?!!e.capture:!!e,k=$e(a);k||(a[af]=k=new Re(a));c=k.add(b,c,d,f,g);if(c.proxy)return c;d=bf();c.proxy=d;d.src=a;d.listener=c;if(a.addEventListener)cf||(e=f),void 0===e&&(e=!1),a.addEventListener(b.toString(),d,e);else if(a.attachEvent)a.attachEvent(df(b.toString()),d);else if(a.addListener&&a.removeListener)a.addListener(d);else throw Error("O");ef++;return c},bf=function(){function a(c){return b.call(a.src,a.listener,c)}var b=ff;return a},Ve=function(a,
b,c,d,e){if(Array.isArray(b)){for(var g=0;g<b.length;g++)Ve(a,b[g],c,d,e);return null}c=Xe(c);return a&&a[Ye]?a.g.add(String(b),c,!0,Fa(d)?!!d.capture:!!d,e):Ze(a,b,c,!0,d,e)},gf=function(a,b,c,d,e){if(Array.isArray(b))for(var g=0;g<b.length;g++)gf(a,b[g],c,d,e);else(d=Fa(d)?!!d.capture:!!d,c=Xe(c),a&&a[Ye])?(a=a.g,b=String(b).toString(),b in a.g&&(g=a.g[b],c=Ue(g,c,d,e),-1<c&&(Qe(g[c]),Array.prototype.splice.call(g,c,1),0==g.length&&(delete a.g[b],a.h--)))):a&&(a=$e(a))&&(b=a.g[b.toString()],a=-1,
b&&(a=Ue(b,c,d,e)),(c=-1<a?b[a]:null)&&hf(c))},hf=function(a){if("number"!==typeof a&&a&&!a.W){var b=a.src;if(b&&b[Ye])Te(b.g,a);else{var c=a.type,d=a.proxy;b.removeEventListener?b.removeEventListener(c,d,a.capture):b.detachEvent?b.detachEvent(df(c),d):b.addListener&&b.removeListener&&b.removeListener(d);ef--;(c=$e(b))?(Te(c,a),0==c.h&&(c.src=null,b[af]=null)):Qe(a)}}},df=function(a){return a in jf?jf[a]:jf[a]="on"+a},ff=function(a,b){if(a.W)a=!0;else{b=new Ne(b,this);var c=a.listener,d=a.ga||a.src;
a.ea&&hf(a);a=c.call(d,b)}return a},$e=function(a){a=a[af];return a instanceof Re?a:null},Xe=function(a){if("function"===typeof a)return a;a[kf]||(a[kf]=function(b){return a.handleEvent(b)});return a[kf]},lf=function(){W.call(this);this.g=new Re(this);this.v=this;this.j=null},nf=function(a,b){var c=a.j;if(c){var d=[];for(var e=1;c;c=c.j)d.push(c),++e}a=a.v;c=b.type||b;"string"===typeof b?b=new Ke(b,a):b instanceof Ke?b.target=b.target||a:(e=b,b=new Ke(c,a),Ad(b,e));e=!0;if(d)for(var g=d.length-1;0<=
g;g--){var f=b.g=d[g];e=mf(f,c,!0,b)&&e}f=b.g=a;e=mf(f,c,!0,b)&&e;e=mf(f,c,!1,b)&&e;if(d)for(g=0;g<d.length;g++)f=b.g=d[g],e=mf(f,c,!1,b)&&e},mf=function(a,b,c,d){b=a.g.g[String(b)];if(!b)return!0;b=b.concat();for(var e=!0,g=0;g<b.length;++g){var f=b[g];if(f&&!f.W&&f.capture==c){var k=f.listener,h=f.ga||f.src;f.ea&&Te(a.g,f);e=!1!==k.call(h,d)&&e}}return e&&!d.defaultPrevented},of=function(a,b){lf.call(this);this.i=a||1;this.h=b||x;this.o=La(this.La,this);this.s=Date.now()},pf=function(a){a.ca=!1;
a.F&&(a.h.clearTimeout(a.F),a.F=null)},qf=function(a,b){if("function"!==typeof a)if(a&&"function"==typeof a.handleEvent)a=La(a.handleEvent,a);else throw Error("P");return 2147483647<Number(b)?-1:x.setTimeout(a,b||0)},rf=function(a){var b=null;return(new X(function(c,d){b=qf(function(){c(void 0)},a);-1==b&&d(Error("Q"))})).ia(function(c){x.clearTimeout(b);throw c;})},sf=function(a,b){this.g=a;this.h=b;a=this.g;b=rd(this.h,"docs-clibs");a.Z=b;a=this.g;b=rd(this.h,"docs-cirts");a.Y=b},vf=function(a,
b,c){var d=Vd(new Ud,vc(b));tf(a.g,d);return(new X(function(e,g){var f=Date.now();c.h++;c.i=f;uf(a,e,g)})).ia(function(e){if("number"===typeof e&&(500<=e&&600>e||401==e||0==e)&&4>c.h)return e=$d(c,0===e?1:3),rf(e).then(function(){return vf(a,b,c)});throw wf(e);})},uf=function(a,b,c){a.g.flush(b,function(d,e){d=pd(a.h,"docs-ecir")?e:wf(e);c(d)})},wf=function(a){return"number"===typeof a?new Xd(!(500<=a&&600>a||401==a||0==a)):a},xf=aa("g"),yf=aa("h"),zf=function(a){this.l=I(a)},Af=function(a){this.l=
I(a)},Bf=function(a){this.l=I(a)},Cf=function(a){this.l=I(a)},Df=function(a){this.l=I(a)},Y=function(a){this.l=I(a)},Ef=aa("h"),Ff=function(){return new X(function(a){a()})},Gf=function(a,b){this.h=a;this.g=b},Hf=function(a,b){this.i=a;this.o=b;this.h=!1;this.j=function(){return Date.now()};this.m=this.j()},Jf=function(a,b){a.i=b;a.g&&a.h?(If(a),a.start()):a.g&&If(a)},If=function(a){a.h=!1;a.g&&(clearTimeout(a.g),a.g=void 0)},Kf=function(a){if(a.h){var b=Math.max(a.j()-a.m,0);b<.8*a.i?a.g=setTimeout(function(){Kf(a)},
a.i-b):(a.g&&(clearTimeout(a.g),a.g=void 0),a.o(),a.h&&(If(a),a.start()))}else a.g=void 0},Lf=function(a){this.h=this.g=this.i=a},Mf=function(a){this.l=I(a)},Nf=function(a){this.l=I(a)},Of=function(a){this.l=I(a)},Pf=function(a,b){pc(a,Nf,1,b)},Qf=function(a){this.l=I(a)},Uf=function(a,b){b=void 0===b?Rf:b;if(!Sf){var c;a=null==(c=a.navigator)?void 0:c.userAgentData;if(!a||"function"!==typeof a.getHighEntropyValues||a.brands&&"function"!==typeof a.brands.map)return Promise.reject(Error("U"));c=(a.brands||
[]).map(function(e){var g=new Nf;g=R(g,1,e.brand);return R(g,2,e.version)});Pf(O(Tf,2,a.mobile),c);Sf=a.getHighEntropyValues(b)}var d=new Set(b);return Sf.then(function(e){var g=wc(Tf);d.has("platform")&&R(g,3,e.platform);d.has("platformVersion")&&R(g,4,e.platformVersion);d.has("architecture")&&R(g,5,e.architecture);d.has("model")&&R(g,6,e.model);d.has("uaFullVersion")&&R(g,7,e.uaFullVersion);return g}).catch(function(){return wc(Tf)})},Vf=function(a){this.l=I(a)},Wf=function(a){this.l=I(a,19)},Xf=
function(a){this.l=I(a,8)},Yf=function(a){this.l=I(a)},cg=function(a){W.call(this);var b=this;this.i=[];this.ba="";this.pa=this.T=-1;this.S=this.o=0;this.ya=1;this.Y=0;this.P=a.P;this.V=a.V||q();this.h=new Zf(a.P,a.K);this.B=a.B;this.R=a.R||null;this.Z=1E3;this.s=a.Oa||null;this.L=a.L||null;this.U=a.U||!1;this.withCredentials=!a.sa;this.K=a.K||!1;this.H="undefined"!==typeof URLSearchParams&&!!(new URL($f())).searchParams&&!!(new URL($f())).searchParams.set;var c=S(new Vf,1,1);ag(this.h,c);this.j=
new Lf(1E4);a=bg(this,a.qa);this.g=new Hf(this.j.g,a);this.aa=new Hf(6E5,a);this.U||this.aa.start();this.K||(document.addEventListener("visibilitychange",function(){"hidden"===document.visibilityState&&b.v()}),document.addEventListener("pagehide",this.v.bind(this)))},bg=function(a,b){return a.H?b?function(){b().then(function(){a.flush()})}:function(){a.flush()}:q()},tf=function(a,b){a.H&&(b instanceof Ud?a.log(b):(b=Vd(new Ud,vc(b)),a.log(b)))},$f=ba("https://play.google.com/log?format=json&hasfast=true"),
Zf=function(a,b){this.K=b=void 0===b?!1:b;this.h=this.locale=null;this.g=new Wf;Number.isInteger(a)&&this.g.X(a);b||(this.locale=document.documentElement.getAttribute("lang"));ag(this,new Vf)},ag=function(a,b){M(a.g,Vf,1,b);sc(b,1)||S(b,1,1);a.K||(b=dg(a),rc(b,5)||R(b,5,a.locale));a.h&&(b=dg(a),L(b,Of,9)||M(b,Of,9,a.h))},gg=function(a,b){K(eg(a),Qf,11)&&(a=fg(a),S(a,1,b))},hg=function(a,b){K(eg(a),Qf,11)&&(a=fg(a),O(a,2,b))},eg=function(a){return L(a.g,Vf,1)},ig=function(a,b){var c=void 0===c?Rf:
c;var d=a.K?void 0:window;d?b(d,c).then(function(e){a.h=e;e=dg(a);M(e,Of,9,a.h);return!0}).catch(ba(!1)):Promise.resolve(!1)},dg=function(a){a=eg(a);var b=L(a,Qf,11);b||(b=new Qf,M(a,Qf,11,b));return b},fg=function(a){a=dg(a);var b=L(a,Mf,10);b||(b=new Mf,O(b,2,!1),M(a,Mf,10,b));return b},jg=function(a,b,c,d,e){var g=0,f=0;c=void 0===c?0:c;g=void 0===g?0:g;f=void 0===f?0:f;d=void 0===d?0:d;if(K(eg(a),Qf,11)){var k=fg(a);P(k,3,d)}K(eg(a),Qf,11)&&(d=fg(a),P(d,4,g));K(eg(a),Qf,11)&&(g=fg(a),P(g,5,f));
a=wc(a.g);a=gc(a,4,Fb(Date.now().toString()));b=pc(a,Ud,3,b);e&&(a=new Rd,e=P(a,13,e),a=new Sd,e=M(a,Rd,2,e),a=new Td,e=M(a,Sd,1,e),e=S(e,2,9),M(b,Td,18,e));c&&Q(b,14,c);return b},kg=function(){this.Aa="undefined"!==typeof AbortController},lg=function(a,b){W.call(this);this.P=a;this.L=b;this.i="https://play.google.com/log?format=json&hasfast=true";this.h=!1;this.g=this.R=this.B=null},mg=function(a){return(a=a.exec(y()))?a[1]:""},og=function(){var a,b,c=null==(a=x.navigator)?void 0:null==(b=a.userAgentData)?
void 0:b.brands;a=Va()||!!c&&Array.isArray(c)&&c.some(function(d){return(d=d.brand)&&-1!=d.indexOf("Microsoft Edge")});return ng&&a},qg=function(){var a=x.performance;if(!a||!a.getEntriesByType)return null;var b=a.getEntriesByType("navigation");if(!b||!b[0])return null;b=b[0];a=a.timeOrigin||a.timing.navigationStart;var c=new id;c=Q(c,1,1E3*Math.round(a));c=Q(c,2,Math.round(1E3*(a+b.redirectStart)));c=Q(c,3,Math.round(1E3*(a+b.redirectEnd)));c=Q(c,17,Math.round(1E3*(a+b.workerStart)));c=Q(c,4,Math.round(1E3*
(a+b.fetchStart)));c=Q(c,5,Math.round(1E3*(a+b.domainLookupStart)));c=Q(c,6,Math.round(1E3*(a+b.domainLookupEnd)));c=Q(c,7,Math.round(1E3*(a+b.connectStart)));c=Q(c,8,Math.round(1E3*(a+b.connectEnd)));b.secureConnectionStart&&Q(c,9,Math.round(1E3*(a+b.secureConnectionStart)));Q(c,10,Math.round(1E3*(a+b.requestStart)));Q(c,11,Math.round(1E3*(a+b.responseStart)));Q(c,12,Math.round(1E3*(a+b.responseEnd)));P(c,18,b.redirectCount);S(c,19,pg[b.type]||1);return c},rg=function(a){this.l=I(a)},sg=function(a){this.l=
I(a)},tg=q(),ug=aa("g"),vg=function(a){this.l=I(a)},wg=function(a){this.l=I(a)},xg=function(a){this.l=I(a)},yg=function(){this.h=this.g=null},zg=function(a){var b=L(a.g,Y,5);null==b&&(b=new Y,M(a.g,Y,5,b));return b},Ag=function(a){N(a.g,10);null!=N(a.g,6)||N(a.g,10);if(2==sc(L(a.g,xg,8),3)&&null!=N(a.g,13)){var b=L(L(a.g,xg,8),vg,2);N(b,2)}var c=L(a.g,Y,5);null!=c&&(b=a.g,c=wc(c),M(b,Y,5,c));return a.g},Bg=function(){yg.call(this)},Cg=function(a){W.call(this);this.h=a;this.g={}},Eg=function(a,b,c,
d){Array.isArray(c)||(c&&(Dg[0]=c.toString()),c=Dg);for(var e=0;e<c.length;e++){var g=We(b,c[e],d||a.handleEvent,!1,a.h||a);if(!g)break;a.g[g.key]=g}},Fg=function(a){yd(a.g,function(b,c){this.g.hasOwnProperty(c)&&hf(b)},a);a.g={}},Gg=function(a,b,c){a=new of(a);Id(c,a);var d=new Cg(c);Id(c,d);Eg(d,a,"tick",b);a.start()},Hg=function(){yg.call(this)},Ig=function(a,b,c){var d=1E3*Date.now();if(0==b){b=new xg;var e=new wg;e=Q(e,1,d);M(b,wg,1,e);S(b,3,1);M(a.g,xg,8,b);Q(a.g,12,c);Q(a.g,13,c);Q(a.g,4,d);
Q(a.g,3,c)}else 1==b&&(b=new xg,e=new vg,d=Q(e,1,d),M(b,vg,2,d),S(b,3,2),M(a.g,xg,8,b),Q(a.g,12,c),Q(a.g,3,c));return a},Lg=function(a,b){if(b&&a in b)return a;var c=Jg?"Webkit":Le?"Moz":Kg?"ms":null;return c?(c=c.toLowerCase(),a=c+Bd(a),void 0===b||a in b?a:null):null},Mg=function(){Ke.call(this,"visibilitychange")},Pg=function(a){function b(g){var f=v(g);g=f.next().value;f=ia(f);return a.apply(g,f)}function c(g){g=v(g);g.next();g=ia(g);return d(e,g)}var d=void 0===d?Ng:d;var e=Ia(a);return function(){var g=
Ba.apply(0,arguments),f=this||x,k=Og.get(f);k||(k={},Og.set(f,k));return db(k,[this].concat(ja(g)),b,c)}},Ng=function(a,b){a=[a];for(var c=b.length-1;0<=c;--c)a.push(typeof b[c],b[c]);return a.join("\v")},Rg=function(a){lf.call(this);a||(a=Qg||(Qg=new ee));this.h=a;if(this.i=this.Ea())this.o=We(this.h.g,this.i,La(this.Ga,this))},Sg=function(a,b){W.call(this);this.h=a;this.g=new Rg(b);Id(this,this.g);this.i=new Cg(this);Id(this,this.i);this.g.N()&&Eg(this.i,this.g,"visibilitychange",this.j)},Tg=function(a,
b,c){c=void 0===c?!1:c;W.call(this);this.g=a;this.h=b;Id(this,this.h);this.i=c},Ug=function(a,b,c){W.call(this);this.s=null!=c?a.bind(c):a;this.o=b;this.h=null;this.j=!1;this.g=null},Vg=function(a){a.g=qf(function(){a.g=null;a.j&&(a.j=!1,Vg(a))},a.o);var b=a.h;a.h=null;a.s.apply(null,b)},Wg=function(a,b,c,d,e){W.call(this);this.g=a;this.v=b;this.h=new Ug(this.j,3E3,this);this.i=new Set;this.o=d;this.s=e||6E4},Xg=function(a){W.call(this);this.g=a;Id(this,this.g)},Yg=function(a){this.l=I(a)},Zg=function(a){this.l=
I(a)},$g=function(a){this.l=I(a)},ah=function(a){this.l=I(a)},bh=function(a){this.l=I(a)},ch=function(a){this.l=I(a)},dh=function(a){this.l=I(a)},eh=function(a){this.l=I(a)},fh=function(a){this.l=I(a)},gh=function(a){this.l=I(a)},jh=function(){this.h=this.o=!1;this.i=null;for(var a=this.m=this.j=0;a<hh.length;a++){var b=hh[a];if(b&&b.chrome&&b.chrome.runtime&&b.chrome.runtime.connect){this.o=!0;ih(this,b);break}}},ih=function(a,b){a.i=b.chrome.runtime.connect("ciiamoeghmklpofjbocenebdfbgjapaa");a.h=
!0;a.i.onDisconnect.addListener(function(){a.h=!1});document.addEventListener("mousemove",function(c){a.j=c.clientX;a.m=c.clientY})},kh=q(),lh=function(){this.h={};this.i={};this.g=null},mh=function(){this.g=[]},nh=function(){this.g={}},oh=function(a){this.l=I(a)},ph=function(a){this.l=I(a)},qh=function(a){this.l=I(a,500)},rh=function(a,b){this.h=a;this.H=b;this.o=1;this.m=this.i=null;this.u=new nh;this.g=new mh;this.s=this.v=this.j=!1},th=function(a,b){var c=new yg,d=new oh;a=a.H;c.g=d;c.h=a;d=c.h.h[sh.g].va();
a=c.h;d.g=c.g;d.h=a;Q(d.g,10,b);return d},vh=function(a){var b=a.g,c=b.g;b.g=[];b=new qh;var d=wc(a.h.g);b=M(b,Yg,2,d);d=a.h;uh(d);(d=d.i?wc(d.i):null)&&M(b,gh,5,d);d=a.h;for(var e,g=c.length-1;0<=g;g--){var f=L(c[g],Y,5);if(f&&L(f,jd,1)){f=L(f,jd,1);null!=ic(f,12)&&void 0===e&&(e=void 0,e=void 0===e?!1:e,e=tc(ic(f,12),e));f=L(f,hd,20);if(void 0!==f&&void 0===k){var k=new Md;var h=ic(f,2);void 0!==h&&O(k,2,h);f=ic(f,1);void 0!==f&&O(k,1,f)}if(void 0!==e&&void 0!==k)break}}d=d.j?wc(d.j):null;if(void 0!==
e||void 0!==k)d||(d=new Pd),void 0!==e&&O(d,6,e),void 0!==k&&M(d,Md,13,k);(k=d)&&M(b,Pd,3,k);a=wc(a.h.m);M(b,ph,4,a);pc(b,oh,1,c);return b},xh=function(a,b){var c=Ig(th(a,b),0,a.o++);var d=a.u;var e=Object.keys(d.g);if(0==e.length)d=null;else{for(var g=[],f=0;f<e.length;f++){var k=Number(e[f]),h=d.g[k],l=new zf;k=Q(l,1,k);h=N(h.g,10);h=Q(k,2,null==h?void 0:h);g.push(h)}d=g}716!=b&&(b=a.m,Q(c.g,6,a.i),e=new Af,b=Q(e,1,b),d&&pc(b,zf,2,d),d=zg(c),M(d,Af,3,b));wh(a,c);return c},wh=function(a,b){a.i=N(b.g,
12);a.m=N(L(L(b.g,xg,8),wg,1),1)},yh=q(),zh=function(){this.g=this.h=null},Ah=function(){this.g=new Yg;this.i=null;this.m=new ph;S(this.m,1,6);this.j=this.h=null},Z=function(a){null==a.i&&(a.i=new gh);return a.i},Bh=function(a){a=Z(a);var b=L(a,ah,16);b||(b=new ah,M(a,ah,16,b));return b},Ch=function(a){a=Z(a);var b=L(a,bh,9);b||(b=new bh,M(a,bh,9,b));return b},uh=function(a){null!=rc(a.g,1)&&null!=sc(a.g,6)&&sc(a.g,6)},Dh=function(){this.h=this.j=null;this.g=new Ah;this.i=null;this.m=!1},Oh=function(a,
b,c,d){var e=md(),g=md(),f=sd(g,"gaia_session_id"),k=rd(g,"docs-cclt"),h=sd(g,"docs-ccu");f=new lg(k,f||"0");f.i=h;f.h=!0;pd(g,"docs-ecuach")&&(f.g=Uf);f.B||(f.B=new kg);g=new cg({P:f.P,V:f.V?f.V:Gd,L:f.L,Oa:f.i,K:!1,U:f.h,sa:f.j,qa:f.qa,B:f.B});Id(f,g);f.R&&(g.R=f.R);ig(g.h,Uf);f.g&&ig(g.h,f.g);f.B.X&&f.B.X(f.P);f.B.Ka&&f.B.Ka(g);h=new sf(g,e);g=new Wd(g);var l,m;null!=(l=null==d?void 0:d())?m=l:m=new Gf(new yf(new Ef(h)),g);l=(new Qd(!1,e)).get();d=Ld();g=new lh;h=new ed;f=new fd;b=S(f,1,b);c=P(b,
2,c);K(h.g,fd,152);M(h.g,fd,152,c);c=new xf(h);g.i.docs_app_load_invariants=c;g.g&&(b=g.g,K(b,dd,3),M(b,dd,3,c.g.g));c=new Dh;c.j=m.h;c.h=m.g;c.i=g;b=c.g;sc(b.g,6);S(b.g,6,a);a=c.g;b=d.v;rc(a.g,1);R(a.g,1,b);c.m=!1;uh(c.g);a=new zh;a.h=c.g;null!=c.i&&(a.g=c.i);var n;null==a.g&&(a.g=new lh);a.g.h[sh.g]=new yh;a.g.h[Eh.g]=new kh;b=a.g;m=a.h;K(Z(m),$g,1)||(g=Z(m),h=new $g,M(g,$g,1,h));m=L(Z(m),$g,1);b.g=m;g=b.i;m=[];h=0;for(n in g)m[h++]=g[n];for(n=0;n<m.length;n++)g=m[n],h=b.g,K(h,dd,3),M(h,dd,3,g.g.g);
n=new rh(a.h,a.g);c.m&&(a=new yf(new jh),c.j=a,c.h=a);n=new Tg(n,new Wg(n,c.j,c.h,!1,null),!1);c.m&&(n=new Xg(n));this.g=n;n=this.g.ja();a=c=b=null;b=void 0===b?null:b;c=void 0===c?null:c;a=void 0===a?null:a;m=d.s;N(n.g,3);Q(n.g,3,m);m=d.o;rc(n.g,4);R(n.g,4,m);m=1E3*Date.now();N(n.g,2);Q(n.g,2,m);n.j=l;l=new sg;Fh?S(l,1,1):Gh?S(l,1,2):Hh?S(l,1,5):Ih?S(l,1,4):bb()?S(l,1,6):Jh&&S(l,1,3);R(l,2,Kh);m=Z(n);K(m,sg,3);m=Z(n);M(m,sg,3,l);l=d.u;null==l||pd(e,"icso")||(m=Ch(n),Bb(bc(m,4)),P(m,4,l));l=d.i;null!==
l&&(null==n.h&&(n.h=new Zg,m=Z(n),M(m,Zg,2,n.h)),rc(n.h,1),R(n.h,1,l));if(l=qg())m=Z(n),K(m,id,6),m=Z(n),M(m,id,6,l);l=d.g;m=Z(n);sc(m,4);m=Z(n);S(m,4,l);l=pd(e,"docs-idu")?1:2;m=Z(n);sc(m,5);m=Z(n);S(m,5,l);l=pd(e,"docs-isb");m=Bh(n);ic(m,3);O(m,3,l);l=Bh(n);ic(l,7);O(l,7,Lh);l=Bh(n);ic(l,8);O(l,8,Mh);ng&&pd(e,"docs-ccdil")&&(e=new rg,e=S(e,1,d.j),e=mc(e,2,d.m),d=Bh(n),K(d,rg,9),M(d,rg,9,e));Nh&&(e=!window.showModalDialog,d=Bh(n),ic(d,6),O(d,6,e));e=x.navigator.deviceMemory;Number.isFinite(e)&&(d=
Ch(n),Bb(bc(d,9)),P(d,9,1024*e));e=x.navigator.hardwareConcurrency;Number.isFinite(e)&&(d=Ch(n),Bb(bc(d,10)),P(d,10,e));e=x.document.wasDiscarded;void 0!==e&&(d=Z(n),l=L(d,fh,18),l||(l=new fh,M(d,fh,18,l)),d=l,l=L(d,eh,1),l||(l=new eh,M(d,eh,1,l)),d=l,ic(d,1),O(d,1,e));b&&(e=b.g())&&(d=Z(n),K(d,gd,13),d=Z(n),M(d,gd,13,e));c&&(e=c.g())&&(d=Z(n),K(d,dh,20),d=Z(n),M(d,dh,20,e));e=og();d=Bh(n);ic(d,11);O(d,11,e);a&&(e=a.g(),d=Z(n),K(d,ch,21),d=Z(n),M(d,ch,21,e));this.g.la()},Ph=function(a,b){this.j=a;
this.i=b;this.h=!1;this.g=window.performance.timing.responseStart-window.performance.timeOrigin},Qh=function(a,b,c,d){d=void 0===d?null:d;a=a.j.g;b=a.da(b,0);var e=zg(b),g=new Bf;c=Q(g,1,1E3*c);M(e,Bf,23,c);d&&(c=zg(b),M(c,Df,108,d));a.ha(b)},Sh=function(a,b){var c=x.DOCS_drawing_decode[b];c&&Qh(a,47540,c-a.g,Rh(b))},Th=function(a,b){var c=x.DOCS_drawing_load[b];b=Rh(b);c&&Qh(a,47538,c-a.g,b);a.h||null==a.i||a.i!=Object.keys(x.DOCS_drawing_load).length||(Qh(a,47539,Math.max.apply(Math,ja(Object.values(x.DOCS_drawing_load)))-
a.g),a.h=!0)},Rh=function(a){var b=new Df,c=document.getElementById(a);if(!c||!c.hasAttribute("src"))return b;c.getAttribute("src").startsWith("data:")?(a=S(b,1,2),c=Math.ceil(c.getAttribute("src").length/1024),P(a,2,c)):S(b,1,1);return b},fa="function"==typeof Object.defineProperties?Object.defineProperty:function(a,b,c){if(a==Array.prototype||a==Object.prototype)return a;a[b]=c.value;return a},ea=da(this);
u("Symbol",function(a){function b(g){if(this instanceof b)throw new TypeError("b");return new c(d+(g||"")+"_"+e++,g)}function c(g,f){this.g=g;fa(this,"description",{configurable:!0,writable:!0,value:f})}if(a)return a;c.prototype.toString=function(){return this.g};var d="jscomp_symbol_"+(1E9*Math.random()>>>0)+"_",e=0;return b});
u("Symbol.iterator",function(a){if(a)return a;a=Symbol("c");for(var b="Array Int8Array Uint8Array Uint8ClampedArray Int16Array Uint16Array Int32Array Uint32Array Float32Array Float64Array".split(" "),c=0;c<b.length;c++){var d=ea[b[c]];"function"===typeof d&&"function"!=typeof d.prototype[a]&&fa(d.prototype,a,{configurable:!0,writable:!0,value:function(){return ha(ca(this))}})}return a});
var Uh="function"==typeof Object.assign?Object.assign:function(a,b){for(var c=1;c<arguments.length;c++){var d=arguments[c];if(d)for(var e in d)la(d,e)&&(a[e]=d[e])}return a};u("Object.assign",function(a){return a||Uh});var ma="function"==typeof Object.create?Object.create:function(a){function b(){}b.prototype=a;return new b},Vh;
if("function"==typeof Object.setPrototypeOf)Vh=Object.setPrototypeOf;else{var Wh;a:{var Xh={a:!0},Yh={};try{Yh.__proto__=Xh;Wh=Yh.a;break a}catch(a){}Wh=!1}Vh=Wh?function(a,b){a.__proto__=b;if(a.__proto__!==b)throw new TypeError("e`"+a);return a}:null}var na=Vh;oa.prototype.s=aa("u");oa.prototype.return=function(a){this.g={return:a};this.h=this.j};oa.prototype.O=aa("h");
u("Promise",function(a){function b(f){this.g=0;this.i=void 0;this.h=[];this.u=!1;var k=this.j();try{f(k.resolve,k.reject)}catch(h){k.reject(h)}}function c(){this.g=null}function d(f){return f instanceof b?f:new b(function(k){k(f)})}if(a)return a;c.prototype.h=function(f){if(null==this.g){this.g=[];var k=this;this.i(function(){k.m()})}this.g.push(f)};var e=ea.setTimeout;c.prototype.i=function(f){e(f,0)};c.prototype.m=function(){for(;this.g&&this.g.length;){var f=this.g;this.g=[];for(var k=0;k<f.length;++k){var h=
f[k];f[k]=null;try{h()}catch(l){this.j(l)}}}this.g=null};c.prototype.j=function(f){this.i(function(){throw f;})};b.prototype.j=function(){function f(l){return function(m){h||(h=!0,l.call(k,m))}}var k=this,h=!1;return{resolve:f(this.T),reject:f(this.m)}};b.prototype.T=function(f){if(f===this)this.m(new TypeError("h"));else if(f instanceof b)this.aa(f);else{a:switch(typeof f){case "object":var k=null!=f;break a;case "function":k=!0;break a;default:k=!1}k?this.S(f):this.o(f)}};b.prototype.S=function(f){var k=
void 0;try{k=f.then}catch(h){this.m(h);return}"function"==typeof k?this.ba(k,f):this.o(f)};b.prototype.m=function(f){this.s(2,f)};b.prototype.o=function(f){this.s(1,f)};b.prototype.s=function(f,k){if(0!=this.g)throw Error("i`"+f+"`"+k+"`"+this.g);this.g=f;this.i=k;2===this.g&&this.Z();this.v()};b.prototype.Z=function(){var f=this;e(function(){if(f.H()){var k=ea.console;"undefined"!==typeof k&&k.error(f.i)}},1)};b.prototype.H=function(){if(this.u)return!1;var f=ea.CustomEvent,k=ea.Event,h=ea.dispatchEvent;
if("undefined"===typeof h)return!0;"function"===typeof f?f=new f("unhandledrejection",{cancelable:!0}):"function"===typeof k?f=new k("unhandledrejection",{cancelable:!0}):(f=ea.document.createEvent("CustomEvent"),f.initCustomEvent("unhandledrejection",!1,!0,f));f.promise=this;f.reason=this.i;return h(f)};b.prototype.v=function(){if(null!=this.h){for(var f=0;f<this.h.length;++f)g.h(this.h[f]);this.h=null}};var g=new c;b.prototype.aa=function(f){var k=this.j();f.fa(k.resolve,k.reject)};b.prototype.ba=
function(f,k){var h=this.j();try{f.call(k,h.resolve,h.reject)}catch(l){h.reject(l)}};b.prototype.then=function(f,k){function h(t,p){return"function"==typeof t?function(z){try{l(t(z))}catch(F){m(F)}}:p}var l,m,n=new b(function(t,p){l=t;m=p});this.fa(h(f,l),h(k,m));return n};b.prototype.catch=function(f){return this.then(void 0,f)};b.prototype.fa=function(f,k){function h(){switch(l.g){case 1:f(l.i);break;case 2:k(l.i);break;default:throw Error("j`"+l.g);}}var l=this;null==this.h?g.h(h):this.h.push(h);
this.u=!0};b.resolve=d;b.reject=function(f){return new b(function(k,h){h(f)})};b.race=function(f){return new b(function(k,h){for(var l=v(f),m=l.next();!m.done;m=l.next())d(m.value).fa(k,h)})};b.all=function(f){var k=v(f),h=k.next();return h.done?d([]):new b(function(l,m){function n(z){return function(F){t[z]=F;p--;0==p&&l(t)}}var t=[],p=0;do t.push(void 0),p++,d(h.value).fa(n(t.length-1),m),h=k.next();while(!h.done)})};return b});
u("String.prototype.startsWith",function(a){return a?a:function(b,c){var d=Ca(this,b,"startsWith"),e=d.length,g=b.length;c=Math.max(0,Math.min(c|0,d.length));for(var f=0;f<g&&c<e;)if(d[c++]!=b[f++])return!1;return f>=g}});
u("WeakMap",function(a){function b(h){this.g=(k+=Math.random()+1).toString();if(h){h=v(h);for(var l;!(l=h.next()).done;)l=l.value,this.set(l[0],l[1])}}function c(){}function d(h){var l=typeof h;return"object"===l&&null!==h||"function"===l}function e(h){if(!la(h,f)){var l=new c;fa(h,f,{value:l})}}function g(h){var l=Object[h];l&&(Object[h]=function(m){if(m instanceof c)return m;Object.isExtensible(m)&&e(m);return l(m)})}if(function(){if(!a||!Object.seal)return!1;try{var h=Object.seal({}),l=Object.seal({}),
m=new a([[h,2],[l,3]]);if(2!=m.get(h)||3!=m.get(l))return!1;m.delete(h);m.set(l,4);return!m.has(h)&&4==m.get(l)}catch(n){return!1}}())return a;var f="$jscomp_hidden_"+Math.random();g("freeze");g("preventExtensions");g("seal");var k=0;b.prototype.set=function(h,l){if(!d(h))throw Error("m");e(h);if(!la(h,f))throw Error("n`"+h);h[f][this.g]=l;return this};b.prototype.get=function(h){return d(h)&&la(h,f)?h[f][this.g]:void 0};b.prototype.has=function(h){return d(h)&&la(h,f)&&la(h[f],this.g)};b.prototype.delete=
function(h){return d(h)&&la(h,f)&&la(h[f],this.g)?delete h[f][this.g]:!1};return b});
u("Map",function(a){function b(){var k={};return k.G=k.next=k.head=k}function c(k,h){var l=k[1];return ha(function(){if(l){for(;l.head!=k[1];)l=l.G;for(;l.next!=l.head;)return l=l.next,{done:!1,value:h(l)};l=null}return{done:!0,value:void 0}})}function d(k,h){var l=h&&typeof h;"object"==l||"function"==l?g.has(h)?l=g.get(h):(l=""+ ++f,g.set(h,l)):l="p_"+h;var m=k[0][l];if(m&&la(k[0],l))for(k=0;k<m.length;k++){var n=m[k];if(h!==h&&n.key!==n.key||h===n.key)return{id:l,list:m,index:k,A:n}}return{id:l,
list:m,index:-1,A:void 0}}function e(k){this[0]={};this[1]=b();this.size=0;if(k){k=v(k);for(var h;!(h=k.next()).done;)h=h.value,this.set(h[0],h[1])}}if(function(){if(!a||"function"!=typeof a||!a.prototype.entries||"function"!=typeof Object.seal)return!1;try{var k=Object.seal({x:4}),h=new a(v([[k,"s"]]));if("s"!=h.get(k)||1!=h.size||h.get({x:4})||h.set({x:4},"t")!=h||2!=h.size)return!1;var l=h.entries(),m=l.next();if(m.done||m.value[0]!=k||"s"!=m.value[1])return!1;m=l.next();return m.done||4!=m.value[0].x||
"t"!=m.value[1]||!l.next().done?!1:!0}catch(n){return!1}}())return a;var g=new WeakMap;e.prototype.set=function(k,h){k=0===k?0:k;var l=d(this,k);l.list||(l.list=this[0][l.id]=[]);l.A?l.A.value=h:(l.A={next:this[1],G:this[1].G,head:this[1],key:k,value:h},l.list.push(l.A),this[1].G.next=l.A,this[1].G=l.A,this.size++);return this};e.prototype.delete=function(k){k=d(this,k);return k.A&&k.list?(k.list.splice(k.index,1),k.list.length||delete this[0][k.id],k.A.G.next=k.A.next,k.A.next.G=k.A.G,k.A.head=null,
this.size--,!0):!1};e.prototype.clear=function(){this[0]={};this[1]=this[1].G=b();this.size=0};e.prototype.has=function(k){return!!d(this,k).A};e.prototype.get=function(k){return(k=d(this,k).A)&&k.value};e.prototype.entries=function(){return c(this,function(k){return[k.key,k.value]})};e.prototype.keys=function(){return c(this,function(k){return k.key})};e.prototype.values=function(){return c(this,function(k){return k.value})};e.prototype.forEach=function(k,h){for(var l=this.entries(),m;!(m=l.next()).done;)m=
m.value,k.call(h,m[1],m[0],this)};e.prototype[Symbol.iterator]=e.prototype.entries;var f=0;return e});u("Number.MAX_SAFE_INTEGER",ba(9007199254740991));u("Number.isFinite",function(a){return a?a:function(b){return"number"!==typeof b?!1:!isNaN(b)&&Infinity!==b&&-Infinity!==b}});u("Number.isInteger",function(a){return a?a:function(b){return Number.isFinite(b)?b===Math.floor(b):!1}});u("Number.isSafeInteger",function(a){return a?a:function(b){return Number.isInteger(b)&&Math.abs(b)<=Number.MAX_SAFE_INTEGER}});
u("Math.trunc",function(a){return a?a:function(b){b=Number(b);if(isNaN(b)||Infinity===b||-Infinity===b||0===b)return b;var c=Math.floor(Math.abs(b));return 0>b?-c:c}});u("Object.values",function(a){return a?a:function(b){var c=[],d;for(d in b)la(b,d)&&c.push(b[d]);return c}});u("Object.is",function(a){return a?a:function(b,c){return b===c?0!==b||1/b===1/c:b!==b&&c!==c}});
u("Array.prototype.includes",function(a){return a?a:function(b,c){var d=this;d instanceof String&&(d=String(d));var e=d.length;c=c||0;for(0>c&&(c=Math.max(c+e,0));c<e;c++){var g=d[c];if(g===b||Object.is(g,b))return!0}return!1}});u("String.prototype.includes",function(a){return a?a:function(b,c){return-1!==Ca(this,b,"includes").indexOf(b,c||0)}});u("Array.prototype.keys",function(a){return a?a:function(){return Da(this,function(b){return b})}});
u("Array.prototype.values",function(a){return a?a:function(){return Da(this,function(b,c){return c})}});u("Array.from",function(a){return a?a:function(b,c,d){c=null!=c?c:function(k){return k};var e=[],g="undefined"!=typeof Symbol&&Symbol.iterator&&b[Symbol.iterator];if("function"==typeof g){b=g.call(b);for(var f=0;!(g=b.next()).done;)e.push(c.call(d,g.value,f++))}else for(g=b.length,f=0;f<g;f++)e.push(c.call(d,b[f],f));return e}});
u("Set",function(a){function b(c){this.g=new Map;if(c){c=v(c);for(var d;!(d=c.next()).done;)this.add(d.value)}this.size=this.g.size}if(function(){if(!a||"function"!=typeof a||!a.prototype.entries||"function"!=typeof Object.seal)return!1;try{var c=Object.seal({x:4}),d=new a(v([c]));if(!d.has(c)||1!=d.size||d.add(c)!=d||1!=d.size||d.add({x:4})!=d||2!=d.size)return!1;var e=d.entries(),g=e.next();if(g.done||g.value[0]!=c||g.value[1]!=c)return!1;g=e.next();return g.done||g.value[0]==c||4!=g.value[0].x||
g.value[1]!=g.value[0]?!1:e.next().done}catch(f){return!1}}())return a;b.prototype.add=function(c){c=0===c?0:c;this.g.set(c,c);this.size=this.g.size;return this};b.prototype.delete=function(c){c=this.g.delete(c);this.size=this.g.size;return c};b.prototype.clear=function(){this.g.clear();this.size=0};b.prototype.has=function(c){return this.g.has(c)};b.prototype.entries=function(){return this.g.entries()};b.prototype.values=function(){return this.g.values()};b.prototype.keys=b.prototype.values;b.prototype[Symbol.iterator]=
b.prototype.values;b.prototype.forEach=function(c,d){var e=this;this.g.forEach(function(g){return c.call(d,g,g,e)})};return b});var x=this||self,Zh=x._F_toggles||[],Ga="closure_uid_"+(1E9*Math.random()>>>0),Ha=0;Oa(Pa,Error);Pa.prototype.name="CustomError";var Qg;var $h=String.prototype.trim?function(a){return a.trim()}:function(a){return/^[\s\xa0]*([\s\S]*?)[\s\xa0]*$/.exec(a)[1]};var ai=!!(Zh[0]&256),bi=!!(Zh[0]&512),ci=!!(Zh[0]&4);var Ra=ai?bi:Ea(610401301),xc=ai?ci:Ea(188588736);var A,di=x.navigator;A=di?di.userAgentData||null:null;var Se=Array.prototype.indexOf?function(a,b){return Array.prototype.indexOf.call(a,b,void 0)}:function(a,b){if("string"===typeof a)return"string"!==typeof b||1!=b.length?-1:a.indexOf(b,0);for(var c=0;c<a.length;c++)if(c in a&&a[c]===b)return c;return-1},wd=Array.prototype.forEach?function(a,b){Array.prototype.forEach.call(a,b,void 0)}:function(a,b){for(var c=a.length,d="string"===typeof a?a.split(""):a,e=0;e<c;e++)e in d&&b.call(void 0,d[e],e,a)};cb[" "]=q();var ei=Ua(),Kg=Ta()?!1:B("Trident")||B("MSIE"),fi=B("Edge"),Le=B("Gecko")&&!(-1!=y().toLowerCase().indexOf("webkit")&&!B("Edge"))&&!(B("Trident")||B("MSIE"))&&!B("Edge"),Jg=-1!=y().toLowerCase().indexOf("webkit")&&!B("Edge"),Gh=ab(),Fh=Ya()?"Windows"===A.platform:B("Windows"),Jh=(Ya()?"Linux"===A.platform:B("Linux"))||bb(),Ih=Ya()?"Android"===A.platform:B("Android"),gi=Za(),hi=B("iPad"),ii=B("iPod"),Hh=$a(),ji;
a:{var ki="",li=function(){var a=y();if(Le)return/rv:([^\);]+)(\)|;)/.exec(a);if(fi)return/Edge\/([\d\.]+)/.exec(a);if(Kg)return/\b(?:MSIE|rv)[: ]([^\);]+)(\)|;)/.exec(a);if(Jg)return/WebKit\/(\S+)/.exec(a);if(ei)return/(?:Version)[ \/]?(\S+)/.exec(a)}();li&&(ki=li?li[1]:"");if(Kg){var mi,ni=x.document;mi=ni?ni.documentMode:void 0;if(null!=mi&&mi>parseFloat(ki)){ji=String(mi);break a}}ji=ki}var oi=ji;var Nh=Wa(),pi=Za()||B("iPod"),qi=B("iPad"),ri=B("Android")&&!(Xa()||Wa()||Ua()||B("Silk")),ng=Xa(),si=B("Safari")&&!(Xa()||(Ta()?0:B("Coast"))||Ua()||(Ta()?0:B("Edge"))||Va()||(Ta()?Sa("Opera"):B("OPR"))||Wa()||B("Silk")||B("Android"))&&!$a();var Sb={},Rb=null;var Ob="undefined"!==typeof Uint8Array,Qb=!Kg&&"function"===typeof btoa;var D=0,gb=0;var ti=mb(),dc=mb("2ex");Math.max.apply(Math,ja(Object.values({Xa:1,Va:2,Ua:4,ab:8,Za:16,Ya:32,Pa:64,cb:128,Ta:256,Sa:512,Wa:1024,Qa:2048,bb:4096,Ra:8192})));
var Zb=ti?function(a,b){a[ti]|=b}:function(a,b){void 0!==a.J?a.J|=b:Object.defineProperties(a,{J:{value:b,configurable:!0,writable:!0,enumerable:!1}})},H=ti?function(a){return a[ti]|0}:function(a){return a.J|0},J=ti?function(a){return a[ti]}:function(a){return a.J},G=ti?function(a,b){a[ti]=b;return a}:function(a,b){void 0!==a.J?a.J=b:Object.defineProperties(a,{J:{value:b,configurable:!0,writable:!0,enumerable:!1}});return a};var Ib={},qb={},uc,nc,ui=[];G(ui,55);nc=Object.freeze(ui);Object.freeze(new (q()));Object.freeze(new (q()));var ec;var yb=/^-?([1-9][0-9]*|0)(\.[0-9]+)?$/;var Kb;T.prototype.toJSON=function(){return uc?yc(this,this.l,!1):yc(this,Ub(this.l,Wb,void 0,void 0,!1),!0)};T.prototype.oa=Ib;T.prototype.toString=function(){return yc(this,this.l,!1).toString()};var Ac=1E3;Cc.prototype.I=function(a){return Object.is(this,a)||null==this&&null==a};Cc.prototype.wa=function(){return this.h||(Object.defineProperties(this,{h:{value:vi=vi+1|0,enumerable:!1}}),this.h)};Cc.prototype.toString=function(){return V(Zc($c(this.constructor)))+"@"+V((this.wa()>>>0).toString(16))};w(Dc,Cc);Dc.prototype.toString=function(){var a=Zc($c(this.constructor)),b=this.i;return null==b?a:V(a)+": "+V(b)};w(Fc,Dc);w(Gc,Fc);var qd;r=Hc.prototype;r.toString=function(a){a=a||10;if(2>a||36<a)throw Error("C`"+a);var b=this.g>>21;if(0==b||-1==b&&(0!=this.h||-2097152!=this.g))return b=Ic(this),10==a?""+b:b.toString(a);b=14-(a>>2);var c=Math.pow(a,b),d=U(c,c/4294967296);c=Qc(this,d);d=Math.abs(Ic(this.add(Lc(Mc(c,d)))));var e=10==a?""+d:d.toString(a);e.length<b&&(e="0000000000000".slice(e.length-b)+e);d=Ic(c);return(10==a?d:d.toString(a))+e};r.wa=function(){return this.h^this.g};r.I=function(a){return this.h==a.h&&this.g==a.g};
r.add=function(a){var b=this.g>>>16,c=this.g&65535,d=this.h>>>16,e=a.g>>>16,g=a.g&65535,f=a.h>>>16;a=(this.h&65535)+(a.h&65535);f=(a>>>16)+(d+f);d=f>>>16;d+=c+g;return U((f&65535)<<16|a&65535,((d>>>16)+(b+e)&65535)<<16|d&65535)};r.and=function(a){return U(this.h&a.h,this.g&a.g)};r.or=function(a){return U(this.h|a.h,this.g|a.g)};r.xor=function(a){return U(this.h^a.h,this.g^a.g)};var Rc=U(0,0),Oc=U(1,0),Pc=U(-1,-1),Tc=U(4294967295,2147483647),Nc=U(0,2147483648);w(Uc,Gc);w(Vc,Uc);w(Wc,Gc);w(Xc,Wc);var vi=0;w(Yc,Cc);Yc.prototype.toString=function(){return"class "+V(Zc(this))};var bd=String.prototype.repeat?function(a,b){return a.repeat(b)}:function(a,b){return Array(b+1).join(a)};w(ed,Cc);w(fd,T);w(dd,T);dd.D=[90,91,125,93,103,155,123,139];w(gd,T);gd.D=[3];w(hd,T);w(id,T);w(jd,T);jd.D=[3,42];var kd;w(nd,Cc);nd.prototype.get=function(){if(!this.g){var a=x._docs_flag_initialData;this.g=a?a:{}}return this.g};w(ld,Cc);ld.prototype.clear=function(){this.g=new nd};ld.prototype.get=function(a){return this.g.get()[a]};var Dd={};var zd="constructor hasOwnProperty isPrototypeOf propertyIsEnumerable toLocaleString toString valueOf".split(" ");Cd.prototype.set=function(a,b,c){var d=!1;if("object"===typeof c){var e=c.gb;d=c.hb||!1;var g=c.domain||void 0;var f=c.path||void 0;var k=c.Ha}if(/[;=\s]/.test(a))throw Error("J`"+a);if(/[;\r\n]/.test(b))throw Error("K`"+b);void 0===k&&(k=-1);this.g.cookie=a+"="+b+(g?";domain="+g:"")+(f?";path="+f:"")+(0>k?"":0==k?";expires="+(new Date(1970,1,1)).toUTCString():";expires="+(new Date(Date.now()+1E3*k)).toUTCString())+(d?";secure":"")+(null!=e?";samesite="+e:"")};
Cd.prototype.get=function(a,b){for(var c=a+"=",d=(this.g.cookie||"").split(";"),e=0,g;e<d.length;e++){g=$h(d[e]);if(0==g.lastIndexOf(c,0))return g.slice(c.length);if(g==a)return""}return b};Cd.prototype.clear=function(){for(var a=(this.g.cookie||"").split(";"),b=[],c=[],d,e,g=0;g<a.length;g++)e=$h(a[g]),d=e.indexOf("="),-1==d?(b.push(""),c.push(e)):(b.push(e.substring(0,d)),c.push(e.substring(d+1)));for(a=b.length-1;0<=a;a--)c=b[a],this.get(c),this.set(c,"",{Ha:0,path:void 0,domain:void 0})};W.prototype.u=!1;W.prototype.ta=function(){this.u||(this.u=!0,this.C())};W.prototype.C=function(){if(this.m)for(;this.m.length;)this.m.shift()()};var Kd=null;w(Md,T);w(Nd,T);Nd.D=[1];w(Od,T);w(Pd,T);w(Qd,W);Qd.prototype.get=function(){if(this.g)return this.g;var a=new Pd;a=R(a,1,"en");a=R(a,2,y());"number"===typeof this.h&&P(a,11,this.h);var b=new Od;b=O(b,2,this.s);var c=pd(this.j,"icso");b=O(b,1,c);M(a,Od,5,b);S(a,9,this.o);b=new Nd;b=mc(b,1,this.i);M(a,Nd,10,b);return this.g=a};w(Rd,T);w(Sd,T);w(Td,T);w(Ud,T);Ud.D=[3,20,27];Wd.prototype.g=function(a){a=Vd(new Ud,vc(a));tf(this.h,a);this.h.v()};w(Xd,Pa);var Yd=5E3*(.75+.5*Math.random());ae.prototype.get=function(){if(0<this.h){this.h--;var a=this.g;this.g=a.next;a.next=null}else a=this.i();return a};var pe;ge.prototype.add=function(a,b){var c=qe.get();c.set(a,b);this.h?this.h.next=c:this.g=c;this.h=c};var qe=new ae(function(){return new je},function(a){return a.reset()});je.prototype.set=function(a,b){this.h=a;this.g=b;this.next=null};je.prototype.reset=function(){this.next=this.g=this.h=null};var ke,me=!1,he=new ge;se.prototype.reset=function(){this.i=this.h=this.m=this.g=null;this.j=!1};var te=new ae(function(){return new se},function(a){a.reset()});X.prototype.then=function(a,b,c){return Fe(this,"function"===typeof a?a:null,"function"===typeof b?b:null,c)};X.prototype.$goog_Thenable=!0;r=X.prototype;r.ia=function(a,b){return Fe(this,null,a,b)};r.catch=X.prototype.ia;r.cancel=function(a){if(0==this.g){var b=new Ee(a);ne(function(){Ae(this,b)},this)}};r.Ma=function(a){this.g=0;re(this,2,a)};
r.Na=function(a){this.g=0;re(this,3,a)};r.Ca=function(){for(var a;a=Be(this);)Ce(this,a,this.g,this.u);this.o=!1};var Je=Qa;Oa(Ee,Pa);Ee.prototype.name="cancel";Ke.prototype.h=function(){this.defaultPrevented=!0};var Lh="ontouchstart"in x||!!(x.document&&document.documentElement&&"ontouchstart"in document.documentElement)||!(!x.navigator||!x.navigator.maxTouchPoints&&!x.navigator.msMaxTouchPoints),Mh="PointerEvent"in x,cf=function(){if(!x.addEventListener||!Object.defineProperty)return!1;var a=!1,b=Object.defineProperty({},"passive",{get:function(){a=!0}});try{var c=q();x.addEventListener("test",c,b);x.removeEventListener("test",c,b)}catch(d){}return a}();Oa(Ne,Ke);var Me={2:"touch",3:"pen",4:"mouse"};Ne.prototype.h=function(){Ne.M.h.call(this);var a=this.i;a.preventDefault?a.preventDefault():a.returnValue=!1};var Ye="closure_listenable_"+(1E6*Math.random()|0);var Oe=0;Re.prototype.add=function(a,b,c,d,e){var g=a.toString();a=this.g[g];a||(a=this.g[g]=[],this.h++);var f=Ue(a,b,d,e);-1<f?(b=a[f],c||(b.ea=!1)):(b=new Pe(b,this.src,g,!!d,e),b.ea=c,a.push(b));return b};var af="closure_lm_"+(1E6*Math.random()|0),jf={},ef=0,kf="__closure_events_fn_"+(1E9*Math.random()>>>0);Oa(lf,W);lf.prototype[Ye]=!0;lf.prototype.removeEventListener=function(a,b,c,d){gf(this,a,b,c,d)};lf.prototype.C=function(){lf.M.C.call(this);if(this.g){var a=this.g,b=0,c;for(c in a.g){for(var d=a.g[c],e=0;e<d.length;e++)++b,Qe(d[e]);delete a.g[c];a.h--}}this.j=null};Oa(of,lf);r=of.prototype;r.ca=!1;r.F=null;r.La=function(){if(this.ca){var a=Date.now()-this.s;0<a&&a<.8*this.i?this.F=this.h.setTimeout(this.o,this.i-a):(this.F&&(this.h.clearTimeout(this.F),this.F=null),nf(this,"tick"),this.ca&&(pf(this),this.start()))}};r.start=function(){this.ca=!0;this.F||(this.F=this.h.setTimeout(this.o,this.i),this.s=Date.now())};r.C=function(){of.M.C.call(this);pf(this);delete this.h};sf.prototype.i=function(a){var b=this;if(pd(this.h,"docs-ecir"))return vf(this,a,new Zd(this.h));a=Vd(new Ud,vc(a));tf(this.g,a);return new X(function(c,d){uf(b,c,d)})};yf.prototype.g=function(a){return this.h.g(a)};w(zf,T);w(Af,T);Af.D=[2];w(Bf,T);w(Cf,T);Cf.D=[2,27,36];w(Df,T);w(Y,T);Ef.prototype.g=function(a){return this.h.i(a).ia(function(b){if(!(b instanceof Xd&&b.g)){b=oc(a,oh,1);b=v(b);for(var c=b.next();!c.done;c=b.next()){c=c.value;if(!K(c,Y,5)){var d=c,e=new Y;M(d,Y,5,e)}K(L(c,Y,5),Cf,34)||(d=L(c,Y,5),e=new Cf,M(d,Cf,34,e));O(L(L(c,Y,5),Cf,34),26,!0)}return Ff()}},this)};Hf.prototype.start=function(){var a=this;this.h=!0;this.g||(this.g=setTimeout(function(){Kf(a)},this.i),this.m=this.j())};Lf.prototype.reset=function(){this.h=this.g=this.i};w(Mf,T);Mf.prototype.na=function(){return sc(this,1)};w(Nf,T);w(Of,T);Of.D=[1];w(Qf,T);var Rf=["platform","platformVersion","architecture","model","uaFullVersion"],Tf=new Of,Sf=null;w(Vf,T);w(Wf,T);Wf.prototype.X=function(a){return S(this,2,a)};Wf.D=[3,5];w(Xf,T);var wi=function(a){return function(b){if(null==b||""==b)b=new a;else{b=JSON.parse(b);if(!Array.isArray(b))throw Error("y");Zb(b,32);b=Lb(a,b)}return b}}(Xf);Xf.D=[5,6,7];w(Yf,T);var xi;xi=new function(a,b){this.h=a;this.g=b;this.i=L;this.defaultValue=void 0}(175237375,Yf);w(cg,W);cg.prototype.C=function(){this.v();If(this.g);If(this.aa);W.prototype.C.call(this)};cg.prototype.log=function(a){if(this.H){a=wc(a);var b=this.ya++;a=Q(a,21,b);if(!qc(a)){var c=Date.now();b=a;c=Number.isFinite(c)?c.toString():"0";gc(b,1,Fb(c))}null!=N(a,15)||Q(a,15,60*(new Date).getTimezoneOffset());b=this.i.length-this.Z+1;0<b&&(this.i.splice(0,b),this.o+=b);this.i.push(a);this.U||this.g.h||this.g.start()}};
cg.prototype.flush=function(a,b){var c=this;if(0===this.i.length)a&&a();else{var d=Date.now();if(this.pa>d&&this.T<d)b&&b("throttled");else{this.B&&("function"===typeof this.B.na?gg(this.h,this.B.na()):gg(this.h,0));var e=jg(this.h,this.i,this.o,this.S,this.R);d={};var g=this.V();g&&(d.Authorization=g);this.s||(this.s=$f());try{var f=(new URL(this.s)).toString()}catch(h){f=(new URL(this.s,window.location.origin)).toString()}f=new URL(f);this.L&&(d["X-Goog-AuthUser"]=this.L,f.searchParams.set("authuser",
this.L));if(g&&this.ba===g)b&&b("stale-auth-token");else{this.i=[];this.g.h&&If(this.g);this.o=0;var k=vc(e);d={url:f.toString(),body:k,fb:1,Ia:d,Ja:"POST",withCredentials:this.withCredentials,Y:this.Y};f=function(h){c.j.reset();Jf(c.g,c.j.g);if(h){var l=null;try{var m=JSON.stringify(JSON.parse(h.replace(")]}'\n","")));l=wi(m)}catch(n){}l&&(h=Number,m="-1",m=void 0===m?"0":m,m=tc(qc(l),m),h=h(m),0<h&&(c.T=Date.now(),c.pa=c.T+h),l=xi.g?xi.i(l,xi.g,xi.h,!0):xi.i(l,xi.h,null,!0),l=null===l?void 0:l)&&
(h=-1,h=void 0===h?0:h,l=tc(Bb(bc(l,1)),h),-1!==l&&(c.j=new Lf(1>l?1:l),Jf(c.g,c.j.g)))}a&&a();c.S=0};k=function(h,l){var m=oc(e,Ud,3);var n=N(e,14),t=c.j;t.h=Math.min(3E5,2*t.h);t.g=Math.min(3E5,t.h+Math.round(.2*(Math.random()-.5)*t.h));Jf(c.g,c.j.g);401===h&&g&&(c.ba=g);n&&(c.o+=n);void 0===l&&(l=500<=h&&600>h||401===h||0===h);l&&(c.i=m.concat(c.i),c.U||c.g.h||c.g.start());b&&b("net-send-failed",h);++c.S};c.B&&c.B.send(d,f,k)}}}};cg.prototype.v=function(){hg(this.h,!0);this.flush();hg(this.h,!1)};
Zf.prototype.X=function(a){this.g.X(a);return this};kg.prototype.send=function(a,b,c){var d=this,e,g,f,k,h,l,m,n,t,p,z,F;return za(new ya(new ua(function(C){switch(C.h){case 1:return g=(e=d.Aa?new AbortController:void 0)?setTimeout(function(){e.abort()},a.Y):void 0,C.m=2,C.j=3,f=Object.assign({},{method:a.Ja,headers:Object.assign({},a.Ia)},a.body&&{body:a.body},a.withCredentials&&{credentials:"include"},{signal:a.Y&&e?e.signal:null}),ra(C,fetch(a.url,f),5);case 5:k=C.u;if(200!==k.status){null==(h=c)||h(k.status);C.O(3);break}if(null==(l=b)){C.O(7);
break}n=m=l;return ra(C,k.text(),8);case 8:n(C.u);case 7:case 3:C.v=[C.g];C.m=0;C.j=0;clearTimeout(g);ta(C);break;case 2:t=sa(C);switch(null==(p=t)?void 0:p.name){case "AbortError":null==(z=c)||z(408);break;default:null==(F=c)||F(400)}C.O(3)}})))};kg.prototype.na=ba(4);w(lg,W);lg.prototype.sa=function(){this.j=!0;return this};(function(){if(Nh)return mg(/Firefox\/([0-9.]+)/);if(Kg||fi||ei)return oi;if(ng){if($a()||ab()){var a=mg(/CriOS\/([0-9.]+)/);if(a)return a}return mg(/Chrome\/([0-9.]+)/)}if(si&&!$a())return mg(/Version\/([0-9.]+)/);if(pi||qi){if(a=/Version\/(\S+).*Mobile\/(\S+)/.exec(y()))return a[1]+"."+a[2]}else if(ri)return(a=mg(/Android\s+([0-9.]+)/))?a:mg(/Version\/([0-9.]+)/);return""})();var pg={navigate:2,reload:3,back_forward:4,prerender:5};w(rg,T);rg.D=[2];w(sg,T);var Kh=function(){if(Fh){var a=/Windows NT ([0-9.]+)/;return(a=a.exec(y()))?a[1]:"0"}return Gh?(a=/1[0|1][_.][0-9_.]+/,(a=a.exec(y()))?a[0].replace(/_/g,"."):"10"):Ih?(a=/Android\s+([^\);]+)(\)|;)/,(a=a.exec(y()))?a[1]:""):gi||hi||ii?(a=/(?:iPhone|CPU)\s+OS\s+(\S+)/,(a=a.exec(y()))?a[1].replace(/_/g,"."):""):""}();w(vg,T);w(wg,T);w(xg,T);w(Bg,yg);var Eh=new ug("high_frequency_builder");Oa(Cg,W);var Dg=[];Cg.prototype.C=function(){Cg.M.C.call(this);Fg(this)};Cg.prototype.handleEvent=function(){throw Error("V");};w(Hg,yg);var sh=new ug("system_builder");w(Mg,Ke);var Og=new WeakMap;Oa(Rg,lf);r=Rg.prototype;r.Ea=Pg(function(){var a=!!this.N(),b="hidden"!=this.N();if(a){var c;b?c=(((Jg?"Webkit":Le?"Moz":Kg?"ms":null)||"")+"visibilitychange").toLowerCase():c="visibilitychange";a=c}else a=null;return a});r.N=Pg(function(){return Lg("hidden",this.h.g)});r.Fa=Pg(function(){return Lg("visibilityState",this.h.g)});r.Ga=function(){var a=this.N()?this.h.g[this.Fa()]:null;a=new Mg(!!this.h.g[this.N()],a);nf(this,a)};r.C=function(){hf(this.o);Rg.M.C.call(this)};w(Sg,W);Sg.prototype.j=function(){if(this.h.ka()){var a=this.g;a=!!a.h.g[a.N()];a=this.h.da(a?102001:102E3,0);this.h.ha(a)}};w(Tg,W);r=Tg.prototype;r.ha=function(a){var b=this.g;Q(a.g,6,b.i);a=Ag(a);b.g.add(a);b.j=!0;b=this.h;3<=b.g.g.g.length&&b.h.i();return new tg};r.da=function(a,b){a=Ig(th(this.g,a),b,this.g.o++);1==b&&(b=this.g,sc(L(a.g,xg,8),3),b.u.add(a));return a};r.ja=function(){return this.g.h};r.la=function(){var a=this.g,b=xh(a,716);wh(a,b);b=Ag(b);a.g.add(b);a.v=!0;a.s=!0;a=this.h;Gg(a.s,a.h.i,a.h);Gg(36E5,a.H,a);this.h.h.i();this.i&&new Sg(this)};r.ma=function(){this.h.j();return xe(Array.from(this.h.i)).then()};
r.ka=function(){var a=this.g;return a.v&&a.s&&!0};w(Ug,W);Ug.prototype.i=function(a){this.h=arguments;this.g?this.j=!0:Vg(this)};Ug.prototype.C=function(){W.prototype.C.call(this);this.g&&(x.clearTimeout(this.g),this.g=null,this.j=!1,this.h=null)};w(Wg,W);Wg.prototype.j=function(){var a=this;if(0!=this.g.g.g.length&&(!this.o||this.g.j)){var b=vh(this.g),c=this.v.g(b);c&&(ze(c,function(){return void a.i.delete(c)}),this.i.add(c))}};Wg.prototype.H=function(){var a=this.g,b=xh(a,1153);b=Ag(b);a.g.add(b);this.h.i()};w(Xg,W);r=Xg.prototype;r.ha=function(a){a=this.g.ha(a);this.ma();return a};r.da=function(a,b){return this.g.da(a,b)};r.ja=function(){return this.g.ja()};r.la=function(){return this.g.la()};r.ma=function(){return this.g.ma()};r.ka=function(){return this.g.ka()};w(Yg,T);w(Zg,T);w($g,T);w(ah,T);w(bh,T);w(ch,T);w(dh,T);w(eh,T);w(fh,T);w(gh,T);var hh=x.window?[x.window,x.window.opener,x.window.parent]:[];jh.prototype.g=function(a){if(this.o&&this.h){var b=L(a,Yg,2)||new Yg,c=L(a,gh,5)||new gh,d=L(a,Pd,3)||new Pd;a=oc(a,oh,1);a=v(a);for(var e=a.next();!e.done;e=a.next())this.i.postMessage({detail:{impression:vc(e.value),session_info:vc(b),session_invariants:vc(c),client_info:vc(d)}});document.dispatchEvent(new CustomEvent("ripple",{detail:{clientX:this.j,clientY:this.m}}))}};kh.prototype.va=function(){return new Bg};mh.prototype.add=function(a){this.g.push(a)};nh.prototype.add=function(a){sc(L(a.g,xg,8),3);var b=void 0===b?0:b;b=tc(N(a.g,12),b);this.g[b]=a};w(oh,T);w(ph,T);w(qh,T);qh.D=[1];yh.prototype.va=function(){return new Hg};Ph.prototype.start=function(){var a=x.DOCS_drawing_load,b=x.DOCS_drawing_decode,c;for(c in a)Th(this,c);for(var d in b)Sh(this,d);(a=x.DOCS_timing)&&(a=a.ejl-a.sjl)&&Qh(this,29035,a);Qh(this,29031,performance.now()-this.g)};x.DOCS_initPublishImpressionTracker=function(a,b,c){var d=document.querySelectorAll("img[id^='ed.']").length;a=new Oh(a,b,d,c);var e=new Ph(a,d);Na("DOCS_notifyDrawingLoad",function(g){return Th(e,g)});Na("DOCS_notifyDrawingDecode",function(g){return Sh(e,g)});e.start();return e};
})(this._pubi);
// Google Inc.

//# sourceMappingURL=publish_binary_core.sourcemap
DOCS_timing['ejl'] = performance.now(); DOCS_initPublishImpressionTracker( 142.0 ,  1.0 ); DOCS_timing['epr'] = performance.now();</script></body><div style="position: absolute; bottom: 0px; left: 0px; height: 0px; z-index: 5;"><template shadowrootmode="open"><div id="plasmo-mount-container" style="display: flex; position: relative; top: 0px; left: 0px;"></div></template></div></html>